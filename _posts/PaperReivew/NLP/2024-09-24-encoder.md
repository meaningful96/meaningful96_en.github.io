---
title: "[논문리뷰]Encoders - XLNet, MPNet, ConvBERT"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2024-09-24
last_modified_at: 2024-09-24
---

# XLNet
## Problem Statement
기존의 BERT와 같은 모델은 **마스크 언어 모델(Masked Language Model)** 방식을 사용했는데, 이는 문장의 일부 단어를 마스킹하여 해당 단어를 예측하는 방식이다. 이로 인해 단어 간의 순차적인 관계나 의존성 정보를 충분히 학습하지 못하는 문제가 있다. 또한 BERT는 문장 내 단어 순서 정보를 명시적으로 고려하지 않아, **autoregressive prediciton**이 부족한 한계가 있다.

## Method
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/c63ab063-b765-4902-9c42-f71b2d2964c4">
</p>

XLNet은 <span style="color:red">**Permuted Language Modeling**</span>이라는 방식을 도입하여, **문장 내 모든 단어의 순서를 재배열한 다양한 시퀀스를 학습**하는 모델이다. 이 방식을 통해 단어 간의 순차적 관계를 더 잘 반영할 수 있으며, 문장 내 더 풍부한 문맥 정보를 학습할 수 있게 되었다. 일반적인 언어 모델은 고정된 순서로 문장을 학습하는 반면, XLNet은 순서를 재배열함으로써 모든 단어가 다양한 문맥에서 예측될 수 있도록 한다. 이를 통해 문장의 다양한 순열 조합을 학습하는 것이 가능하다.

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/2f284d89-91a6-4f74-8541-8deba9de052d">
</p>

하지만 XLNet에는 한 가지 한계점이 있다. **전체 위치 정보를 명확히 활용하지 않기** 때문에, Pre-training과 Fine-tuning간의 위치 불일치 문제가 발생할 수 있다. 이는 모델이 학습 과정에서 특정 토큰의 정확한 위치 정보를 고려하지 않아서 발생하는 문제로, 모델 성능에 부정적인 영향을 줄 수 있다.

## Contribution
XLNet은 **순차적 관계와 문맥 정보를 잘 반영**하는 모델로, 자연어 이해와 생성 모두에서 뛰어난 성능을 보여주었다. 특히 Transformer 구조에서 더 나은 언어 모델링 방법을 제시함으로써 자연어 처리 성능을 크게 향상시켰다.

<br/>
<br/>

# MPNet
## Problem Statement
XLNet은 문장 내 **전체 위치 정보를 완전히 활용하지 않기 때문에** 사전학습과 파인튜닝 간의 **위치 불일치 문제**가 발생할 수 있다. 또한, 마스크된 단어를 예측하는 과정에서 문맥 정보의 손실이 있다.

## Method
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/68ef69c8-50ba-485c-8bd7-a6e608b7f85e">
</p>

MPNet은 <span style="color:red">**보조 위치 정보(auxiliary position information)를 도입**</span>하여 모든 토큰의 위치 정보를 활용하고, 문장 내 모든 위치 정보를 입력으로 받도록 설계되었다. 이를 통해 모델이 전체 문맥을 고려하면서도 토큰 간 종속성을 학습할 수 있게 되었으며, 사전학습과 파인튜닝 간의 위치 불일치 문제를 해결하였다. 또한 마스킹된 단어 주변의 문맥 정보뿐만 아니라 문장의 전체 정보를 균형 있게 학습한다.

## Contribution
- MPNet은 기존 모델들의 **위치 정보 활용 한계를 해결**

<br/>
<br/>

# ConvBERT
## Problem Statement
BERT와 같은 트랜스포머 기반 모델은 **계산 복잡도**가 매우 커서 대규모 데이터에서 학습과 추론이 비효율적이라는 문제가 있다. 또한 장기 의존성 정보만을 학습하는 방식으로 인해 **Local Context에 대한 문맥 정보를 충분히 반영하지 못한다**.

## Method
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/9a103b31-0c1d-4ca5-b36b-6bc234f59f1b">
</p>

ConvBERT는 **CNN(Convolutional Neural Networks) 구조를 트랜스포머에 결합**하여 **로컬 문맥 정보를 더 잘 학습**할 수 있도록 설계된 모델이다. Convolutional 모듈을 도입함으로써 문장 내 국소적 정보까지 효과적으로 학습할 수 있었으며, 동시에 계산 복잡도를 크게 줄여 효율성을 높였다. 

특히 ConvBERT에서 도입된 <span style="color:red">**Span-based Dynamic Convolution**</span>은 기존의 CNN이 고정된 필터를 사용하는 것과 달리, **동적인 필터**를 사용하여 문맥을 더 유연하게 반영하는 방식을 채택하였다. 문장을 일정한 span으로 나누어, 각 span 내에서 동적인 필터를 적용하여 **로컬 문맥을 정교하게 학습**할 수 있었다. 이 동적인 필터는 각 span의 문맥에 맞게 생성되어, 고정된 필터보다 더 적응적이고 세밀하게 문장의 특정 구간을 반영할 수 있다.

이 방식은 트랜스포머 구조와 **상호 보완적**으로 작용하여, **장기적 의존성**뿐만 아니라 **로컬 문맥 정보**도 균형 있게 학습할 수 있도록 한다. 결과적으로 ConvBERT는 BERT 같은 기존 모델이 갖고 있던 한계를 극복하면서, 계산 비용을 줄이고 추론 속도를 높이면서도 **더 효율적이고 정교한 문맥 학습**을 가능하게 한 모델이다.

## Contribution
ConvBERT는 CNN 구조를 트랜스포머에 결합하여 로컬 문맥 정보를 더 잘 학습할 수 있도록 설계되었다. Convolutional 모듈을 사용함으로써 문장 내 **국소적 정보(local context)도 효과적으로 학습**할 수 있었고, 동시에 계산 복잡도를 크게 줄여 효율성을 높였다

# Reference
\[1\] [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237), 2019    
\[2\] [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297), 2020    
\[3\] [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496), 2020  
