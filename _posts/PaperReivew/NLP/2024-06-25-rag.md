---
title: "[논문리뷰]RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"

categories: 
  - NR

  
toc: true
toc_sticky: true

date: 2024-06-25
last_modified_at: 2024-06-25
---

*Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S.*, & Kiela, D. (2020, May 22). **Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks**. arXiv.org. https://arxiv.org/abs/2005.11401

# Problem Statement
## 1. 정보의 한계성
기존의 언어 모델은 주로 training set에서 학습한 정보에 근거하여 답변을 생성한다. 이는 모델이 <span style="color">학습한 데이터 범위 내의 정보만 제공할 수 있다는 한계를 가진다. 즉, 모델이 학습하지 않은 최신 데이터나 특수한 Knowledge에 대해서는 답변을 제공하기 어렵다

## 2. Hallucination 
Hallucination이란 문제가 질문(Query)에 대한 오답을 마치 정답처럼 생성해내는 것을 말한다. LLM에서는 특히 이 Hallucination 문제가 매우 중요한 한계로 작용하고 있다. 모델이 학습한 데이터셋의 크기가 작거나 vocabulary 사이즈가 너무 작은 경우, 혹은 overfitting이 되었거나 데이터의 qulity가 낮은 경우 발생한다.

Retrieval-Augmented Generation(RAG)는 이러한 문제점을 해결하기 위해 <span style="color:red">검색 기반의 접근</span>을 하며 <span style="color:red">생성 모델과의 통합</span>을 통해 해결하고자 하였다.

<br/>
<br/>

# Related Work
## 1. Open Domain Question Answering(ODQA)
Open Domain Question Answering은 보통 Open Domain QA 혹은 ODQA로 불린다. 이 task는 매우 넓은 범위의 주제에 대해 질문(query)에 답할 수 있는 모델을 설계하는 것을 목표로 한다. 이러한 질문은 특정 도메인에 국한되지 않으며, 시스템은 인터넷이나 데이터베이스와 같은 넓은 범위의 지식에서 답변을 검색한다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/e9ff7c5f-e5c2-49f3-9afa-580bb7781d9b" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>


쉽게 말해, 입력으로 들어온 query에 대해 Wikidata와 같은 외부 데이터베이스에서 관련된 문서(passage)를 search하고, query와 관련된 case를 여러 개 가지고와 모델이 이를 통해 가장 정합한 case를 고르고 답변을 추론하는 것이다. (즉, 모델이 Input sequence와 함께 외부 데이터의 문서를 같이 이용해서 추론을 한다.) 또한 답변(answer)는 "바나나, 사과, 수박"과 같이 연속된 여러 개의 토큰으로 출력이 가능하다. 외부 데이터의 문서를 검색하기 때문에 <span style="color:red">답변이 외부 데이터베이스나 질문에 존재가 가능</span>하다. 

- Example
  1. 질문(Query): "브라질의 수도는 어디인가요?"
    - 답변(Answer): "브라질리아"
    - 이 때 모델은 브라질의 지리적, 정치적 정보를 검색하여 "브라질리아"를 답변한다.
  2. 질문(Query): "양조의 과정은 어떤 것이 있나요?"
    - 답변(Answer): "제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장"
    - Wikidata에서 "양조가 이루어지는 데에는 제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장 등의몇 가지 과정을 거친다"를 검색하여 답변을 추론한다.

## 2. Knowledge Intensive Task(KIT)
Knowledge Intensive Task는 특정 지식이 요구되는 작업을 지칭한다. 이런 작업들은 단순한 정보 검색을 넘어서, 복잡한 추론, 문맥 이해, 전문 지식을 필요로 한다. 특히 사실 확인(fact checking)문제가 대표적인 예이다. ODQA와 달리 KIT는 더 복잡한 추론과 전문 지식을 필요로 하며, 답변을 생성하기 위해 여러 출처의 정보를 통합하고 유추하는 과정이 필요할 수 있다. 따라서 <span style="color:red">질문(query)이나 검색된 외부 데이터(passage)에 정답 토큰이 직접적으로 존재하지 않을 수 있다</span>.

- Example (Fact Checking)
  1. 질문(Query): "한국의 수도는 강원도 태백이다." 
    - 답변(Answer): "거짓"
    - Wikidata에 검색된 passage: 대한민국의 수도는 서울특별시이며 한강이 도시를 관통한다.

이처럼 질문과 검색된 외부데이터(passage)어디에도 거짓이라는 토큰이 존재하지 않음에도 불구하고 모델이 "거짓"을 답변으로 추론할 수 있다. 

## 3. ODQA와 KIT를 푸는 모델들의 종류

Data 형식
- Question($$q$$): 질문(Query)에 해당하며 외부 지식없이 쉽게 답할 수 없다.
- Answer($$a$$): 답변, Passage 내에 연속된 span으로 존재한다고 보장이 불가능하다.
- Knowledge Base($$KB$$): 외부 지식 데이터 베이스, 수백만 ~ 수억 개의 문서로 구성되며 대표적으로 Wikidata가 이에 해당한다.
- Passage($$p$$): 질문과 관련된 문서, KB에서 선택된 문서이다.

모델은 크게 세 가지로 나뉘어 진다. Retreiver, Reader와 Generator가 있으며, Reader와 Generator는 방법론에 따라 달리 선택된다.

### 1) Retriever
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/ecbc564a-3a89-495f-a461-8b32ef9c8448" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

Retriever는 **질문과 관련된 passage를 KB에서 찾는 모델**이다. BERT와 같은 Encoder 구조를 가진다. 위의 그림처럼 Bi-encoder의 형태일 수 있다. 위에서는 정답을 추론하기 위해 score를 (i)Passage 인코더와 (ii)Question 인코더의 출력 벡터의 내적으로 삼았다. 이 때, Passage 인코더에서 출력된 벡터가 일종의 weight로 동작하게 된다. score값이 높을 수록 검색된 문서가 정답 추론에 중요한 역할을 한다는 것을 의미한다.

<br/>

### 2) Reader
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/9b7072eb-fcea-489c-99d2-78f2a1829e6e" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

Reader는 **passage에서 정답 후보가 될 수 있는 Span을 찾는 모델**이다. 먼저 BERT가 질문과 passage text를 함께 입력으로 받아 hidden representation을 출력한다. 이후, Reader가 시작 (CLS)과 끝 (SEP) 토큰의 Hidden representation을 concatenation한 벡터를 입력으로 받고 최종 score를 산출한다. 한 query에 대해 top-k개의 passage마다 score를 각각 산출하고 Marginalize하여 최종 정답을 선택한다.

<br/>

### 3) Generator
Generator는 <span style="color:red">passage와 짛질문을 입력으로 하여 정답을 생성하는 몯모델</span>이다. Reader와 달리 Marginalize하는 과정이 없으며, Generation을 하기 위해 BART나 T5와 같이 Encoder-Decoder 구조를 가진다.

## 4. 외부 지식(passage)이 필요한 이유.
만약 Open Domain QA를 푼다고 가정할 때 외부 지식이 없다고 가정하면, 학습된 데이터로만 정답을 추론하게 된다. 이 때 질문 $$q$$는 외부 지식 없이 쉽게 답할 수 없는 질문이며, 정답 $$a$$는 passage 내에 연속된 span으로 존쟇재한다고 보장이 불가능하다. 이러한 상황에서 외부 지식을 사용하지 않을 경우 정답에 대한 확률 값은 **질문이 주어졌을 때 정답의 Likelihood**가 되며 이를 수식으로 쓰면 $$max P(a \vert q)$$이다. 즉, 질문에 대한 정답의 확률값을 모두 구하고 이 확률이 최대가 되는 값을 정답으로 삼는 것이다.

Related Work Section에서 예시로 "양조의 과정은 어떤 것이 있나?"라는 질문을 했을 때 정답을 "제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장"라고 답했고, 이는 외부 지식(passage)에 연속된 형태로 존재하던 토큰이다. 즉, Open Domain QA에서 이 외부 지식이 없으면 위와 같은 정답을 유추해내지 못한다. 다시 말해 <u>ODQA에서 QA만으로 task 수행이 불가능</u>하다. (이 수식을 베이즈 정리를 적용한다 하더라도 정답에 대한 질문의 확률 분포를 모른다.)

반면 passage를 사용하게 되면 $$max P(a \vert q)$$의 식은 $$max P(a, p \vert q)$$로 바뀌게 된다. 이는 베이즈 정리(Bayes' theory)에 의해 $$max P(a, p \vert q) = max P(a \vert p, q)P(p \vert q)$$로 바뀌게 된다. 식의 의미를 생각해보면 다음과 같다.

(i) $$P(a \vert p, q)$$는 질문과 passage가 입력으로 들어왔을 때 정답의 likelihood 
(ii) $$P(p \vert q)$$는 해당 질문에 대해 가장 적합한 passage의 likelihood  

다시 말해 외부 지식이 있으면 ODQA의 정답에 대한 liklihood는 (i) <span style = "color:red">**Passage의 확률분포**</span>와 (ii) <span style="color:red">**Answer에 대한 Passage의 liklihood**</span>로 분해되어 쉽게 풀 수 있다. 학습은 이 둘을 maximize하는 방향으로 진행하면 된다.

## 5. 모델의 발전 양상
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/a4a72737-e686-42d8-b9a8-52ed487d532e" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>


위에처럼 기존에는 Retriever-Reader를 중심으로 Open Domain QA문제를 푸는 것을 목표로 했다. RAG가 가지는 중요한 의미는 ODQA task를 KIT와 결합하여 확장했다는 것이다. RAG이전의 모델들은 간단하게 정리하면 다음과 같다.

- **DrQA**: Retriever에 대한 training이 불가하여 passage 개선이 불가능하다.
- **ORQA**: Retriever를 학습하는 것이 가능하다. 하지만, passage를 Latent variable에 도입하는 모델링($$P(a, p \vert q) = P(a \vert p, q)P(p \vert q)$$)이 불가능하다.
- **REALM**: Retriever 업데이트 시 필요한 정보는 "정답 추론 시 passage가 도움이 되는 정도"이다. REALM을 통해 Passage를 Latent Variable로 도입한 모델링이 가능하다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/2a17f6c3-a686-44d9-81a1-8c274030f956" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

참고: [PDF](https://github.com/meaningful96/Blogging/blob/main/Paper_Review/RAG/Before_RAG.pdf)(<span style="font-size: 80%"><span style="font-size:80%">※ 이 자료는 고려대학교 DSBA Lab의 자료이다.</span></span>)
  
<br/>
<br/>

# Method

## 1. Background

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/5711a4d5-4960-445a-9814-6b51feed86f0" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

Closed-book QA는 **질문에 대한 정보를 전혀 주지 않은 채, 모델에게 답변을 생성하도록 하는 task**이다. 외부 지식에 접근할 필요가 없기 때문에 End-to-End로 추론이 가능하다. 하지만, 생성 과정에서 지식을 직접 사용자가 수정하는 것은 불가능하며, Hallucination문제가 발생하기 쉽다.

반면 Open Domain QA는 **외부 지식(passage)을 답변을 찾는데 이용하는 task**로, 검색된 외부 지식을 통해 사용자가 수정, 업데이트가 가능하다는 장점이 있다. 하지만, 답변을 생성하는 것이 아니기 때문에 답변 내용에 제한이 존재한다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/44ebb71d-3cd7-4107-9c42-7852ad427ba9" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

최근 KIT는 Retrieval-Augmented LM을 기반으로 다양한 연구가 진행되고 있으며, 이는 RAK의 framewalk를 기반으로 발전하고 있다.

## 2. Model Architecture
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/0a091f83-165e-4c9c-bc89-479a2f7c0df3">
</p>

RAG는 크게 1)Retriever과 2)Generator로 구성된다. 1)Retriever의 질문(query)를 입력받는 Query Encoder로 DPR Query Encoder를 이용하였으며, 외부지식을 검색하고 Top-k개의 문서를 검색해오는 Passage Encoder은 DPR Passage Encoder를 사용하였다. 2)Generator는 BART-Large를 사용하였다.

Query가 입력되면 Query Encoder를 통과하여 hidden representation $$q(x)$$를 생성하고, $$q(x)$$와 가장 내적 값이 큰 top-k개의 Passage를 탐색한다. 탐색된 Passage는 기존 Query와 concatenation하여 Generator에 입력으로 사용하고, 각 Passage별 생성 결과를 Marginalize하여 최종 결과물을 도출하게 된다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/bd257ed0-5f29-463d-a91f-a54a864fb945" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

RAG의 구조는 REALM의 모델 구조에서 Reader를 Generator로 변경한 모델 구조이다. Generator로 변경함으로써 Span prediction task를 Answer Generation task로 전환하였으며, QA task외에도 다양한 생성 기반의 문제를 푸는데 적용이 가능하게 되었다.

### 1) Retriever

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/46fe54cd-b5a1-4915-a590-aaa4ab832215" style="display: block; margin: auto;">
</figure>

Retriever는 DPR을 활용하여 관련 문서를 검색한다. DPR이란 Dense Passage Retriever의 약자로 키워드 기반의 검색 시스템이 아닌 의미적 유사성을 기반으로 검색을 하여 정확한 검색 결과를 제공한다. 그래서 DPR을 통해 입력에 따른 문서(document)를 제공받게 된다. 이 때 검색되 문서들이 바로 latent passage이다.

옊가장 먼저 DPR을 활용하기 위해서는 질문 query와 passage의 hidden representation을 뽑아내야한다. 위의 그림과 같이 두 인코더는 서로 다른 BERT이며 query의 hidden representation을 $$q(x)$$, document(passage)의 hidden representation을 $$d(z)$$로 정의했다. 이 때, representation을 만드는 것을 query와 document 동시에 진행한다. 그 후 여기서 관련도가 가장 높은 $$k$$개를 뽑기위해 Maximum Inner Product Search(MIPS)를 진행한다. 이는 `Faiss` 라이브러리를 통해서 빠르게 진행이 가능하며, sub-linear time으로 진행이 가능하다.

<span style="color:red">한 query에 대해 특정 document와의 유사도는 두 벡터의 내적에 비례하며 이는 $$p_{\eta}(z \vert x)$$로 정의</span>된다. RAG는 Open Domain QA를 풀기 위해서 답변이 포함된 document를 통해 수행하고 검색된 문서들은 모델의 파라미터에 직접적으로 통합되지 않는다. 대신, 이 문서들은 필요에 따라 검색되어 사용되므로 non-parametric memory로 분류된다.

참고로 parametric model이란 고정된 수의 파라미터를 가지며, 학습된 이후에는 이 파라미터들만을 사용하여 예측을 수행하는 모델을 말하며 일반적인 신경망 모델이 이에 해당한다. 반면 non-parametric model은 고정된 파라미터 세트에 의존하지 않고, 데이터 자체를 직접 참조하여 예측을 수행하며 데이터의 크기가 증가함에 따라 모델의 복잡성도 증가할 수 있다.

다시 말해, RAG에서 Retriever는 학습 데이터(documents)를 직접 저장하고 참조하고 새로운 질문이 들어오면, 저장된 문서들 중에서 가장 관련성 높은 것을 검색하는 <span style ="color:red">**직접적인 데이터 참조 방식**</span>을 채택하고, 저장된 document수만큼만 파라미터수를 사용해도 무방하기 때문에 파라미터 수를 고정하지 않는 non-parametric model인 것이다.

### 2) Generator

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/6cb4bc64-2e7c-416e-a38b-75566424187f">
</p>

Generator는 Encoder-Decoder구조를 가지는 사전 학습된 BART-Large이다. Generator는 Retriever에서 MIPS를 통해 구한 $$k$$개의 연관성이 높은 document와 Query의 hidden representation을 함께 입력받는다. 이 때, 하나의 query는 총 $$k$$개의 document와 각각 concatenation하므로, <span style="color:red">하나의 query에 대해서 만들어지는 generator의 입력은 $$k$$개</span>이다. 이는 배치 처리를 통해 추론하기 때문에 빠르다. 이후 각 Query-Document pair는 vocab distribution을 생성하게되고 aggregation과정을 통해 하나의 문장을 생성하게 된다.

BART는 BERT와 GPT의 장점을 결합하여 만든 인코더-디코더 구조의 transformer이다. 학습 과정에서 원래 문장에 다양한 방식으로 노이즈를 추가해 준 뒤에 원래 형태로 복원하는 작업을 통해서 train을 하게 된다. 이는 텍스트의 의미와 문맥을 잘 이해할 수 있도록 도와 주기에 이 task에 적합하다고 볼 수 있다.

### 3) How to Decode?

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/fed85500-5516-4361-957e-b9fe75d3a496" style="display: block; margin: auto;">
</figure>

RAG(Retrieval-Augmented Generation) 모델은 문서 검색을 통해 정보를 통합하는 방식으로, **RAG-Token**과 **RAG-Sequence** 두 가지 변형으로 구현된다. 이 두 방식은 검색된 문서를 어떻게 생성 과정에 통합(aggregation)하는지에 차이가 있다.

<br/>

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/4988c120-0bc7-434a-948c-c8ea475d5f93" style="display: block; margin: auto;">
</figure>

첫 번째는 통합 방식은 **RAG-Seqequence**이다. RAG-Seqeunce는 <span style="color:red">전체 시퀀스를 생성한 후에 document에 대한 marginalize를 수행</span>한다. 이를 위해 각 document별로 별도의 Beam Search를 수행합니다. 이는 각 document에 대해 독립적으로 가장 적합한 시퀀스를 생성한 후, 이들 중 최적의 시퀀스를 선택하는 방식이다. 당연하게도 Decoding과정에서 빨간색 박스들과 같이 탐색되지 못한 Passage별 문장이 존재할 수 있다. 이들은 logit계산시 모두 0으로 처리된다. 생성된 모든 문장들을 각각 forwarding하여 logit을 계산하게되므로 beam의 크기가 커질 경우 연산량이 가중된다. 

<br/>

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/f5db1850-e3b0-496d-8aa1-a6e7d0463fb7" style="display: block; margin: auto;">
</figure>

두번째는 **RAG-Token**이다. RAG-Token 모델은 auto-regressive sequence-to-sequence (seq2seq) generator로, 각 토큰에 대한 transition 확률을 가지고 있다. 이 모델의 디코더는 표준 beam search 디코더를 사용한다. 기본적으로 이 모델은 전통적인 seq2seq generator와 유사하게 작동한다. <span style= "color:red">각 토큰은 marginalize 과정을 거쳐 개별적으로 평가</span>되며, 이는 기존의 seq2seq 모델과 큰 차이가 없다. 그러나 RAG-Token의 주요 차별점은 문서에서 얻은 output 분포를 활용하여 각 출력값을 결정한다는 점이다. 이를 통해 모델은 문서의 정보를 통합하며 보다 정확한 답변 생성을 도모한다.


<br/>
<br/>

# Experiments
## 1. Open Domain QA

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/a0dac67b-66f6-441e-882f-daa1b2e0ebda">
</p>

기존 QA 모델들과는 달리 Generation으로 접근을 하였다. Reader를 사용한 DPR보다 성능이 좋은 것을 알 수 있다. 또한 **외부 지식을 사용하지 않은 Closed-Book보다 성능이 압도적으로 좋다.**

## 2. Knowledge Intensive Task

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/5cc94920-c8bb-4a67-aad8-6933432037f1" style="display: block; margin: auto;">
</figure>

- Abstract QA task와 Jeopardy Question Generation
  - BART에 비해 Hallucination이 더 적게 발생하고, Diversity또한 더 높다. SOTA에 근사한 결과를 얻었다.
  - Jeopardy QG 결과를 통해 위키피디아에 해당 질문 내용이 없더라도 잘 생성하는 것을 알 수 있으며 SOTA를 달성하였다.
  - 기존 SOTA모델들은 매우 구조가 복잡하며 domain-specific하다. 따라서, RAG의 범용성을 고려 하였을때 충분히 좋은 성능을 지닌다고 한다.
 

## 3. Additional Experiments
### 1) Ablation Study

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/e2680766-fd55-4f98-9120-71e557e5d5a6">
</p>

(1) RAG의 Retriever를 BERT에서 BM25으로 교체
  - 대부분의 task에서 기존 모델보다 성능이 떨어짐
  - FEVER에서는 높은 성능이 나옴. 하지만 Fact Checking task로 실제 문서에서 중요한 토큰의 등장 여부가 중요하다. 의미적 정보가 중요하지 않다.
    
(2) Retriever를 freeze한 실험: Generator만 학습 (기존 DPR모델 사용)
  - 성능이 감소
  - 따라서 Retriever또한 학습하는 것이 좋음
    
(3) Knowledge Base 시점 변화 시 성능 변화


- |               | Wiki_2018 | Wiki_2016 |
  |---------------|-----------|-----------|
  | Leader_2018   | 0.68      | 0.04      |
  | Leader_2016   | 0.12      | 0.7       |


  - 각국의 지도자를 맞추는 task를 실시하였다. 예를 들어 Query는 "Who is {the president of Country}?"가 된다.
  - KB의 시점 변화에 따라 성능 변화 관찰이 가능하다.
  - 이를 통해 시점에 따른 데이터도 중요핟하다
  - KG를 사용할 경우 Temporal정보같은 meta data를 잘 다뤄야함을 시사해줌.

### 2) Posterior Visualization

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/85a5c709-557b-4346-ac65-90aa082b6dec">
</p>

RAG-Token 모델이 각 토큰을 생성 시 passage 별 posterior를 시각화한 실험이다. 헤밍워에기 답인 질문을 생성하는 task로 헤밍웨이가 쓴 소설 제목을 생성할 때, 각 소설의 Wikipedia document에 더 높은 점수를 부여한다.

이 때, **각 주제별 첫 단어를 생성하면 특정 document에 가중치를 줄이는 것**을 알 수 있다. BART의 parametic knowledge를 이용하는 것으로 추정된다. 이를 통해 생성 시 효과적으로 다양한 문서를 이용할 수 있음을 알 수 있다.

### 3) Retrieved Passages

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/f0e2746a-56a5-4af5-bfc1-4cb25914cada">
</p>

- 첫번째와 두번째 plot을 통해 RAG-SEQ의 경우 Retrieved Passage의 수가 많을수록 성능이 개선되는 것을 알 수 있다.
- 반면 RAG-TO의 경우 Retrieved Passage의 수가 일정 수 이상이면 성능이 저하된다.
  - Scailing의 제한 존재
  - FiD, FiD-Distil을 통해 극복
 
- 마지막 plot을 통해 Abstractive Answer Generation의 경우 많은 Passage가 필요하지 않음을 알 수 있다. 10개일 때 보통 가장 좋은 성능을 보여주었다.
  - 제한된 외부 지식을 효과적으로 활용한다.   

<br/>
<br/>

# Contribution
1. 본 논문은 <span style="color:green">**Retriever와 Generator를 결합**</span>하는 새로운 방법론을 제시하였다. 이를 통해 Open Domain QA task와 Knowledge Intensive task를 동시에 풀 수 있음을 시사하였다.
2. Retriever에 대해 <span style="color:green">**End-to-End로 학습**</span>을 진행하여 성능을 개선하였다.
3. Generator를 사용함으로써 Hallucination을 줄이고, 모델의 다양성을 증가시켰으며 domain specific한 모델이 아닌 task agnostic한 모델을 만들 수 있게되었다.
4. 외부 지식을 통해 언어 모델이 학습한 Knowledge Base를 쉽게 수정할 수 있고, 생성 결과를 해석할 수 있게 되었다.
5. Knowledge를 직접적으로 디코딩 시 이용할 수 있는 방법론을 제시하였다.

<br/>
<br/>

# Reference
Paper: [RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)      
Video: [고려대학교 산업경영공학부 DSBA](https://www.youtube.com/watch?v=gtOdvAQk6YU)

