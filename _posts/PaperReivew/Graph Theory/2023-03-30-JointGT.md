---
title: "[논문리뷰]JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs"

categories: 
  - GR
  
toc: true
toc_sticky: true

published: true

date: 2023-03-30
last_modified_at: 2023-03-30
---

[JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs]("https://arxiv.org/abs/2106.10502")  
ACL, 2021

# Problem Statement

<span style = "font-size:120%">1. Structural information loss during encoding</span>    

기존의 Transformer 모델을 기반으로 한 Pre-trained 모델들의 경우 **Fully Connected Self-Attention**을 중심으로 모델이 구성된다. FC Self-Attention의 경우 <u>하나의 노드에
대해 missing link를 고려하지 않고 모든 노드에 대해 attention</u>을 구하기 때문에 <span style = "color:red">**그래프의 구조적 정보가 무시되는**</span> 문제점이 발생한다.
    
<br/>  

<span style = "font-size:120%">2. Absense of Explicit graph-text aliggnments</span>  

기존의 Text generation을 위한 Pre-Trained 모델들의 경우 **auto-encoding**이나 **auto-regressive text reconstruction**방식을 채택한다. 이 방식은 유실된 정보가 있는(또는 
Masking 처리 된) 'Corrupted Text Sequence'을 인코딩하고 디코딩 결과 'Original Sequence'가 출력된다.

한 가지 명백한 사실은, <u>Knowledge Graph가 일반적인 Text Sequence보다 더 구조적으로 복잡</u>하기 때문에 <span style = "color:red">**text reconstruction에 기반한 pre-training task를 다이렉트하게 이용하여 graph와 text를 매칭시키는 배열인 graph-text alignment를 학습시키기 난해**</span>하다.

<br/> 
<br/> 

# Related Work
<span style = "font-size:120%">1. KG-to-Text Generation</span>  

KG-to-Text는 세 가지 측면으로 나눠진다. 각각 **Encoder modification, Unsupervised Learning, Building pre-training models**이다.  

1) Encoder modification
  - Linearized Graph를 input으로 하는 Sequence Encoder의 고질적 문제인 그래프 <u>구조 정보 손실을 완화</u>하기위해 보다 더 복잡한 Encoder 모델이 필요하다.

2) Unsupervised Learning
  - graph-to-text task와 text-to-graph task를 non-parallel 한 graph-text 데이터를 이용하여 동시에 conversion하는 것을 목표로한다.
  - Unsupervised training objective to jointly learn = Joint Optimization

3) Building Pre-Training Models       
  - 기존의 모델들은 KG-to-Text 데이터셋을 text-to-text 모델에 직접적으로 fine-tuning함.
  - GPT, BART, T5
  - 이 논문에서는 직접적으로 fine-tuning하는 것이 아닌 <u>pre-training을 graph-text alighmnet를 명시적으로 학습하는데 이용</u>한다.
    

<br/> 

<span style = "font-size:120%">2. KG-Enhanced Pre-Trained Models</span>  
KG-Enhanced Pre-trainined model들이 나오게 된 계기는 Knowledge Graph를 Pre-trained 모델에 **통합**하기 위한 시도에서 출발했다. 이렇게 함으로써, <u>그래프의 Entity와 Relation의 이해(Understanding entities and relations)를 자연어를 이용해 더 용이하게 하기 위함</u>이다. 

<br/> 
<br/> 

# Method
## 1. Task Definition & Overview

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228316119-36225e91-8c04-4904-99fe-61fbba60fd5c.png">
</p>

먼저 Notation을 살펴보면 Knowlege Graph $$\mathcal{G} = (\mathcal{V,E})$$ 이다. $$\mathcal{V} = \{e_1, e_2, \cdots, e_{ㅣ\mathcal{V}ㅣ}\}$$로 엔티티(Entity) 집합을 의미하고, $$\mathcal{E} = (r_{ij})$$로 엔티티와 연결된 릴레이션(relation)을 의미한다. 당연히 i와 j는 모든 노드를 한 번씩 지칭하기에 $$r_{ij}$$의 크기는 $$ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{V}ㅣ$$이 된다. 

이 때, Knowlege Graph의 정보를 Input으로 넣어주기 위해 <span style = "color:green">**Linearize**</span>를 하고, $$\mathcal{G}$$의 linearize를 한 수식이 $$\mathcal{G_{linear}} = (w_1, w_2, \cdots, w_m)$$이다. 

Linearize된 <span style = "color:green">$$\mathcal{G_{linear}}$$는 총 **m개**의 token</span>으로 되어 있고, 이를 통해 <span style = "color:green">생성되는 Text Sequence는 **n개**의 Token으로 이루어진 $$X = (x_1, x_2, \cdots, x_n)$$</span>이다. X는 이 모델 자체가 Transformer를 기반으로한 Encoder-Decoder 모델이며, 이는 **Auto-Regressive**하므로 모델의 전체 입력은 $$\mathcal{G_{linear}}$$와 $$X$$를 연속적으로 연결한 벡터가된다. 

- JoingGT 모델의 특징
  - KG의 Triple을 Linearize하고, 이를 이전 Layer의 출력값과 연결해 하나의 Input을 만든다.
  - Input Graph의 구조 정보를 보존하기위해 Structure-Aware Semantic Aggregation Module을 제시한다.
  - 그리고 3가지 새로운 Pre-Training을 제시하고 이에 접목시켜 디코더 부분을 조금씩 바꿔준다.

## 2 Model Structure

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228418591-d997ce8c-426a-4ece-9e57-aea38d9fd4e1.png">
</p>

3가지 Pre-Training Task에 대하여 공통적인 부분을 차지하는 Encoder Module을 살펴보면 크게 네 부분으로 나눠진다. 

- Encoder
  - Vainilla Self-Attention Layer
  - Pooling Layer
  - Structure-Aware Self-Attention Layer
  - Residual Layer

### 1) Vanilla Self-Attention

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228421899-2c46e6df-6343-4de7-89b3-f2d5e67bf684.png">
</p>

먼저 Vailla Self-Attention Layer를 살펴보면, 전통적인 Transformer 모델의 Self-Attention기법을 사용한다. 즉, 한 토큰이 전체 시퀀스에 있는 모든 토큰과 Attention을 통해 각각의 토큰과 얼마나 연관성이 있는지에 대한 정보를 얻게된다. 

이를 통해 결론적으로 입력 시퀀스의 <span style = "color:red">Textual Information(Contextual Semantic relations)</span>을 얻게된다. Pre-Training과정 중의 입력과 출력을 정리하면 다음과 같다. 

- Goal: Capturing Contextual Semantic Information 
- Input: Sequence ($$\mathcal{G_{Linear}} ㅣㅣ X$$), lineaized graph & corresponding text sequence  
- Output: Attention Value in $$l^{th}$$-layer $$h_i^l$$, 여기서 i는 노드(Head Entity) 번호이고 l은 레이어의 층 수를 의미한다.

다시 한 번 강조하면, Vanilla Self-Attention(Fully connected self-attention)의 결과로 엔티티간 <span style = "color:green">**풍부한 Contextual Semantic Relationship**</span>을 얻을 수 있고, 이 정보를 담고있는 Sequence가 Pooling layer의 입력으로 들어간다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228432118-6e271514-0497-4ff6-9626-6f37d26fede7.png">
</p>  
<center><span style = "font-size:80%">Vanilla Self-Attention 수식</span></center>

<br/>

### 2) Pooling Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228431733-b31c369a-fb28-4fe8-8ed4-827f9b718be2.png">
</p>

Vanilla Self-Attention의 결과로 얻은 **Contextual Semantic information**에 **Structural Information**을 넣어주기 위해서 약간의 Sequence를 변형해야 한다. 이 과정을 Pooling Layer에서 수행하게 된다. Pooling은 <span style = "color:green">**mean pooling**</span>으로 하며 이 결과로 엔티티의 representation과 릴레이션의 representaion을 각각 얻게된다.

- Goal: Entitiy Representation과 Relation Representation으로 Input Sequence를 분리
- Input: Vanilla Self-Attention의 결과로 얻은 Contextual semantic vector
- Output: Entity Representation & Relation Representation

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228432365-cbce93ad-c0a1-49c3-be04-f8b30427b947.png">
</p>  
<center><span style = "font-size:80%">mean pooling 수식</span></center>

<br/>

### 3) Structure-Aware Self-Attention Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228433728-2a982a70-8533-4d08-8199-339e4644f129.png">
</p>

Pooling Layer를 통해서 문맥 정보가 담겨져있는 시퀀스의 정보를 엔티티와 릴레이션의 표현식으로 각각 나뉘어졌다. 이 두 표현식을 입력으로 Structure-Aware Self-Attention에 넣어준다.
수식을 살펴보면 <u>한 엔티티의 정보를 모든 엔티티와 릴레이션의 정보를 Attention하는 것</u>을 알 수 있다. 즉, head 엔티티와 다른 모든 엔티티를 비교해 tail 엔티티가 될 수 있는 확률을 보는 한편, 그에 맞는 릴레이션까지 찾아내기위한 과정임을 알 수 있다. 즉, <span style = "color:green">**Contextual Semantic Information을 가진 Input Sequence에 대해 Local Structure Information를 주입**</span>하게 되는 것이다.

```
This layer integrates the contextual semantic representation of entities and relations 
based on graph structure, therby injecting the structural information into the vanila
Transformer layer which contains textual informations
```

- Goal: Graph Structure Information을 Contextual Semantic Information에 주입
- Input: Semantic Information이 있는 Entity & Relation Representation
- Output: (Semantic + Structural) 엔티티에 대한 Attenton Value

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228440150-ce38f74c-71b1-4fed-b043-bba0097ba2af.png">
</p>
<center><span style = "font-size:80%">Structural-Aware Self-Attention 수식</span></center>

<br/>

### 4) Residual Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228440513-0a5f3af5-4f75-4fbc-962c-92016fcd857b.png">
</p>

레이어의 이름에서도 알 수 있듯이, Residual(잔차)를 이용하는 레이어이다. Residual Connection을 통해서 정보를 결합하는 역할을 한다. 먼저 Structural-Aware Self-Attention의 결과로
나온 <span style = "color:green">**엔티티의 Semantic representation과 Structural representation을 결합**</span>하고 최종적인 Hidden State를 뽑아낸다. 

- Goal: Fusing Semantic and Structural representation
- Input: Structural-Aware Self-Attention의 결과로 얻은 엔티티의 representation(Semantic 정보 + Structural 정보)
- Output: l번째 layer의 최종 Hidden state Sequence(처음 입력값인 $$h^{l-1}$$과 동일한 형태)

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228443038-f427cbe5-fb4c-4094-a79b-10fd3c62152d.png">
</p>
<center><span style = "font-size:80%">Residual Layer 수식</span></center>

<br/>

### 5) Summary of Encoder

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228443221-7b95a3bf-3901-4a4d-928e-22bdaa3a3fdc.png">
</p>

기존의 Structure-Aware Transformer 인코더는 모델 파라미터를 통해 직접적으로 학습시키거나, 다른 KG 임베딩 모델로부터 엔티티 임베딩과 릴레이션 임베딩을 얻는다. 반면에 JointGT 인코더의 경우, <u>엔티티와 릴레이션 임베딩을 Contextual semantic representation을 이용해 얻는다.</u> 이 디자인은 구조적인 정보를 보존하면서 동시에 기존의 Pre-trained 모델로부터 얻은  contextual representation을 완전히 이용한다. 그리고 다른 KG dataset을 fine-tuning하여 새로운 엔티티와 릴레이션을 더 잘 생성한다.

## 3. Pre-Training Tasks

논문에서 제시한 Pre-Traninig Task의 목표는 <span style = "color:red">**Input graph '$$\mathcal{G}$$'와 그에 상응하는 Text Sequence '$$X$$'가 주어졌을 때, Graph Encoder와 Sequence Decoder를  Joinly하게 학습시켜 graph-text alignment(그래프-텍스트 정렬)를 향상**</span>시키는 것이다. 총 세 가지의 새로운 Pre-Traning Task를 제안한다. 
- Graph Enhanced Text Reconstruction
- Text Enhanced Text Reconstruction
- Graph-Text Embedding Alignment

### 1) Graph Enhanced Text Reconstruction

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228449128-8b9b0e1a-a547-461f-afec-bf09c05d62f3.png">
</p>

이 Task가 풀고자 하는 것은 <span style = "color:green">**완전한 KG 그래프를 이용하여 Masking된 Text Sequence를 복원**</span>하는 것이다. Graph Enhanced Text Reconstruction을 풀 때 모델의 구조를 보면 앞서 제안된 JointGT의 Encoder 모듈의 입력으로 Triple정보를 Linearize한 $$\mathcal{G_{linear}}$$와 마스킹 처리된 Text Sequence $$\widehat{X}$$이 들어가고 Structual-Aware Self-Attention layer와 Residual Layer를 거쳐나온 최종 Output Sequence가 Transformer 기반 모델(BART, T5)의 디코더의 입력으로 들어간다. 또한 디코더의 또 다른 입력은 마스킹 처리되지 않은 Text Sequence가 들어가 Self-Attention을 진행하게 된다. 결과적으로 디코더의 출력은 Token으로 이루어진 **Text Sequence**가된다.

- Encoder
  - Input: Linearized Graph + Masked Text Sequence
  - Output: Hidden state Sequence
- Decoder
  - Input: Encoder's Output + Fully Text Sequence
  - Output: Text Sequence

Task를 풀기위해서 모델을 정의했으니 이제 최적화를위한 Loss를 설정해야한다. Loss는 $$\mathcal{L_{text}}$$로 표현하고 그 수학적인 형태는 **Negative MLE**와 비슷하다  

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228452304-9252029a-3e33-4eff-ade2-aa885f613829.png">
</p>
<center><span style = "font-size:80%">Loss of Graph Enhanced Text Reconstruction</span></center>

Masking된 Text Sequence를 만들기 위해서는 엔티티를 40%의 확률로, 나머지는 20%의 확률로 마스킹한다. 이 때, 엔티티의 비율이 높은 이유는 KG-to-Text generation에서 엔티티의 단어가 좀 더 중요하기 때문이다. 또한 난이도를 높이기 위해서 연속적으로 마스킹된 토큰들은 하나의 토큰으로 간주한다.

Knowledge Graph를 <span style ="color:green">**분리된 vocabulary space에서 연결된 릴레이션을 순회하며 Corrupted(Masking)된 Text Sequence를 복원**</span>한다.

<br/>

### 2) Text Enhanced Graph Reconstruction

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228458948-9f1c150d-cf43-4bd8-b788-99261fa1c880.png">
</p>

이 Task가 풀고자 하는 것은 앞서 제안된 Graph Enhanced Text Reconstruction과는 반대로 Text가 주어졌을때 Graph를 복원하는 것이다. 쉽게 설명하면 <span style = "color:green">**Text 정보를 이용해서 그래프의 Missing link또는 Missing Entity를 찾는 것**</span>이다. 엔티티와 릴레이션이 마스킹된 Corrupted KG $$\widehat{\mathcal{G}}$$와 완전한 Text Sequence $$X$$가 주어졌을 때 loss function은 <u>linearized KG에서 마스킹된 엔티티와 릴레이션을 복원</u>한다. 

Loss function은 $$\mathcal{L_{graph}}$$로 표현하고 형태는 $$\mathcal{L_{text}}$$와 유사하다. 다만, $$M_i$$라는 파라미터가 추가적으로 있는데 이것은 indicator function(지칭함수)라 하며, Linearized graph $$\mathcal{G_{linear}}$$의 성분인 $$w_i$$가 마스킹되어 있으면 1이다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228452304-9252029a-3e33-4eff-ade2-aa885f613829.png">
</p>
<center><span style = "font-size:80%">Loss of Text Enhanced Graph Reconstruction</span></center>

Graph를 복원하는데 Text를 사용하고, Text에 있는 엔티티와 릴레이션에 좀 더 집중해 인코더를 가이딩한다. 

<br/>

### 3) Graph-Text Embedding Alignment

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228465034-3dd39931-09bf-4299-aab3-22de8d6dae7f.png">
</p>

Graph-Text Embedding Alignment task는 <u>Embedding space에서 graph-text 정렬을 명시적으로 향상</u>시키기 위해 고안된 Task이다. 참고로 Pre-Training task들은 연속적인 임베딩 공간(continuous embedding space)에서 graph-text alignment를 향상시키기 위함이다. 

논문에서 이 문제를 해결하기 위해 **Optimal Transport(OT)** 를 도입했다. OT는 **Cross-domain alignment** 문제를 해결하는데 자주 사용되며 <u>인코더로부터 얻은 graph representation과 디코더로부터 얻은 text representation의 minimum cost를 계산</u>하는 것을 목표로한다. 정리하자면, Graph-Text Embedding Alignment가 풀고자 하는 것은 결국
<span style = "color:green">**KG의 임베딩 벡터와 Text를 Optimal Transport를 통해 매칭**</span>시키는 것이다.

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228554333-1241813c-e18c-48cc-9290-2d8fcc91c13a.png">
</p>

1) Graph-Text Embedding Alignment의 전체적인 모델 아키텍쳐를 살펴보면, Input은 기존과는 다르게 Linearized된 KG인 $$\mathcal{G_{linear}}$$이다. 이는 총 m개의 토큰으로 구성 되어있는 시퀀스 벡터이다. 인코더를 거쳐서 나온 최종 결과값이 $$H^L$$이다. 인코더 내부에서 Pooling Layer를 거쳤을때도 마찬가지로 Graph Representation을 얻을 수 있다.

2) 또한 Entity Set과 Relation Set의 합집합을 새로운 시퀀스로 정의하고 $$\mathcal{G_{seq}}$$로 표기한다. 모든 엔티티와 릴레이션을 포함하므로 시퀀스의 총 토큰 수는 $$ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{E}$$ 개이다. 이 시퀀스 $$\mathcal{G_{seq}}$$로 인코더에 넣어 다이렉트하게 Contextual Embedding Vector를 얻을 수 있다.

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228559046-f8b276df-f353-47a4-9c49-caf3469d1909.png">
</p>

3) 디코더가 인코더의 출력값과 Text Sequence를 입력으로 받는다. 디코더의 최종 Hidden state로부터 $$X$$의 임베딩 벡터를 얻을 수 있고 이는 $$S = (s_1, s_2, \cdots, s_n)$$로 표기된다. $$X$$와 마찬가지로 $$S$$역시 시퀀스 벡터는 n개의 토큰으로 구성된다.

4) Graph-Text Alignment를 위해 확률분포를 정의해야 하는데, $$\mathcal{G_{seq}}$$와 $$X$$는 모두 이산분포를 따른다.

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228566848-e22ab1c5-dfed-4f6a-9869-3f4c0df2a676.png">
</p>

5) OT Distance를 이용해 Loss를 정의한다. $$T$$는 transport plane이고, $$1_n$$ 과 $$1_{ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{E}ㅣ}$$는 모든 요소가 1인 벡터이다.
$$n$$과 $$ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{E}ㅣ$$이 차원수를 의미한다. 

6) $$T$$를 추적하는 것은 불가능하다.(Intractable) 따라서, **IPOT** 알고리즘을 이용해서 최적화를 진행하며 Iteratively하게 $$T$$의 solution을 얻게된다. $$T$$를 찾고나면 비로서 $$\mathcal{L_{OT}}$$를 모델 파라미터를 최적화하는 alignment loss로써 역할을 할 수 있다. 이 Task가 하고자 하는 바는 결국 <span style = "color:red">**KG와 Text의 Contextual Embedding Vector를 연결하여 결과적으로 연속 공간에서 명시적인 Graph-Text alignment(그래프-텍스트 정렬)를 가능**</span>하게 한다.  

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228571165-a2d0c9ae-81e7-4f24-bebb-30ead8e55969.png">
</p>
<center><span style = "font-size:80%">Loss of Graph-Text Alignment</span></center>

<br/> 
<br/> 

# Experiment & Result

## 1. Dataset
- Pre-Training
  - KGTEXT를 이용하여 사전 학습을 진행.
  - 7M개의 Graph-Text data pair를 가진다.
  - 텍스트는 English Wikidump에서 얻었고, Wikidata로부터 그에 상응하는 KG를 얻었다.
  - Pre-trained 모델은 BART와 T5이다.

- Fine-tuning dataset
  - WebNLG(U)
  - WebNLG(C)
  - WebQuestions
  - PathQuestions   

<br/>

## 2. Fine-Tuning Settings

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228576977-31bf779a-257f-4e9b-87c3-9f1a36da525b.png">
</p>

- WebNLG
  - RDF Triple(Semantic Triple)을 Textual Description으로 변환하는 것을 목표로하는 데이터셋이다.

- WebQuestions
  - Knowledge bases question generation(KBQG)을 위한 Banchmark 데이터셋이다.
  - 이 것은 KG에 상응하는 자연어 질문을 만들어 내는 것을 목적으로 하는 task이다.

- PathQuestions
  - KGQG를 위한 Banchmark 데이터셋이다. 다만, Question-answering dataset으로부터 만들어졌다.
  - 가장 큰 차이점은 엔티티간의 거리가 2-hop 또는 3-hop path라는 것이다.

## 3. Result
### 1) Automatic Evaluation

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228577219-99015d39-6a6e-476b-8f28-ba365457f97b.png">
</p>

SOTA 달성

<br/>

### 2) Human Evaluation

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228577712-bc0d5876-b022-4187-93be-65e33ed16b44.png">
</p>

두 가지 기준을 가지고 Human Evaluation을 진행했다. fluency와 adequacy이다.
- fluency: 문장이 문법적으로 유창한지를 판단(whether a sentence grammatically fluent)
- adequacy: 문장이 Knowledge Graph를 명확하게 묘사하는지를 판단(whether a sentence clearly describes the knowledge graph)

랜덤하게 총 100개의 KG를 test set으로부터 샘플링하고, 가장 경쟁력이 있는 BART나 T5와 JointGT 모델로부터 나온 결과를 모아서 비교했다. 결론적으로 JointGT가 다른 Baseline모델들에 비해 더 좋은 fluency와 adequacy를 보여주었다.

<br/> 
<br/> 

## 4. Ablation Study
### 1) Encoder Structure

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228581624-2a744ef7-8b47-4eb7-932f-b7e8497b179f.png">
</p>

JointGT의 Structure-Aware Semantic Encoder 모듈의 효과를 비교하기위해 기존에 존재하던 Sequence 인코더와 Relation-Aware 인코더를 대채하여 실험을 진행하였다. Evaluation Metric은 BLEU, METEOR, ROUGE 총 세 가지를 사용하여 진행했다. 결론적으로 JointGT가 모든 평가지표에서 가장 높은 점수를 보였다.

또한 WebNLG(U) 데이터 셋에서 다른 Triple input을 넣어서 실험을 진행을 하였고, BLEU 스코어를 평가 지표로 삼았다. 이 역시 JointGT가 가장 좋은 결과가 나왔다. 특히, Input triple의 수가 커질수록 명백하게도 JointGT가 더 복잡한 KG구조에서도 인코딩을 잘 한다는 것을 알 수 있다.

<br/>

### 2) Pre-Training Task

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228583365-46def74d-65d7-40f7-9141-9639aff1208c.png">
</p>

<br/>

### 3) Few-Shot Learning

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228584577-c25853de-820c-46a5-9a42-ca53751dad7c.png">
</p>

Pre-Training data의 양이 작아도 JointGT가 가장 좋은 결과를 보인다. 위의 그림에서 빨간색과 파란색 글씨가 오답인 부분으로, JointGT는 하나도 없는 반면 BART나 T5는 신뢰할 수 없는 부분이 나온다.

<br/> 
<br/> 

# Contribution

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228584751-58b04ba6-a6b2-41b5-83f7-c41c326d5bcc.png">
</p>

1. KG-to-Text를 위한 새로운 Pre-trained Encoder Module을 제시하였다.
2. 여러가지 Banchmark dataset에 대해 SOTA달성

