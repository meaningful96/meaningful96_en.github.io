---
title: "[ë…¼ë¬¸ë¦¬ë·°]Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding"

categories: 
  - GR
  
toc: true
toc_sticky: true

date: 2023-09-20
last_modified_at: 2023-09-20
---

Zhang, H. (2023, May 17). *Investigating the effect of hard negative sample distribution on contrastive knowledge graph embedding*. ["arXiv.org. https://arxiv.org/abs/2305.10563"](https://arxiv.org/abs/2305.10563)

Knowledge Graph ë¶„ì•¼ì—ì„œ ["SimKGC"](https://meaningful96.github.io/paperreview/SimKGC/)ì™€ ê°™ì´ ìì—°ì–´ ê¸°ë°˜ì˜ ëª¨ë¸ì„ ì ìš©í•˜ê³ , negative samplingì„ í†µí•œ Contrastive learningì˜ ì—°êµ¬ê°€ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆë‹¤. 
ë³¸ ë…¼ë¬¸ì¸ Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding ì—­ì‹œ Knolwedge Graph Complitionì— Contrastive learningì„ ì ìš©í•œ ì—°êµ¬ì´ë©°, íŠ¹íˆ ê¸°ì¡´ì˜ negative sampling
ë°©ì‹ì—ì„œ ë²—ì–´ë‚˜, ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì ¼ ë¶„ì•¼ì—ì„œ ë§ì€ ì—°êµ¬ê°€ ì´ë£¨ì–´ì§€ê³  ìˆëŠ” <span style = "color:red"><b>Hard Negative Sampling</b></span>ë°©ì‹ì„ KGCì— ì„±ê³µì ìœ¼ë¡œ ì ìš©í–ˆë‹¤ëŠ” ê²ƒì´ ê°€ì¥ í° Contributionì´ë‹¤.

# Problem Statement

<span style="font-size:110%"><b>Gap between the negative samples and heuristic generation quality negative samples</b></span>  
Knowledge Graph Complition ë¶„ì•¼ì—ì„œë„ ìµœê·¼ë“¤ì–´ ê·¸ë˜í”„ì˜ Text descriptionì„ ì´ìš©í•˜ì—¬ ê·¸ë˜í”„ ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì— ê´€í•œ ë…¼ë¬¸ì´ ë§ì´ ë“±ì¥í•˜ì˜€ë‹¤. íŠ¹íˆ, SimKGCê°™ì´ ìì—°ì–´ ëª¨ë¸ê³¼ ë”ë¶ˆì–´ Contrastive learningì„ ì ‘ëª©í•˜ì—¬ banchmakr 
datasetì¸ WN18RRì—ì„œ SOTAë¥¼ ë‹¬ì„±í•œ ëª¨ë¸ë„ ë“±ì¥í–ˆë‹¤. Batchìˆ˜ë¥¼ ëŠ˜ë¦¬ê³ , negative samplingì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ generalí•œ ê·¸ë˜í”„ì—ì„œ Link predictionì— ì¢‹ì€ ì„±ì ì„ ë‚´ì§€ëŠ” ëª»í•œë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ ë¬¸ì œì ì„ <b>'Gap between the negative samples and heuristic generation quality negative samples'</b>ë¼ê³  ë§í•œë‹¤. ì¦‰, íŠ¹ì • classification taskì—ì„œ ì‚¬ìš©ë˜ëŠ” negative sampleê³¼ íŠ¹ì • heuristic ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ negative sampleì„ ë§í•œë‹¤. ì´ ë•Œ, randomí•˜ê²Œ ìƒì„±ëœ negative sampleê³¼ íŠ¹ì •í•œ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë§Œë“¤ì–´ì§„ negative sampleê°„ì— ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ”ë° ìˆì–´ì„œ random ìƒì„± negative sampleë“¤ì€ ì˜¤íˆë ¤ noiseë¡œ ì‘ë™í•  ìˆ˜ ìˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ ì´ìœ ë¡œ ê·¸ë˜í”„ ë‚´ì˜ **shortest path length**ë¥¼ ì´ìš©í•œ hard negative sampling ë°©ì‹ì„ ì œì•ˆí•œë‹¤.

<br/>
<br/>

# Related Work
## 1. Notation

Knowledge GraphëŠ” **Triple**ì´ë¼ëŠ” ë‹¨ìœ„ë¡œ ë°ì´í„°ê°€ ì €ì¥ëœë‹¤. GraphëŠ” $$\mathcal{G} \; = \; (\mathcal{E}, \mathcal{R}, \mathcal{T})$$ ë¡œ ì •ì˜í•œë‹¤. $$\mathcal{E}$$ì™€ $$\mathcal{R}$$ì€ ê°ê° Entity Setê³¼ Relation Setì„ ì˜ë¯¸í•˜ê³  $$\mathcal{T}$$ëŠ” Triple Setì„ ì˜ë¯¸í•œë‹¤. $$\mathcal{T} = {(h,r,t) \vert h,r \in \mathcal{E}, r \in \mathcal{R}}$$ì˜ ê´€ê³„ë¥¼ ê°€ì§„ë‹¤. ì¦‰, Tripleì€ headì™€ tailì˜ ê´€ê³„ë¥¼ relationìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë©°, ë…¼ë¬¸ì— ë”°ë¼ì„œ headë¥¼ subject, realtionì„ predicate ê·¸ë¦¬ê³  tailì„ objectë¡œ ê¸°ìˆ í•˜ê¸°ë„ í•œë‹¤. <span style = "color:red"><b>Knowledge Graph Completion (KGC)ì´ë¼ëŠ” ê²ƒì€ $$(h,r)$$ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê´€ê³„ì— ì•Œë§ëŠ” $$t$$ë¥¼ ì°¾ëŠ” ê²ƒ</b></span>ì´ ëª©í‘œì¸ taskì´ë‹¤.



## 2. InfoNCE Loss with Simple Negative Triples

InfoNCE LossëŠ” contrastive learningì„ í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ lossë¡œ, <u><b>Cross-Entropyê°€ InfoNCE Lossì˜ Special Case</b></u>ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Knowledge Graphì—ì„œ $$(h,r,t) \in \mathcal{T}$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ Kê°œì˜ negative sampleì„ ë…ë¦½ì ìœ¼ë¡œ ë™ì¼ ë¶„í¬ë‚´ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤. ì´ ë•Œ ë™ì¼ ë¶„í¬ì— ëŒ€í•œ í™•ë¥  ê°’ì„ $$p^{-}(t)$$ë¡œ í‘œí˜„í•˜ê³  negative sampleì„ $$(h,r,t^{'})$$ë¡œ í‘œí˜„í•œë‹¤. ì´ ë•Œ Original InfoNCE LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/e043c892-69a3-4b3f-8408-d491fbbe2884">
</p>

ì´ ë•Œ, <b>$$e_{t_j}^{-}$$</b>ëŠ” negative tail $$t_j^{-}$$ì˜ ì„ë² ë”©ì´ë‹¤. ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì„ í†µí•´ **InfoNCE loss**ëŠ” *log Bayesian* ëª¨ë¸ê³¼ ë™ì¼í•˜ë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ lossì‹ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤. $$\mathcal{T_{batch}} \subseteq \mathcal{T}$$ì´ë©°, \#$$(t)$$ê°€   $$\mathcal{T_{batch}}$$ì—ì„œ ë°œìƒí•œ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.

<p align="center">
<img width="200" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/c4c37594-61d9-4ac2-9499-d4a59154dd51">
</p>

ì´ ë•Œ $$p^{-}(t)$$ëŠ” negative sampleì˜ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë¶„í¬ëŠ” simple distributionìœ¼ë¡œ ë‹¨ìˆœíˆ batchì•ˆì—ì„œ negative sampleì´ **ë°œìƒ íšŸìˆ˜**ë§Œì„ ë”°ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê¸°ì¡´ì˜ SimKGCëŠ” ì´ì²˜ëŸ¼ batchì•ˆì—ì„œ ë‹¨ìˆœí•œ ë°©ì‹ìœ¼ë¡œ Negative samplingì„ ì§„í–‰í•˜ì˜€ë‹¤.

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/c772f3a7-a952-4767-aab0-20bb63ed68f8">
</p>


<br/>
<br/>

# Method
## 1. InfoNCE Loss with Hard Negative Triples
> Hard negatives triples are harder to distinguish from the triples in the KG than arbitrarily generated negative samples.
> One way to generate hard negative triples is to sample the tail entity from a negative sample distribution that also considers the context.

KGE ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ì¢…ì¢… heuristicsë¥¼ ì´ìš©í•´ hard negativeë¥¼ ìƒì„±í•´ë‚¸ë‹¤. ì´ëŸ° hard negativeë¥¼ í†µí•´ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•¨ì´ë‹¤. Hard negative tripleë“¤ì€ <span style = "color:red"><b>ì‹¤ì œ ì •ë‹µê³¼ êµ¬ë¶„í•˜ê¸° í˜ë“ </b></span> negative tripleì´ë‹¤. ë‹¤ì‹œ ë§í•´, $$(h,r)$$ì´ ì£¼ì–´ì¡Œì„ ë•Œ <u>ì •ë‹µ tailê³¼ ê°€ê¹Œìš´ ì—”í‹°í‹°ë“¤ë¡œ ì´ë£¨ì–´ì§„ triple</u>ì´ë‹¤. ì´ëŸ¬í•œ hard negativeë¥¼ ì¶”ì°°í•˜ëŠ” ë°©ë²• ì¤‘ í•œ ê°€ì§€ëŠ” ì•ì„œ ì–¸ê¸‰í•œ randomí•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ì‹ì€ ë„ˆë¬´ ë‹¨ìˆœí•˜ê¸°ì— ë‹¤ë¥¸ samplingë°©ì‹ì„ ì œì•ˆí•œë‹¤. ë°”ë¡œ tail ì—”í‹°í‹°ë¥¼ negative sampling í•˜ë˜, **contextë¥¼ ê³ ë ¤í•´ì„œ ì¶”ì¶œ**í•˜ìëŠ” ê²ƒì´ë‹¤.(ì¼ì¢…ì˜ Context Sub-Graph) ì •í™•í•˜ê²Œ ë§í•˜ë©´, <span style = "color:red">**Structural Context information**</span>ì„ ì‚¬ìš©í•˜ì—¬ negative tailì„ ë½‘ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜í”„ì˜ êµ¬ì¡°ì ì¸ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬, Training setìœ¼ë¡œ ë“¤ì–´ì˜¨ tripleì˜ tailì„ hopìˆ˜ì— dependentí•˜ê²Œ ë°”ê¿”ê°€ë©° negativeë¥¼ ì£¼ëŠ” ë°©ì‹ì´ë‹¤.

<p align="center">
<img width="550" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b081b775-6a8e-4dcd-88ce-347ae04c6d34">
</p>

ì´ì™€ ê°™ì€ negative samplingë°©ì‹ì„ InfoNCE Lossì— ì ìš©í•˜ë©´ ìœ„ì™€ ê°™ì´ ì‹ì´ ë³€í˜•ëœë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” hard negative tripleì„ ë§Œë“¤ ë•Œ Shortest pathë¥¼ ê³ ë ¤í•˜ì˜€ê³ , ì´ë¥¼ ìœ„í•´ ìƒˆë¡œìš´ í™•ë¥  ë¶„í¬ë¥¼ ì •ì˜í•˜ì˜€ë‹¤. ë‹¤ì‹œ ë§í•´, ì •ë‹µ triple(Ground Truth)ì˜ hr ì„ë² ë”© $$e_{hr}$$ì— ëŒ€í•œ ì—¬ëŸ¬ tail(candidate entities)ì˜ preferenceë¥¼ êµ¬í•˜ê³ , ê·¸ ì„ë² ë”©ì´ ê°€ê¹Œìš°ë©´(ë¬¸ë§¥ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ìˆìœ¼ë©´) ë” ë†’ì€ preferenceë¥¼ ë¶€ì—¬í•œë‹¤. ì´ ë°©ì‹ì€ KBGANê³¼ RotatEì™€ ìœ ì‚¬í•˜ë‹¤.

<p align="center">
<img width="350" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/ae117ae5-6046-4ab4-b0d3-83bb3086b465">
</p>

> Generate hard negative triples by giving higher preference to the tail entities whose embeddings
> are close to the context embedding
> 
> Using structural context information, extract negative tails (h,r,t) => (h,r,t')

í•™ìŠµì„ ì‹œì‘í•  ë•ŒëŠ”, ì„ë² ë”©ê³¼ aggregation functionì— ëŒ€í•´ ë…¼ë¬¸ì—ì„œëŠ” <b>Sentence-BERT</b>ë¥¼ ì´ìš©í•´ $$e_{hr}, e_t, e_t^{-}$$ë¥¼ initializationí•˜ì˜€ë‹¤. ì•ì—ì„œ ì œì‹œí•œ Preferenceì— ê·¼ê±°í•´ ë½‘ì€ Kê°œì˜ negative sampleë“¤ì„ ì¶”ì¶œí•˜ê³  ê·¸ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/263e4e45-e4d8-49d6-b6fd-15c67e51f921">
</p>

<span style = "font-size:110%"><b>Proposition 1.</b></span> Hard negativeë¥¼ ì ìš©í•œ InfoNCE lossëŠ” Joint distribution $$p(e_{hr}, e_t)$$ ì™€ $$p(e_{hr}, e_t^{-})$$ ì‚¬ì´ì˜ Kullback-Leiber divergenceì— lower boundë¥¼ ì¤€ë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ ê·¼ê±°ë“¤ì„ ì´ìš©í•´ InfoNCE lossë¥¼ minimizationí•˜ë©´ True Tripleê³¼ Hard negativeì™€ì˜ ì„ë² ë”©ì´ ë‹¤ë¥´ê²Œ ë¶„í¬í•˜ê²Œ ë  ê²ƒì´ë¼ ì˜ˆì¸¡í•˜ì˜€ë‹¤. 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/6dc32c3a-316b-4b38-a83d-a79cb0e32274">
</p>

## 2. Hard Negative Triple may be False Negative Triple

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/1331b6df-d6b1-440a-92a2-84d43ee5cae8">
</p>

False negativeë€ ì‹¤ì œë¡œëŠ” ì •ë‹µì¸ë° ê±°ì§“ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²ƒì„ ë§í•œë‹¤. Graphì˜ êµ¬ì¡° íŠ¹ì„±ìƒ í•˜ë‚˜ì˜ ì—”í‹°í‹°ì— ì—¬ëŸ¬ ê°œì˜ ì´ì›ƒì´ ìˆê¸° ë•Œë¬¸ì— ì‹¤ì œ ì •ë‹µ Tripleì— ëŒ€í•´ì„œ headì˜ ë‹¤ë¥¸ ì´ì›ƒì„ tailë¡œ ëª¨ë¸ì´ ì˜ëª» ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì´ì²˜ëŸ¼ headì˜ ì§ì ‘ì ì¸ ì´ì›ƒì´ hard negativeì´ë©°, ì´ hard negativeë“¤ì„ ê²°êµ­ false negativeê°€ ë  í™•ë¥ ì´ ë†’ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ False negativeì˜ ìˆ˜ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ Banchmark dataset FB15k-237ê³¼ WN18RRì„ í†µí•´ì„œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. ì„ì˜ë¡œ ëœë¤í•˜ê²Œ 30%ì˜ íŠ¸ë¦¬í”Œì„ train setì—ì„œ ì§€ìš°ê³  ì´ setì„ $$\mathcal{T_{missing}}$$ìœ¼ë¡œ ì •ì˜í•˜ì˜€ê³ , ì›ë˜ì˜ setì„ $$\mathcal{T_{retain}}$$ìœ¼ë¡œ ì •ì˜í–ˆë‹¤. 

retain setì—ì„œ Kê°œì˜ negative sampleì„ ì•ì„œ ì œì‹œí•œ í™•ë¥  ë¶„í¬ë¥¼ ì´ìš©í•´ samplingí•˜ì˜€ê³ , ë§Œì•½ negative tripleì´ê³  $$\mathcal{T_{missing}}$$ setì— ì¡´ì¬í•˜ëŠ” Tripleì´ë©´ ì´ tripleì€ ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ëŠ” tripleì´ë©° ì´ ê²ƒì´ ë°”ë¡œ <span style = "color:red"><b>False negative</b></span>ê°€ ë˜ëŠ” ê²ƒì´ë‹¤. False Tripleì´ missing setì—ì„œ ë°œê²¬ì´ ë˜ì§€ ì•Šìœ¼ë©´ True negativeì´ë‹¤. ê° Datasetì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ batch ì‚¬ì´ì¦ˆë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ê³ , KëŠ” hyperparamterë¡œ $$K = 5 \vert \mathcal{T_{batch}} \vert - 1$$ë¡œ ì •ì˜í•˜ì˜€ë‹¤. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/8f9326e8-2bb8-4e53-bbb6-7299f6d1fb27">
</p>

ë‹¤ì‹œ ê°•ì¡°í•˜ìë©´, $$p^{-}(t)$$ëŠ” random negative samplingì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì´ê³ , ìœ„ì˜ ì‹¤í—˜ ê²°ê³¼ì—ì„œ <span style= "color:green">**íŒŒë€ìƒ‰**</span>ì´ê³  ê²°ë¡ ì ìœ¼ë¡œ False negativeê°€ ë” ì ë‹¤. ë°˜ë©´, Hard negativeëŠ” $$p^{-}(t \vert e_{hr})$$ì˜ ë¶„í¬ë¥¼ ë”°ë¼ negativeë¥¼ samlingí•œ ê²ƒì´ê³ , ì‹¤í—˜ ê²°ê³¼ëŠ” <span style = "colog:lime">**ì´ˆë¡ìƒ‰**</span>ì´ë‹¤. ëœë¤ ìƒ˜í”Œë§ ë°©ì‹ê³¼ ë¹„êµí–ˆì„ ë•Œ ë§¤ìš° ë§ì€ False negativeë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

## 3. Shortest Path Length Distinguishes True and False Negative Triple

Context informationì„ ì‚¬ìš©í•˜ê³ , BERTê°™ì€ ìì—°ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì–‘ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë¯€ë¡œ, Knowledge Graphë¥¼ ë¨¼ì € Undirectedí•˜ë©° Unweighted Graphë¡œ ê°€ì •í•˜ê³  ì‹¤í—˜ì„ ì§„í–‰í•œë‹¤. ì´ ë•Œ false negativeë“¤ì´ ì‹¤ì œë¡œ headë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€, shortest pathë¥¼ êµ¬í•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•´ ê° data setë³„ë¡œ í‰ê· ì ìœ¼ë¡œ ëª‡ hopì—ì„œ false negativeê°€ ë§ì€ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/0ff03687-0305-4620-bc37-ee4bbdcef41a">
</p>

## 4. Debiased InfoNCE Loss with Hard Negative Triples
ê¸°ì¡´ì— ì§„í–‰ë˜ì—ˆë˜ contrastive learning ì—°êµ¬ì—ì„œ, debiased contrastive lossì— ëŒ€í•œ ì—°êµ¬ê°€ ìˆë‹¤. ì´ lossëŠ” InfoNCE lossì— false negativeê¹Œì§€ ê³ ë ¤í•´ ì¢€ ë” ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©í–¥ì„±ì„ ì œì‹œí•´ ì¤€ ì—°êµ¬ì´ë‹¤. ì´ LossëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œëŠ” ì–´ë–¤ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œì´ ë‹¤ë¥¸ í´ë˜ìŠ¤ë³´ë‹¤ í›¨ì”¬ ë§ê±°ë‚˜ ì ì€ ê²½ìš° ë°œìƒí•œë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ëª¨ë¸ì´ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ë¬´ì‹œí•˜ê³  í•­ìƒ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. Debiased constrastive lossëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì™„í™”í•˜ê³  ëª¨ë¸ì„ ë” ê· í˜• ì¡íŒ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ, Hard negative sampleì— ëŒ€í•´ fatualí•˜ëƒ non-factualí•˜ëƒì— ë”°ë¼ latent labelì„ ë¶€ì—¬í•´ ì´ Lossë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ì •ì˜í•œ Lossë¥¼ ë…¼ë¬¸ì—ì„œëŠ” $$\mathcal{L_{HaSa}}$$ë¼ê³  ì •ì˜í•˜ì˜€ê³ , ì¢€ ë” í’€ì–´ì„œ ì„¤ëª…í•˜ìë©´ <span style = "colog:gold"><b>Debiased InfoNCE with Hard Negative triple</b></span>ì´ë©°, ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ìµœì¢… ëª¨ë¸ì„ ì´ ì‹ì„ ë”°ë¼ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/51a637a2-b9b0-4e84-a80a-8cd607c12795">
</p>

ì•ì„  ì‹¤í—˜ ê²°ê³¼ë“¤ì„ í† ëŒ€ë¡œ, Negative samplingì„ í•˜ëŠ”ë° ê¸°ì¡´ê³¼ëŠ” ë‹¤ë¥¸ ìƒˆë¡œìš´ Lossë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ í™•ë¥  ë¶„í¬ì— ë”°ë¼ negative samplingì„ í•´ì•¼í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ <span style = "colog:gold">**hopìˆ˜ì— ëŒ€í•´ ìƒˆë¡œìš´ í™•ë¥ ê°’**</span>ì„ ì£¼ì–´ì„œ negative samplingì„ ì§„í–‰í•˜ì˜€ë‹¤. ì¡°ê¸ˆì€ naiveí•œ ë°©ì‹ìœ¼ë¡œ í™•ë¥ ì„ ì£¼ì—ˆëŠ”ë°, ë°”ë¡œ ê¸°ì¤€ì´ ë˜ëŠ” headë¡œë¶€í„° 1-hopê³¼ 2-hop ê±°ë¦¬ì— ìˆëŠ” ì—”í‹°í‹°ë“¤ì˜ ê°œìˆ˜ì˜ ì—­ìˆ˜ë¡œ í™•ë¥ ì„ ë¶€ì—¬í•˜ì˜€ë‹¤. ì—¬ê¸°ì„œ ì ì‹œ ìƒê°í•´ë³¼ ì ì€ 'ì™œ ê°œìˆ˜ì˜ ì—­ìˆ˜ë¡œ í™•ë¥ ì„ ë¶€ì—¬í–ˆëŠ”ê°€?'ì´ë‹¤.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/57e72198-340d-4099-9546-5213a474baa0">
</p>

**3ë²ˆ**ì‹ì„ ì‚´í´ë³´ë©´ Hard negative samplingì„ ìœ„í•œ ìƒˆë¡œìš´ precisionì‹ì„ ì œì‹œí•œë‹¤. ì—¬ê¸°ì„œ $$\alpha(t \vert e_{hr})$$ì´ ë°”ë¡œ <u>ê±°ë¦¬ì— ëŒ€í•œ í™•ë¥  ë¶„í¬</u>ê°’ì´ë‹¤. ë§Œì•½ ê±°ë¦¬ì— ëŒ€í•´ ë¹„ë¡€í•´ì„œ í™•ë¥  ê°’ì„ ë¶€ì—¬í•˜ë©´, shortest pathê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬ë°›ê²Œ ë˜ê³  ì´ëŠ” Hard negativeì™€ëŠ” ê±°ë¦¬ê°€ ë©€ì–´ì§„ë‹¤. ë”°ë¼ì„œ ê±°ë¦¬ì— ëŒ€í•´ ë°˜ë¹„ë¡€í•˜ê²Œ í™•ë¥ ê°’ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ë‹¤. ìµœì¢…ì ìœ¼ë¡œ **5ë²ˆ**ì‹ê³¼ ê°™ì´ negative samplingì— ëŒ€í•œ ê¸°ëŒ“ê°’ì´ ë§Œë“¤ì–´ì§„ë‹¤. **2ë²ˆ**ê³¼ **5ë²ˆ**ì‹ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆì§€ë§Œ, non-factualí•œ ê°’ì— ëŒ€í•˜ì—¬ **Law of Total Expectation**ì„ ì´ìš©í•œë‹¤. ê·¸ ì´ìœ ëŠ”, Negative sampleì„ ë½‘ì„ ë•Œ ì§ì ‘ì ìœ¼ë¡œ directí•˜ê²Œ ê·¸ negativeê°€ fatualí•œì§€ ì•„ë‹ˆì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— í™•ë¥ ê°’ì„ $$Negative(N) = 1 - Positive(P)$$ë¡œ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ë‹¤.

## 5. Improved HaSa: Hasa+

HaSaë³´ë‹¤ negativeë¥¼ ì¢€ ë” ì–´ë µê²Œ ì£¼ê¸° ìœ„í•´ì„œ tailë§Œ corruptioní•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ($$h,r,t$$)ë¥¼ ëª¨ë‘ corruptioní•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰ HaSaì—ì„œëŠ” negative sampleë¡œ ($$h,r,t^{-}$$)ë¥¼ ë½‘ì•˜ë‹¤ë©´, **HaSa+**ì—ì„œëŠ” ($$h^{-},r^{-},t^{-}$$)ë¥¼ ë½‘ëŠ” ê²ƒì´ë‹¤.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/04dbb223-15db-4906-bddd-53d5fa584242">
</p>

<br/>
<br/>

# Experiment

## 1. Dataset

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/ffd2dc0d-9b2d-47fb-b85a-e2fc53d6efbe">
</p>

Datasetì€ Banch markì¸ **WN18RR**ê³¼ **FB15k-237**ì„ ì‚¬ìš©í–ˆìœ¼ë©°, Evaluation metricìœ¼ë¡œëŠ” **MR(Mean Rank)**, **MRR(Mean Reciprocal Rank)**ê³¼ **Hit@N**ì„ ì‚¬ìš©í–ˆë‹¤. 

## 2. Training Process
- Embedding function $$f (\cdot)$$ì„ sentence-BERTë¥¼ í†µí•´ ë§Œë“¤ë©°, sentence-BERTë¥¼ í†µí•´ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ë•Œ ì°¨ì›ì„ 500(d = 500)ê¹Œì§€ ì¤„ì—¬ì„œ ë§Œë“ ë‹¤.
- Aggregation function $$g(\cdot)$$ì€ Gated Recurrent unit (GRU)ë¡œ ì‚¬ìš©í•œë‹¤.
- Optimizationì€ PyTorch AdamWë¥¼ ì´ìš©, learning rate(lr) = $$2 \times 10^{-5}$$
- í•™ìŠµ ì‹œ batch sizeëŠ” $$\mathcal{T_{batch}} = 256$$ìœ¼ë¡œ ë‘”ë‹¤.
- Negative sample ìˆ˜ $$K$$ëŠ” $$K = 5 (\vert \mathcal{T_{batch}} \vert) -1$$ë¡œ ë‘”ë‹¤.

## 3. Result

### 1) Comparing InfoNCE with Simple Negative Samples and Hard Negative Samples

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/4b7416e9-cb93-4518-a8dd-98a0b1eaaa58">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/82ecf7e1-c13c-40cb-8477-7e5c2aba79ca">
</p>

ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ evaluation metricì—ì„œ hard negativeë¥¼ ì´ìš©í•´ samplingí•  ê²½ìš° ì„±ëŠ¥ì´ ë” ì˜¬ë¼ê°„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. Plotì€ Epochë³„ MRRê³¼ Hit@1ì„ ë‚˜íƒ€ë‚¸ë‹¤.

<br/>

### 2) Comparing HaSa and HaSa+ to Other KGE Methods

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/fa08122b-ea72-4f52-987b-0dfcc0df8df6">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/c8dd257e-c5c3-4e9c-87dc-e218b988c2a9">
</p>

HaSa+ê°€ HaSaë³´ë‹¤ ì¢€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. t-SNEë¥¼ í†µí•´ì„œ ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ ì£¼ì–´ì§„ True headì™€ relationì— ëŒ€í•´ Positive tailì´ ì„ë² ë”© ê³µê°„ì—ì„œ ê°™ì´ clusteringë˜ì–´ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤.

<br/>

### 3) Effect of Hyperparameter $$\tau$$

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/c3d3e8f3-6749-4066-831f-b285243228ce">
</p>

- **ğ›•** ì¦ê°€
    - Distributionì´ smoothí•´ì§. Positive-Negative ê°„ ìœ ì‚¬ë„ ì°¨ì´ê°€ ê°ì†Œí•œë‹¤.  ë” í° ***Ï„***ëŠ” ëª¨ë¸ì´ ë” ë¶€ë“œëŸ¬ìš´ ê²°ì •ì„ ìƒì„±í•˜ê²Œ ë§Œë“¤ë©°, í›ˆë ¨ì´ ë” ì‰¬ì›Œì§€ì§€ë§Œ ìœ ìš©í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ”ë° ë°©í•´ê°€ ë  ìˆ˜ ìˆë‹¤.
    - **ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬**: ë†’ì€ *Ï„* ê°’ì€ í™•ë¥  ë¶„í¬ë¥¼ ë” ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ë©°, ëª¨ë¸ì€ positiveì™€ negative ìƒ˜í”Œ ê°„ì˜ ì°¨ì´ì— ëœ ë¯¼ê°í•˜ê²Œ ëœë‹¤.
    - **ë”ìš± ì¼ë°˜ì ì¸ í•™ìŠµ**: ë†’ì€ *Ï„* ê°’ì€ í•™ìŠµì´ ë”ìš± ë¶€ë“œëŸ½ê³ , ì¼ë°˜ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê²Œ í•œë‹¤.
    - This smoothens the distribution, decreasing the difference between the similarities of positive and negative samples. A larger ***Ï„*** makes the model produce softer decisions, making the training potentially easier but possibly less discriminative as it wonâ€™t differentiate as strongly between positive and negative pairs.
- **ğ›•** ê°ì†Œ (for hard negative â†’ `Inv_T` ëŠ” ì¦ê°€)
    - Distributionì´ sharpí•´ì§. Positive-Negative ê°„ì˜ ìœ ì‚¬ë„ ì°¨ì´ê°€ ì»¤ì§„ë‹¤. ë” ì‘ì€ ***Ï„*ëŠ”** ì •í™•í•œ ì¼ì¹˜ì— ëŒ€í•œ ì¤‘ìš”ë„ë¥¼ ë†’ì´ê³ , í›ˆë ¨ì„ ë”ìš± ì§‘ì¤‘ì‹œí‚¤ì§€ë§Œ, ëª¨ë¸ì´ ë” ëª…í™•í•œ ê²°ì •ì„ ë‚´ë¦¬ë„ë¡ ê°•ìš”í•˜ê¸° ë•Œë¬¸ì— í›ˆë ¨ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆë‹¤.
    - **ë‚ ì¹´ë¡œìš´ í™•ë¥  ë¶„í¬**: ë‚®ì€ *Ï„* ê°’ì€ ëª¨ë¸ì´ positive ìƒ˜í”Œê³¼ negative ìƒ˜í”Œ ê°„ì˜ ì°¨ì´ë¥¼ ë”ìš± ëª…í™•í•˜ê²Œ ì¸ì‹í•˜ê²Œ ë§Œë“ ë‹¤.
    - **ë” ë¹ ë¥¸ ìˆ˜ë ´**: *Ï„* ê°’ì„ ë‚®ì¶”ë©´ ëª¨ë¸ì´ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
    - **ê°•ë ¬í•œ ì—…ë°ì´íŠ¸**: ë‚®ì€ *Ï„* ê°’ì€ ëª¨ë¸ì´ ì„ íƒì ìœ¼ë¡œ, ë”ìš± ê°•ë ¬í•˜ê²Œ ì—…ë°ì´íŠ¸ë‹¤.
    - This makes the distribution sharper, amplifying the difference between the similarities of positive and negative samples. A smaller ***Ï„*** leads to higher importance to exact matches and making the training more focused but potentially harder, as the model is forced to make more distinct decisions.
    - If *Ï„*Â is small, the softmax function becomes sharper, meaning that even small differences in the similarity scores will result in a larger difference after the softmax
