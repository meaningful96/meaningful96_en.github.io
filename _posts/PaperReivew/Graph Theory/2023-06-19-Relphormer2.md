---
title: "[ë…¼ë¬¸ë¦¬ë·°]Relphormer: Relational Graph Transformer for Knowledge Graph Representation"

categories: 
  - GR
  
toc: true
toc_sticky: true

date: 2023-07-10
last_modified_at: 2023-07-10
---

Bi, Z. (2022, May 22). *Relphormer: Relational Graph Transformer for Knowledge Graph Representations*. arXiv.org. https://arxiv.org/abs/2205.10852

ì´ë²ˆ í¬ìŠ¤íŒ…ì€ 3ì›” 14ì¼ í¬ìŠ¤íŒ…ëœ ["Relational Graph Transformer for Knowledge Graph Completion-Relphormer"](https://meaningful96.github.io/gr/Relphormer/)ì˜ ì—…ë°ì´íŠ¸ ë²„ì „ì´ë‹¤. ë…¼ë¬¸ ë²„ì „ì´ ìˆ˜ì •ë˜ë©´ì„œ Ablation Studyê°€ ì¶”ê°€ë˜ì—ˆë‹¤.

# Problem Statement

ì¼ë°˜ì ì¸ ê·¸ë˜í”„ì™€ëŠ” ë‹¤ë¥´ê²Œ Knowledge GraphëŠ” ë…¸ë“œ ë˜ëŠ” ë¦´ë ˆì´ì…˜ì˜ íƒ€ì…ì´ ì—¬ëŸ¬ê°€ì§€ì¸ Heterogeneous Graphì´ë‹¤. ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ Transformerê°€ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©´ì„œ Computer Visionë“±ì˜ ì—¬ëŸ¬ ë¶„ì•¼ì— ì ‘ëª©í•˜ë ¤ëŠ” ì‹¤í—˜ì´ ì§„í–‰ë˜ëŠ” ì¤‘ì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ Transformerëª¨ë¸ì´ Knowledge Graphì—ë„ ì ìš©í•˜ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆë‹¤.

TransformerëŠ” ê·¸ë˜í”„ì— ì ìš©í•˜ë©´(i.e., KG-BERT) ëª¨ë“  ë…¸ë“œë“¤ì˜ Attentionì„ í†µí•´ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. í•˜ì§€ë§Œ, ì´ëŸ´ ê²½ìš° ê·¸ë˜í”„ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ ì¤‘ í•˜ë‚˜ì¸ <span style="color:red">**êµ¬ì¡° ì •ë³´(Structural Information)**</span>ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 3ê°€ì§€ ë¬¸ì œì ì„ ì œì‹œí•œë‹¤.

<span style ="font-size:110%"><b>1. Heterogeneity for edges and nodes</b></span>      
ë¨¼ì € **Inductive Bias**ë¼ëŠ” ê°œë…ì„ ì•Œì•„ì•¼í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì´ ê°–ëŠ” ì¼ë°˜í™”ì˜ ì˜¤ë¥˜ëŠ” ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒ(**Brittle**)ê³¼ ê²‰ìœ¼ë¡œë§Œ ê·¸ëŸ´ì‹¸ í•´ ë³´ì´ëŠ” ê²ƒ(**Spurious**)ì´ ìˆë‹¤. ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œ ì˜ ì¼ë°˜í™”í•œ ê²ƒì¸ì§€, í˜¹ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ë§Œ ì˜ ë§ê²Œ ëœ ê²ƒì¸ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¬¸ì œì´ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì´ ë°”ë¡œ Inductive Biasì´ë‹¤. **Inductive Bias**ë€, <u>ì£¼ì–´ì§€ì§€ ì•Šì€ ì…ë ¥ì˜ ì¶œë ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì¼ë°˜í™”ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œ ë§Œì•½ì˜ ìƒí™©ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê°€ì •(Additional Assumptions)ì´ë¼ê³  ë³´ë©´ ëœë‹¤.</u> 

- Models are Brittle: ì•„ë¬´ë¦¬ ê°™ì€ ì˜ë¯¸ì˜ ë°ì´í„°ë¼ë„ ì¡°ê¸ˆë§Œ ë°”ë€Œë©´ ëª¨ë¸ì´ ë§ê°€ì§„ë‹¤.
- Models are Spurious: ë°ì´í„°ì˜ ì§„ì •í•œ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì§€ ëª»í•˜ê³  ê²°ê³¼(Artifacts)ì™€ í¸í–¥(Bias)ì„ ì•”ê¸°í•œë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” <b>ê¸°ì¡´ì˜ Knowledge Graph Transformerê°€ í•¨ì¶•ì ì¸ Inductive Biasë¥¼ ì ìš©</b>í•œë‹¤ê³  ë§í•œë‹¤. ì™œëƒí•˜ë©´ KG-BERTì˜ ê²½ìš° ì…ë ¥ì´ **Single-Hop Triple**ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ´ ê²½ìš° 1-hop ì •ë³´ë§Œ ë°›ì•„ê°€ë¯€ë¡œ <span style = "color:red">**Knowledge Graphì— êµ¬ì¡°ì ì¸ ì •ë³´ë¥¼ ë°˜ì˜í•˜ëŠ”ë° ì œì•½**</span>ì´ ëœë‹¤.

<br/>

<span style ="font-size:110%"><b>2. Topological Structure and Texture description</b></span>        
1ë²ˆ ë¬¸ì œì™€ ë¹„ìŠ·í•œ ë¬¸ì œì´ë‹¤. ê¸°ì¡´ì˜ Transformer ëª¨ë¸ì€ ëª¨ë“  Entityì™€ Relationë“¤ì„ plain tokenì²˜ëŸ¼ ë‹¤ë£¬ë‹¤. í•˜ì§€ë§Œ Knowledge Graphì—ì„œëŠ” ì—”í‹°í‹°ê°€ **ìœ„ìƒ êµ¬ì¡°(Topological Structure) ì •ë³´ì™€ ë¬¸ë§¥(Text Description) ì •ë³´**ì˜ ë‘ ìœ í˜•ì˜ ì •ë³´ë¥¼ ê°€ì§€ë©° TransformerëŠ” ì˜¤ì§ Text descriptionë§Œì„ ì´ìš©í•´ ì¶”ë¡ (Inference)ë¥¼ ì§„í–‰í•œë‹¤. ì¤‘ìš”í•œ ê²ƒì€ **ì„œë¡œ ë‹¤ë¥¸ ì—”í‹°í‹°ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìœ„ìƒ êµ¬ì¡° ì •ë³´ì„ ê°€ì§„ë‹¤**. ë”°ë¼ì„œ, ë§ˆì°¬ê°€ì§€ë¡œ ê²°êµ­ ê¸°ì¡´ì˜ <span style="color:red">**Knowledge Graph Trnasformer ëª¨ë¸ë“¤ì€ í•„ìˆ˜ì ì¸ êµ¬ì¡° ì •ë³´ë¥¼ ìœ ì‹¤**</span>ì‹œí‚¨ë‹¤.

<span style="font-size:120"><b>âœ How to treat heterogeneous information using Transformer architecture?</b></span>

<br/>

<span style ="font-size:110%"><b>3. Task Optimization Universalty</b></span>    
Knowledge GraphëŠ” ê¸°ì¡´ì— ë³´í†µ Graph Embedding ëª¨ë¸ë“¤ì— ì˜í•´ taskë¥¼ í’€ì—ˆë‹¤. í•˜ì§€ë§Œ ì´ ê¸°ì¡´ì˜ ë°©ì‹ë“¤ì˜ ë¹„íš¨ìœ¨ì ì¸ ë©´ì€ ë°”ë¡œ Taskë§ˆë‹¤ ì‚¬ì „ì— Scoring functionì„ ê°ê° ë‹¤ë¥´ê²Œ ì •ì˜í•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë‹¤ë¥¸ <span style="color:red">**Task objectë§ˆë‹¤ ë‹¤ë¥¸ Scoring functionì„ í•„ìš”**</span>ë¡œ í•˜ê¸° ë•Œë¬¸ì— ë¹„íš¨ìœ¨ì ì´ë‹¤. ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì„ ë‹¤ì–‘í•œ Taskì— ëŒ€í•´ í†µì¼ëœ representationì„ ì œì‹œí•˜ì§€ ëª»í•œë‹¤.

<span style="font-size:120"><b>âœ How to unite Knowledge Graph Representation for KG-based tasks?</b></span>


<br/>
<br/>

# Related Work

<span style = "font-size:110%"><b>Knowledge Graph Embedding</b></span>  
KG Representation Learningì€ <b>ì—°ì†ì ì¸ ì €ì°¨ì›ì˜ ë²¡í„° ê³µê°„ìœ¼ë¡œ ì—”í‹°í‹°ì™€ ë¦´ë ˆì´ì…˜ë“¤ì„ projectioní•˜ëŠ” ê²ƒì„ íƒ€ê²Ÿ</b>ìœ¼ë¡œí•œë‹¤. TransE, TransR, RotatEë“±ì˜ ëª¨ë¸ë“¤ì´ ì¡´ì¬í•œë‹¤. í•˜ì§€ë§Œ ì•ì„œ ë§í–ˆë“¯, ì„œë¡œ ë‹¤ë¥¸ Taskë“¤ì— ëŒ€í•´ ì‚¬ì „ì— ì •ì˜ëœ Scoring functionì„ í•„ìš”ë¡œ í•œë‹¤ëŠ” ë¹„íš¨ìœ¨ì„±ì´ ì¡´ì¬í•œë‹¤.  

<span style = "font-size:80%">ì°¸ê³ : [Knowledge Graph Completion](https://meaningful96.github.io/graph/cs224w-10/)</span>

<br/>
<br/>

# Method

## 1. Overview

### 1) Model Architecture

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/c73fd70f-1111-48c7-b87f-5c205fa4e9ec">
</p>

1) **Triple2Seq**: ì—”í‹°í‹°ì™€ ë¦´ë ˆì´ì…˜ì˜ ë‹¤ì–‘ì„±(Heterogeneity)ë¥¼ ëŒ€ì‘í•˜ê³  ëª¨ë¸ì˜ Input Sequenceë¡œì„œ Contextual Sub-Graphë¥¼ Samplingí•œë‹¤.(Dynamic Sampling)
2) **Structured-Enhanced Mechanism**: Structural Informationê³¼ Textual Informationì„ ë‹¤ë£¨ê¸° ìœ„í•¨
3) **Masked Knowledge Modeling**: KG Representation Leanrningì˜ Taskë“¤ì„ í†µí•©

<br/>

### 2) Preliminaries & Notations

Knowledge GraphsëŠ” triple($$head, relation, tail$$)ë¡œ êµ¬ì„±ëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” **Knowledge Graph Completion** Taskì™€ **Knowledge Graph-Enhanced Downstream Task**ë¥¼ í‘¸ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ëª¨ë¸ì„ ì‚´í´ë³´ê¸° ì „ Notationì„ ì‚´í´ë´ì•¼ í•œë‹¤.

<p align="center">
<img width="500" alt="1" src="https://user-images.githubusercontent.com/111734605/224572006-9fcb2f52-8504-43c1-b8ef-b04e1cd4db07.png">
</p>

- ì£¼ì˜ê¹Šê²Œ ë´ì•¼í•  Notation
  - Relational Graph $$G = (\mathscr{E}, R)$$
  - Node Set $$V = \mathscr{E} \; \cup \; R$$
  - Adjacency Matrix = ìš”ì†Œë“¤ì´ [0,1] ì‚¬ì´ì— ìˆê³ , ì°¨ì›ì´ $$ \vert V \vert \times \vert V \vert$$

- Knowledge Graph Completion
  - Triple $$(v_{subject}, v_{predicate}, v_{object}) = (v_s, v_p, v_o) = T$$  
  - As the label set $$T$$, $$f: T_M,A_G \rightarrow Y$$, $$ Y \in \mathbb{R}^{\vert \mathscr{E} \vert \times \vert R \vert} $$ ë¡œ ì •ì˜ëœë‹¤.

<br/>

## 2.1 Triple2Seq

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/be4bd6fd-a0a4-488e-9f7f-bcbb3e9e8903">
</p>

Knowledge GraphëŠ” ë§ì€ ìˆ«ìì˜ **Relational Information**ì„ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ê·¸ë˜í”„ë¥¼ ì§ì ‘ directí•˜ê²Œ Transformer ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì§‘ì–´ë„£ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. Full-graph-based Transformerì˜ ì´ëŸ¬í•œ ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ë…¼ë¬¸ì—ì„œëŠ” **Triple2Seq**ë¥¼ ì œì•ˆí•œë‹¤. Triple2SeqëŠ” <span style="color:red">**Contextualized Sub-Graphsë¥¼ Input Sequenceë¡œ ì‚¬ìš©í•´ Local Structure ì •ë³´ë¥¼ ì¸ì½”ë”©**</span>í•œë‹¤.

### 1) Contextualized Sub-Graph

Triple $$\mathcal{T}$$ì˜ Contextualized sub-graphì¸ <b>$$\mathcal{T_G}$$</b>ì€ Sub-graphì— ì¤‘ì‹¬ì— í•´ë‹¹í•˜ëŠ” Center Triple <b>$$\mathcal{T_C}$$</b>ì™€ Center Tripleì„ ë‘˜ëŸ¬ì‹¼ Surrounding neighborhood triple set <b>$$\mathcal{T_{context}}$$</b>ë¥¼ í¬í•¨í•œë‹¤. ì´ ë•Œ, Sub-graph sampling processëŠ” ì˜¤ì§ triple levelì—ì„œë§Œ ì¼ì–´ë‚œë‹¤. Githubì— ì˜¬ë¼ì˜¨ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ë©´ ì´ Sub-graphì˜ ì´ tripleìˆ˜ëŠ” ë³€ìˆ˜ë¡œ ì§€ì •ë˜ì–´ìˆê³ , Tripleì˜ ìµœëŒ€ hopìˆ˜ëŠ” 1ë¡œ ì •í•´ì ¸ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Triple $$\mathcal{T}$$ì— ë‘˜ëŸ¬ì‹¸ì¸ ì´ì›ƒë“¤ì— í•´ë‹¹í•˜ëŠ” $$\mathcal{T_{context}}$$ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<span style="font-size:110%"><center>$$\mathcal{T_{context} \; = \; \{ {\mathcal{T} \vert \mathcal{T_i} \in \mathcal{N}}} \}$$</center></span> 
<span style="font-size:110%"><center>$$\mathcal{T_G} \; = \; \mathcal{T_C} \; \cup \; \mathcal{T_{context}}$$</center></span>

<br/>

### 2) Dynamic Sampling

ì—¬ê¸°ì„œ $$\mathcal{N}$$ì€ Center Triple $$\mathcal{T_C}$$ì˜ ê³ ì •ëœ í¬ê¸°ì˜ ì´ì›ƒ Triple setì´ë‹¤. ë…¼ë¬¸ì—ì„ ëŠ” Local structural informationì„ ì¢€ ë” ì˜ ë½‘ì•„ë‚´ê¸° ìœ„í•´ í•™ìŠµ ì¤‘ <span style="color:red">**Dynamic Sampling**</span>ì„ í•˜ì˜€ë‹¤. ì´ëŠ” <u>ê° Epochë§ˆë‹¤ ê°™ì€ Center Tripleì— ëŒ€í•´ ì—¬ëŸ¬ê°œì˜ Contextualized Sub-graphë¥¼ <b>ë¬´ì‘ìœ„(random)ë¡œ ì„ íƒ</b>í•´ ì¶”ì¶œí•˜ëŠ” ë°©ë²•</u>ì´ë‹¤. 

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/6bc4842e-ce90-4fcb-8f59-e004e070624d">
</p>

Triple2Seqì˜ ê²°ê³¼ë¡œ ì–»ëŠ” ê²ƒì´ ë°”ë¡œ Contextualized Sub-Graphì¸ $$\mathcal{T_G}$$ì´ë‹¤. ë˜í•œ $$\mathcal{T_G}$$ì˜ local structure informationì€ ì¸ì ‘ í–‰ë ¬(Adjacency matrix) $$A_G$$ì— ì €ì¥ëœë‹¤. ì´ì „ì— ë‚˜ì™”ë˜ ë…¼ë¬¸ ì¤‘ [HittER: Hierarchical transformers for knowledge graph embeddings](https://meaningful96.github.io/paperreview/HittER/)ì„ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ì‚¬ì‹¤ì´ í•˜ë‚˜ ìˆë‹¤. ë°”ë¡œ <u>ì—”í‹°í‹°-ë¦´ë ˆì´ì…˜(Entity-Relation)ìŒì— ì €ì¥ëœ ì •ë³´ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.</u> ì´ëŸ¬í•œ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¬¸ì—ì„œëŠ” <span style="color:red">**ì—”í‹°í‹°-ë¦´ë ˆì´ì…˜ ìŒì„ Plain tokenìœ¼ë¡œ í‘œí˜„í•˜ê³  ë¦´ë ˆì´ì…˜ì„ contextualized sub-graphì˜ special node**</span>ë¡œ ê°„ì£¼í•œë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì—”í‹°í‹°-ë¦´ë ˆì´ì…˜, ì—”í‹°í‹°-ì—”í‹°í‹° ë° ë¦´ë ˆì´ì…˜-ë¦´ë ˆì´ì…˜ ìŒì„ í¬í•¨í•œ ë…¸ë“œ ìŒ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì„œ ê²°ë¡ ì ìœ¼ë¡œ **ë¦´ë ˆì´ì…˜ ë…¸ë“œë¥¼ special node**ë¡œ ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

<br/>

### 3) Global Node

Triple2SeqëŠ” ê²°êµ­ Contextualized Sub-graphë¥¼ í†µí•´ Localityë¥¼ ë½‘ì•„ë‚¸ë‹¤. ì´ëŸ´ ê²½ìš° global informationì— ëŒ€í•œ ì •ë³´ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” <span style="color:red">**Global node**</span>ì˜ ê°œë…ì„ ë„ì…í•œë‹¤. global nodeëŠ” ì‰½ê²Œ ë§í•˜ë©´ ì„ì˜ì˜ ìƒˆë¡œìš´ ì—”í‹°í‹°ë¥¼ ë§Œë“¤ì–´ training setì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ì—”í‹°í‹°ì™€ 1-hopìœ¼ë¡œ ì—°ê²°ì‹œì¼œë†“ì€ ê²ƒì´ë‹¤. ì¦‰ ëª¨ë‘ì™€ 1-hopìœ¼ë¡œ ì—°ê²°ëœ ì—”í‹°í‹°ì´ë‹¤. í•˜ì§€ë§Œ, ë…¼ë¬¸ì—ì„œëŠ” global nodeë¥¼ training set ì „ì²´ì—ë‹¤ê°€ ì—°ê²°ì‹œí‚¨ ê²ƒì´ ì•„ë‹Œ, <span style="color:red">**ì¶”ì¶œëœ Sub-graphì— ìˆëŠ” ëª¨ë“  ì—”í‹°í‹°ì™€ ì—°ê²°ëœ ì—”í‹°í‹°ë¥¼ ì˜ë¯¸**</span>í•œë‹¤.

<span style="font-size:110%"><b>Remark 1.</b></span>  
> Triple2SeqëŠ” Input Sequenceë¥¼ ë§Œë“¤ê¸°ìœ„í•´ contextualized sub-graphë¥¼ dynamic samplingí•œë‹¤.
> ê²°ê³¼ì ìœ¼ë¡œ TransformerëŠ” Large KGì— ëŒ€í•´ì„œë„ ì‰½ê²Œ ì ìš©ë  ìˆ˜ ìˆë‹¤.
> RelphormerëŠ” Heterogeneous graphì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì´ë©°,
> sequential modelingì„ ìœ„í•´ ë¬¸ë§¥í™”ëœ í•˜ìœ„ ê·¸ë˜í”„(Contextualized sub-graph)ì—ì„œ edge(relation)ë¥¼ í•˜ë‚˜ì˜ Special nodeë¡œ ì·¨ê¸‰í•œë‹¤.
> ê²Œë‹¤ê°€, Sampling processëŠ” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” data augmentation operatorë¡œ ë³¼ ìˆ˜ ìˆë‹¤.
>
> Note that with Triple2Seq, which dynamically samples
> contextualized sub-graphs to construct input sequences, Transformers
> can be easily applied to large knowledge graphs. However,
> our approach focuses on heterogeneous graphs and regards edges (relation)
> as special nodes in contextualized sub-graphs for sequential
> modeling. Besides, the sampling process can also be viewed as a data
> augmentation operator which boosts the performance.


## 2.2 Structure enhanced self-attention 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/de87859d-492d-48ee-883e-94e98157a9c8">
</p>

### 1) Attention Bias

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì…ë ¥ìœ¼ë¡œ Sequenceë¥¼ ë°›ëŠ”ë‹¤. ì´ ë•Œ, <span style="font-size:105%"><b>Sequential Inputê°€ Fully-Connected Attention Mechanismì„ ê±°ì¹˜ë©´ì„œ Structural Informationì„ ìœ ì‹¤</b></span>ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ê·¸ ì´ìœ ëŠ” Fully-Connected ë¼ëŠ” ê²ƒì€ ê²°êµ­ Dense-layerì˜ í˜•íƒœì´ë‹¤. ì¦‰, Neural Networkë¥¼ ì˜ˆë¡œ ë“¤ë©´ ëª¨ë“  drop-outì´ 0ì¸ ìƒíƒœì¸ë° <u><b>í•œ ë…¸ë“œì— ëŒ€í•´ ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œë“¤ê³¼ì˜ attentionì„ êµ¬í•˜ë¯€ë¡œ(êµ¬ì¡°ì™€ ìƒê´€ì—†ì´ ëª¨ë“  ë…¸ë“œë¥¼ ìƒëŒ€í•˜ê¸° ë•Œë¬¸) êµ¬ì¡° ì •ë³´ê°€ ë°˜ì˜ë˜ì§€ ëª»í•˜ëŠ” ê²ƒ</b></u>ì´ë‹¤.

ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” <span style="color:green">**Attention Bias**</span>ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•˜ì˜€ë‹¤. Attention biasë¥¼ í†µí•´ <span style="color:red"><b>Contextualized Sub-Graph ì•ˆì˜ ë…¸ë“œìŒë“¤ì˜ êµ¬ì¡° ì •ë³´(Structural information)ì„ ë³´ì¡´</b></span>í•  ìˆ˜ ìˆë‹¤. ë…¸ë“œ $$v_i$$ì™€ $$v_j$$ì‚¬ì´ì˜ attention biasëŠ” <b>$$\phi(i,j)$$</b>ë¡œ í‘œê¸°í•œë‹¤.

<p align="center">
<img width="700" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/d87d1256-1235-46d0-af2f-e22cd9625f83">
</p>

Triple2Seqì—ì„œ ìƒ˜í”Œë§ëœ Contextualized Sub-graphì˜ êµ¬ì¡° ì •ë³´ëŠ” ì¸ì ‘ í–‰ë ¬(Adjacency Matrix) $$A_G$$ì— ì €ì¥ëœë‹¤. ì´ ë•Œ, Sub-graphì˜ êµ¬ì¡° ì •ë³´ë¥¼ Normalizationí•œ ê°’ì„ <b>$$\widetilde{A}$$</b>ìœ¼ë¡œ í‘œê¸°í•œë‹¤. ì´ëŸ¬í•œ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ <b>$$\widetilde{A^m}$$($$\widetilde{A}$$ to the $$m$$-th power)</b>ë¥¼ ì •ì˜í•œë‹¤. ì´ëŠ” $$\widetilde{A}$$ë¥¼ **më²ˆ ì œê³±**í•œ ê²ƒ(**í–‰ë ¬ê³±**ì„ më²ˆ ìˆ˜í–‰ <span style="font-size:80%">ex)$$ \widetilde{A} \; @ \; \widetilde{A}$$</span>) mì€ hyperparameterì´ë‹¤. 

ì—¬ê¸°ì„œ ì•Œì•„ì•¼ í•  ê°œë…ì´ ìˆëŠ”ë°, ì¸ì ‘ í–‰ë ¬(Adjacency Matrix)ì˜ ì œê³±, ì„¸ì œê³± ë“±ì´ ê°€ì§€ëŠ” ì˜ë¯¸ì´ë‹¤. ì œê³±ì„ ì˜ˆë¡œ ì„¤ëª…í•˜ë©´, m = 2ì¸ ìƒí™©ìœ¼ë¡œ, ì–´ë–¤ ë…¸ë“œ $$v_i$$ì—ì„œ ë˜ ë‹¤ë¥¸ ë…¸ë“œ $$v_j$$ë¡œ ì´ë™í•  ë•Œ 2ë²ˆ(m=2)ì›€ì§ì—¬ì„œ ê°ˆ ìˆ˜ ìˆëŠ” íšŸìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, ë…¸ë“œë¥¼ ìˆœíšŒí•  ë•Œ, më²ˆ ì´ë™í•˜ì—¬ ê°ˆ ìˆ˜ ìˆëŠ” ê²½ìš°ì˜ ìˆ˜ê°€ ê° $$\widetilde{A^m}$$ì˜ ìš”ì†Œê°€ ëœë‹¤. ê°„ë‹¨í•˜ê²Œ ì½”ë“œë¥¼ í†µí•´ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/73c55cdf-79f8-47d3-a053-7d0d60b8b9ab">
</p>

```python
import networkx as nx
import numpy as np

head = [1,2,3,4]

G = nx.Graph()
G.add_nodes_from(head)
G.add_edges_from([(1,1),(1,2),(1,4),(2,3),(2,4),(3,4)])

## Adjacency Matrix
a1 = np.array([1,1,0,1])
a2 = np.array([1,0,1,1])
a3 = np.array([0,1,0,1])
a4 = np.array([1,1,1,0])

A = np.c_[a1,a2,a3,a4]
```

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/506fa430-1dd8-48a7-98d0-35fa2c8003b2">
</p>

ì•ì˜ ì‹¤í—˜ì„ í†µí•´ $$\widetilde{A^m}$$ì´ í™•ì‹¤í•˜ê²Œ ì •ì˜ë˜ì—ˆê³ , $$f_{structure}$$ëŠ” êµ¬ì¡° ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” **Linear Layer**ë¡œ $$\widetilde{A^m}$$ì„ ì…ë ¥ìœ¼ë¡œ í•œë‹¤. ì´ë¥¼ í†µí•´ ìµœì¢…ì ìœ¼ë¡œ Attention biasì¸ $$\phi(i,j)$$ê°€ ì •ì˜ëœë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°í•˜ì§€ë§Œ, <span style="color:red">**Attention biasëŠ” ìƒ˜í”Œë§ëœ Contextualized Sub-Graphì˜ êµ¬ì¡° ì •ë³´ë¥¼ í¬ì°©**</span>í•˜ëŠ” ì—­í• ì„ í•˜ë©°, ì´ ë¶€ë¶„ì´ ì´ ë…¼ë¬¸ì˜ ê°€ì¥ í° Contributionì¤‘ í•˜ë‚˜ì´ë‹¤.

<br/>

### 2) Contrastive Learning Strategy

Denseí•œ Knowledge Graph(WN18RRë³´ë‹¤ëŠ” FB15k-237ì´ relationì˜ ì¢…ë¥˜ê°€ ë” ë§ìœ¼ë¯€ë¡œ ë” Denseí•¨)ì˜ ê²½ìš°, í•˜ë‚˜ì˜ Center Tripleì— ëŒ€í•´ ë§ì€ ìˆ˜ì˜ Contextualized Sub-Graphë¥¼ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ì •ë³´ì˜ í¸í–¥ì´ ìƒê¸¸ ìˆ˜ ìˆë‹¤. Dynamic Samplingì„ í•  ë•Œ, ì´ëŸ° ë¶ˆì•ˆì •ì„±ì„ ê·¹ë³µí•˜ê³ ì **Contextual contrastive strategy**ë¥¼ ì´ìš©í•œë‹¤. 

> We use the contextualized sub-graph of the same triple in different epochs triple in different epochs to enforce the model to conduct similar predictions.

ì¦‰, <span>ì„œë¡œ ë‹¤ë¥¸ Epochì—ì„œ ê°™ì€ Tripleì˜ contextualized sub-graphë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì—ê²Œ ìœ ì‚¬ë„ë¥¼ í•™ìŠµí•˜ëŠ” ê±¸ ê°•ìš”í•œë‹¤. Contextualized sub-graphì˜ Input Sequenceë¥¼ ì¸ì½”ë”©í•˜ê³  hidden vector $$\mathcal{h_{mask}}$$ë¥¼ í˜„ì¬ Epoch $$t$$ì—ì„œ $$c_t$$, ë§ˆì§€ë§‰ Epoch $$c_{t-1}$$ë¡œ ê°€ì ¸ì˜¨ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Lossë¥¼ ì •ì˜í•˜ëŠ”ë° $$\mathcal{L_{contextual}}$$ë¥¼ contextual lossë¡œ ì •ì˜í•œë‹¤. ì´ ì‘ì—…ì„ ê±°ì³ ìµœì¢… ëª©í‘œëŠ” <span style="color:red">**ì„œë¡œ ë‹¤ë¥¸ sub-graphì‚¬ì´ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”(minimization)í•˜ëŠ” ê²ƒ**</span>ì´ë‹¤. 

<p align="center">
<img width="700" alt="1" src="https://github-production-user-asset-6210df.s3.amazonaws.com/111734605/252900409-5e4d57c7-c3d3-4692-8acb-c33f8a5bc005.png">
</p>

ìˆ˜ì‹ì—ì„œë„ ë³´ì´ë“¯ì´, Contrastive Learningì˜ í˜•íƒœë¥¼ ë”°ë¥´ë©° ì£¼ì˜í•  ê²ƒì€ ë¹¨ì‚°ìƒ‰ ë¶€ë¶„ì´ë‹¤. ë¶„ëª¨ëŠ” $$v_i$$ ë…¸ë“œì˜ tì™€ t-1ì‹œì ì˜ ìœ ì‚¬ë„ì™€ tì‹œì ì—ì„œ $$v_i$$ë…¸ë“œì™€ $$v_j$$ë…¸ë“œì˜ ìœ ì‚¬ë„ì˜ í•©ì´ë‹¤. ì´ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ í•™ìŠµì˜ ë°©í–¥ì´ ì„œë¡œ ë‹¤ë¥¸ sub-graphì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°„ë‹¤ëŠ” ê²ƒì´ë‹¤.

<span style="font-size:110%"><b>Remark 2.</b></span>  
> ì´ Structure-enhanced TransformerëŠ” ëª¨ë¸ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©° ë”°ë¼ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì— ì˜ë¯¸ë¡ ì (semantic) ë° > êµ¬ì¡°ì  ì •ë³´(Structure Information)ë¥¼ ì£¼ì…í•˜ëŠ” ê¸°ì¡´ì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ì§êµí•œë‹¤ëŠ” ì ì— ì£¼ëª©í•´ì•¼ í•œë‹¤. 
> Original Graphì—ì„œ ë§ ê·¸ëŒ€ë¡œ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ëª¨ë“  ë…¸ë“œì— ëŒ€í•œ attentionì„ ìˆ˜í–‰í•˜ë¯€ë¡œ êµ¬ì¡° ì •ë³´ê°€ ë°˜ì˜ë˜ì§€ ëª»í•œë‹¤.
> í•˜ì§€ë§Œ, Structure-Enhanced Transformerë¥¼ ì‚¬ìš©í•˜ë©´ Local Contextualized Sub-graphì˜ êµ¬ì¡°ì™€ ì˜ë¯¸ë¡ ì  íŠ¹ì§•ì˜ ì˜í–¥ë ¥ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤. Local graph êµ¬ì¡°ì—ì„œ ìœ ì‚¬í•œ ë…¸ë“œê°„ì˜ ì •ë³´ êµí™˜ì— í¸ë¦¬í•˜ë‹¤.
>
> It should be noted that our structure-enhanced Transformer
> is model-agnostic and, therefore, orthogonal to existing approaches,
> which injects semantic and structural information into the
> Transformer architecture. In contrast to [34] where attention operations
> are only performed between nodes with literal edges in the
> original graph, structure-enhanced Transformer offers the flexibility
> in leveraging the local contextualized sub-graph structure and influence
> from the semantic features, which is convenient for information
> exchange between similar nodes in the local graph structure.



ì¢€ ë” ì‰½ê²Œ ë§í•˜ìë©´, ê¸°ì¡´ì˜ atttention operationì€ ë‹¨ìˆœíˆ ì „ì²´ ê·¸ë˜í”„ ì•ˆì—ì„œ ë…¸ë“œì™€ ì˜ë¯¸ìˆëŠ” relationì‚¬ì´ì—ì„œ ê³„ì‚°ì„ ì§„í–‰í•˜ëŠ”ê²ƒì— ë°˜í•´, Structure-enhances self attentionì€ <span style="color:red">**Contextualized Sub-graph êµ¬ì¡°ë¥¼ ì´ìš©í•œ Locality ì •ë³´ì™€ Semantic featureë“¤ì— ëŒ€í•´ë„ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ì£¼ëŠ” ìœ ì—°ì„±ì„ ì´ëŒì–´ë‚´ë©° ì´ë¥¼ í†µí•´ Transformer ëª¨ë¸ì— êµ¬ì¡°ì  ì •ë³´(Structural information)ì™€ ì˜ë¯¸ë¡ ì  ì •ë³´(Semantic feature)ë¥¼ ë™ì‹œì— ì¤„ ìˆ˜ ìˆë‹¤**</span>ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤.

## 2.3 Masked Knowledge Modeling

Masked Knowledge Modelingì€ íŠ¹ë³„í•œ ê²ƒì´ ì•„ë‹Œ, Masked Langauge Modeling(MLM)ê³¼ ìœ ì‚¬í•˜ë‹¤. Knowledge Graph Completionì€ $$f: \mathcal{T_M}, A_G \; \rightarrow \; Y$$ë¥¼ í‘¸ëŠ” Taskì´ë‹¤. Relphormerì—ì„œëŠ” ëœë¤í•˜ê²Œ Input Sequenceì˜ í† í°ë“¤ì„ ë§ˆìŠ¤í‚¹í•˜ê³ , ê·¸ ë§ˆìŠ¤í‚¹ëœ í† í°ë“¤ì„ ì˜ˆì¸¡í•œë‹¤.

> We randomly mask specific tokens of the input sequences and then predict those masked tokens.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/2dfca278-beee-49b2-b3c1-f1156768b771">
</p>

Input Contexturalized Sub-Graph node sequence $$\mathcal{T_G}$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëœë¤í•˜ê²Œ Center tripleì„ ë§ˆìŠ¤í‚¹í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ relation predictionì„ í•  ë•ŒëŠ” headë‚˜ tail ë‘˜ ì¤‘ì— í•˜ë‚˜ë¥¼ ë§ˆìŠ¤í‚¹í•œë‹¤. ì´ë¥¼ Tripleë¡œ í‘œí˜„í•˜ë©´ $$(\; v_{h},?,[MASK] \;) \; or \; (\; [MASK], ?, v_t \;)$$ì´ë‹¤. 

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/cd798629-c5fd-45d8-8f8b-cf7be1837f85">
</p>

$$Y$$ëŠ” Candidate(í›„ë³´)ì´ë‹¤. Masked Knowledge Modelingì´ ê¶ê·¹ì ìœ¼ë¡œ í’€ê³ ì í•˜ëŠ” ê²ƒì€ <b>ë§ˆìŠ¤í‚¹ëœ node sequence $$\mathcal{T_M}$$ê³¼ Contextualized Sub-graphì˜ êµ¬ì¡° ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” $$A_G$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ Original Triple $$\mathcal{T}$$ì˜ missing partë¥¼ ì°¾ëŠ” ê²ƒ</b>ì´ë‹¤. ì°¸ê³ ë¡œ, Yì˜ shapeì€ ($$Y \in \mathbb{R^{\vert \mathcal{E} \vert \times \vert \mathcal{R} \vert}}$$)ì´ë‹¤.

êµ¬ì²´ì ìœ¼ë¡œ, <span style="color:red">**Contextualized Sub-Graphì˜ ìœ ë‹ˆí¬í•œ êµ¬ì¡°ì • ì •ë³´ë¥¼ ì´ìš©í•´ Contextual informationì„ ë” ì˜ í†µí•©í•˜ê¸°ìœ„í•´ Sequenceì—ì„œ ë‹¨ í•˜ë‚˜ì˜ í† í°ë§Œ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹**</span>í•œë‹¤. 

ì§ê´€ì ìœ¼ë¡œ masked knowledge modelingì€ scoring functionì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì´ì „ translation distance ë°©ë²•ë“¤ê³¼ëŠ” í™•ì—°í•œ ì°¨ì´ë¥¼ ë³´ì—¬ì¤€ë‹¤. ë‹¤ë§Œ, <u>ë§ˆìŠ¤í‚¹ì„ í•  ë•Œ Headì™€ Tailì˜ ì¸ì ‘í•œ ë…¸ë“œ(ì´ì›ƒ ë…¸ë“œ)ë¥¼ ë™ì‹œì— ìƒ˜í”Œë§í•  ê²½ìš° Link predictionì‹œ ì‹¬ê°í•œ <b>Label leakage</b>ë¥¼ ìœ ë°œ</u>í•  ìˆ˜ ìˆë‹¤. ì£¼ì˜í•´ì•¼ í•  ì ì€ ë°”ë¡œ Training ì¤‘ì—ëŠ” ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ í† í°ì˜ êµ¬ì¡°ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ Label Leakgageë¥¼ ê·¹ë³µí•´ ê³µí‰í‰í•œ ë¹„êµ(fair comparison)ë¥¼ ë³´ì¥í•˜ê¸°ìœ„í•´ <span style="color:red">**íƒ€ê²Ÿ ì—”í‹°í‹°(Target entity)ì˜ context nodeë¥¼ ì œê±°**</span>í•˜ì—¬ trainingê³¼ testingì˜ ì°¨ì´ë¥¼ ì¢í˜€ì¤€ë‹¤.    

<span style="font-size:110%"><b>Remark 3.</b></span>  
> Masked Knowledge Modelingì€ ë” ì¢‹ì€ Link preidctionì„ ìœ„í•´ ì ì ˆí•œ ìµœì í™” targetì„
> ìë™ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë§¤ê°œ ë³€ìˆ˜ scoring functionì˜ ê·¼ì‚¬ì¹˜ì¼ ìˆ˜ ìˆë‹¤.
>  
> The advancement of empirical results (See section 4)
> illustrates that masked knowledge modeling may be a parametric
> score function approximator, which can automatically

## 3. Training and Inference

### 1) Pseudo Code
<span style="font-size:110%"><b>Hypothesis 1.</b></span>  
(Score function approximator) $$\mathcal{T_M}$$ì„ masked tripleì´ë¼ê³  í•  ë•Œ, $$\mathbf{h} \; \in \; \mathbb{R^d}$$ëŠ” Relphormer $$\mathcal{M}(\theta)$$ì—ì„œ multi-head attentionì„ í†µí•´ ì–»ì–´ì§„ ë§ˆìŠ¤í‚¹ëœ headì´ë‹¤. Vocabulary í† í° ì„ë² ë”©ì€ $$W \; \in \; \mathbb{R^{d \times N}}$$ì´ë©° $$N \; = \; \vert \mathcal{E} \vert \; + \; \vert \mathcal{R} \vert $$ì´ë‹¤. 

Pseudo CodeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/4d2b0251-2e6c-49c8-bfac-87ada18edda3">
</p>

ë§Œì•½ $$\mathcal{T_M} = (v_s, v_p, [MASK])$$ì´ë©´ tailì´ ë§ˆìŠ¤í‚¹ëœ íŠ¸ë¦¬í”Œì„ ì˜ë¯¸í•œë‹¤. $$g(\cdot)$$ì€ multi-head attention layer ëª¨ë“ˆì´ê³  $$V_{object} \; \subset \; W$$ì€ tailì´ ë  ìˆ˜ ìˆëŠ” í›„ë³´ë“¤ì˜ ì„ë² ë”©ì„ ì˜ë¯¸í•œë‹¤. Output Logitì€ $$sigmoid(W \mathbf{h})$$ì´ë©° ì´ëŠ” ê·¼ì‚¬ì ìœ¼ë¡œ $$sigmoid(V_{object} \mathbf{h})$$ì™€ ë™ì¼í•˜ë‹¤.

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/84820e60-7883-4948-837d-6ad26f116f97">
</p>

ìµœì¢…ì ì¸ final logitì€ ìœ„ì™€ê°™ë‹¤. í•˜ë‚˜ì˜ $$f(v_s, v_p,v_{object})$$ termì„ ê³ ë¥¸ í›„ $$f( \cdot ) \; \approx \; v_{object_i} \, g( \cdot )$$ì„ ì´ìš©í•œë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ fê°€ ê²°êµ­ <u>score functionì²˜ëŸ¼ ë™ì‘í•˜ê²Œ ë˜ë©° ê²°ë¡ ì ìœ¼ë¡œ Masked knowledge modelingì´ score function approximator</u>ì²˜ëŸ¼ ëœë‹¤.

<br/>

### 2) Training and Inference

ê²°ë¡ ì ìœ¼ë¡œ ìœ„ì˜ Pseudo code algorithmê³¼ ê°™ì´ RelphormerëŠ” ë™ì‘í•œë‹¤. í•™ìŠµ ì¤‘ì—ëŠ” joint optimizationì„ ì´ìš©í•´ masked knowledge lossì™€ contextual contrastive constrained objectë¥¼ ë™ì‹œì— ìµœì í™”í•œë‹¤. ë”°ë¼ì„œ ìµœì¢… LossëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤. $$\lambda$$ëŠ” hyperparameterì´ê³  $$\mathcal{L_{MKM}}$$ê³¼ $$\mathcal{L_{contextual}}$$ì€ ê°ê° masked knowledge lossì™€ contextual lossì´ë‹¤.

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/dfb55d7e-c856-45da-ada2-dbc0360520b8">
</p>


<br/>

<span style="font-size:105%"><b>KG Completion</b></span>  
**ì¶”ë¡ ì‹œ(Inference)**, multi-sampling strategyë¥¼ ì´ìš©í•´ ì˜ˆì¸¡ì˜ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤. ì´ ë•Œ, $$\mathbf{y_k} \in \mathbb{R^{\vert V \vert \times 1}}$$ì˜ shapeì„ ê°€ì§€ë©° í•˜ë‚˜ì˜ Contextual sub-graphì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, $$K$$ëŠ” ìƒ˜í”Œë§ëœ sub-graphì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

<p align="center">
<img width="100" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/dd015b54-677a-4eed-80c3-929cd002be43">
</p>


<br/>

<span style="font-size:105%"><b>Question Answering and Recommendation</b></span>  
Relphomrerì— fine-tuningì„ í•˜ì—¬ QA taskì™€ ì¶”ì²œ ì‹œìŠ¤í…œì— ì ìš©í•˜ì˜€ë‹¤. QA taskì˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤. $$\mathcal{Q_M}$$ì€ ë§ˆìŠ¤í‚¹ëœ queryì´ê³  $$\mathcal{M(\theta)}$$ëŠ” pre-trainedëœ KG transformerì´ë‹¤. downstream taskì— ë”°ë¼ì„œ $$\mathcal{Q_M}$$ì˜ í‘œí˜„ì€ ì¡°ê¸ˆì”© ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. (QA = Question Answering, RS = Recommandataion System)

<p align="center">
<img width="170" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/4381859f-bf18-4662-8aed-dd2a093cbd5a">
</p>


- In QA: $$\mathcal{Q_M}$$ is defined by \[ *question tokens*; \[MASK\] \] = KGì—ì„œ ì •ë‹µì¸ ì—”í‹°í‹°ë¥¼ ì˜ˆì¸¡
- In RS: $$\mathcal{Q_M}$$ is defined by \[ *items tokens*; \[MASK\] \]

<br/>

<span style="font-size:105%"><b>Model Time Complexity Anaylsis</b></span>  
KG-BERTì™€ Relphormerì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸°ì— ì•ì„œ ë¨¼ì € Time Complexityë¥¼ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. Relphormerì˜ ê²½ìš°ê°€ KG-BERTì— ë¹„í•´ í›¨ì”¬ ë” ì¢‹ì€ Time Complexityë¥¼ ë³´ì´ë©° í•™ìŠµê³¼ ì¶”ë¡ ì‹œê°„ì— ìˆì–´ì„œ ì°¨ì´ê°€ ë§ì´ ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. RelphormerëŠ” Masked knowledge modelingì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì´ ë§ˆìŠ¤í‚¹ëœ ì—”í‹°í‹°ë‚˜ ë¦´ë ˆì´ì…˜ì„ ì˜ˆì¸¡í•œë‹¤. ë¹„ë¡ Triple2Seqì—ì„œ ì‹œê°„ì´ ì¢€ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, Relphormerê°€ ì—¬ì „íˆ KG-BERTì— ë¹„í•´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/3d1c546c-68a4-42c9-a03d-9792eb2493b8">
</p>

<br/>
<br/>

# Experiment & Result
ì´ 6ê°œì˜ Banchmark Datasetì„ ì‚¬ìš©
- Knowledge Graph Completion(KGC)
  - WN18RR
  - FB15k-237
  - UMLS   
- Knowledge-Base Qusetion Answering
  - FreeBaseQA
  - WebQuestionSP
- Recommandation
  - MovieLens      

## 1. KG Completion & Relation Preidction

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/f4a94433-1443-48b7-b761-c3b00a61db53">
</p>

Table 3ì—ì„œ ë°©ì‹ì´ Relphormerê°€ baselineë“¤ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë“  Datasetì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤. RelphormerëŠ” Hits@1 ë° MRR Metricì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  WN18Rì—ì„œ Hits@10ì—ì„œ ë‘ ë²ˆì§¸ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì‚°ì¶œí–ˆë‹¤. QuatEì™€ ê°™ì€ ì´ì „ SOTA ë³€í™˜ ê±°ë¦¬ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë“  Metricì—ì„œ ê°œì„ ë˜ì—ˆë‹¤. Relphormerê°€ WN18Rì—ì„œ SOTA Transformer ê¸°ë°˜ ëª¨ë¸ HitERë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤.

ë˜í•œ FB15K-237 ë°ì´í„° ì„¸íŠ¸ì—ì„œ Relphormerê°€ ëŒ€ë¶€ë¶„ì˜ ë²ˆì—­ ê±°ë¦¬ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ RelphormerëŠ” Hits@1ì—ì„œ KG BERT, StAR ë° HittERë³´ë‹¤ ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜í•˜ë‹¤. HitERëŠ” **FB15K-237ì—ì„œ ë” ë§ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™œìš©**í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, RelphormerëŠ” ì—¬ì „íˆ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ì–»ìŠµë‹ˆë‹¤. ê²Œë‹¤ê°€, ìš°ë¦¬ëŠ” Relphormerê°€ UMLS, íŠ¹íˆ Hits@10ì—ì„œ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. Relphormerì˜ Relational Transformer Frameworkê°€ KGCì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë§Œë“¤ì–´ë‚¸ë‹¤. 

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/c30d94bd-eb04-4491-b911-b0d4a2d3a9db">
</p>

Table 4ì—ì„œ Relphormerê°€ baselineë“¤ê³¼ ë¹„êµí•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. WN18RR ë°ì´í„° ì„¸íŠ¸ì—ì„œ RelphormerëŠ” ì´ë¯¸ ëª¨ë“  baselineì„ ëŠ¥ê°€í•˜ë©°, ì´ëŠ” relation predictionì„ ìœ„í•œ Relphormerì˜ ì ‘ê·¼ ë°©ì‹ì´ ì„±ëŠ¥ í–¥ìƒì— ì§ì ‘ì ì„ì„ ë³´ì—¬ì¤€ë‹¤. TransEì™€ ë¹„êµí•˜ì—¬ Hits@1ì—ì„œ 15.8%, 9.7% í–¥ìƒë˜ì—ˆë‹¤. FB15K-237ì—ì„œ Relphormerì˜ ì„±ëŠ¥ í–¥ìƒì€ íŠ¹íˆ Hits@3ì—ì„œ ì¤‘ìš”í•˜ë‹¤. RelphormerëŠ” DistMultë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ì§€ë§Œ RotatEë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.

## 2. Question-Answering & Recommandation

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/19fd7be1-9a8b-4b89-8359-ed466cc48e1e">
</p>

QAì˜ ê²½ìš°, Relphormerê°€ FreebaseQA ë° WebQuestionSPì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.(Figure 3.) HitERì— ë¹„í•´ RelphormerëŠ” FreebaseQA Datasetì˜ *Full Setting*ì—ì„œ 6.8% í–¥ìƒë˜ì—ˆë‹¤. ë˜í•œ, RelphormerëŠ” WebQuestionSPì˜ *Full & Filter Setting*ì—ì„œ 2.9% ë° 1.4% í–¥ìƒë˜ì—ˆë‹¤. RelphormerëŠ” BERTë¡œ ì‰½ê²Œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìœ¼ë©° masked knowledge modelingìœ¼ë¡œ ìµœì í™”ë˜ë¯€ë¡œ QA ì‘ì—…ì„ ìœ„í•´ <span style="color:red">**Relphormerë¡œ ì‚¬ì „ í›ˆë ¨ëœ í‘œí˜„ì„ ì£¼ì…í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë¯€ë¡œ ì„±ëŠ¥ì´ í–¥ìƒ**</span>ëœë‹¤. Relphormerê°€ í›¨ì”¬ ë” íš¨ìœ¨ì ì´ë©°, HitERì™€ ê°™ì€ ì¼ë¶€ KG í‘œí˜„ ëª¨ë¸ì˜ ê²½ìš° QA ì‘ì—…ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë³µì¡í•œ í†µí•© ì „ëµì„ ì„¤ê³„í•´ì•¼í•œë‹¤.

í•œ ê°€ì§€ ê°„ë‹¨í•œ ë°©ë²•ì€ Pre-trained representationì„ Extra QA ëª¨ë¸ì— ì£¼ì…í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ Pre-trainedëœ KG ëª¨ë¸ê³¼ Downstream ëª¨ë¸ ê°„ì˜ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ê¸°ê°€ ì–´ë µë‹¤. Freebaseì— í’ë¶€í•œ Textural ë° Structual informationì´ ìˆëŠ” FreeBaseQA(Figure 4.)ë¥¼ ì´ìš©í•œ hard sampleì„ í†µí•´ Relphormerê°€ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ ë˜ëŠ” ì—”í‹°í‹° ê°„ì˜ ëª…ì‹œì ì´ê³  ì•”ë¬µì ì¸ ìƒê´€ê´€ê³„ë¥¼ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•´ì•¼í•œë‹¤.

<br/>

Recommandationì˜ ê²½ìš° Relphormerê°€ ëª¨ë“  baseline ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤(Figure 3). BERT4RECì™€ ë¹„êµí•˜ì—¬ RelphormerëŠ” Hits@1ì—ì„œ 2%, MRRì—ì„œ 1% í–¥ìƒë˜ì—ˆë‹¤. ë˜í•œ RelphormerëŠ” ê° ë…¸ë“œì˜ BERT ì„ë² ë”© ê³„ì‚° ë° aggregationì— ì˜í•´ êµ¬í˜„ë˜ëŠ” KG-BERTë¥¼ ëŠ¥ê°€í•œë‹¤. Figure 4ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, íŠ¹ì • ì‚¬ìš©ìê°€ ê¸´ ëª©ë¡ì˜ ì˜í™”ë¥¼ ì‹œì²­í•œ ê²½ìš°, ì—¬ê¸°ì„œ ëª©í‘œëŠ” ë‹¤ìŒì— ì‹œì²­í•  ì˜í™”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ ì˜í™”ë“¤ ì¤‘ *Sleepless in Seattle* ê³¼ *Tin Cup* ì€ ë‘ ì˜í™”ì˜ ì£¼ì œê°€ ëª¨ë‘ ë¡œë§¨ìŠ¤ì™€ ì½”ë¯¸ë””ì— ê´€í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— ë°€ì ‘í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤. í•œí¸, ì¶”ê°€ëœ KGsì˜ ì˜í™” *Mighty Aphrodite* ë„ ê°™ì€ ì´ìœ ë¡œ *Sleepless in Seattle* ê³¼ *Tin Cup* ì— ì—°ê²°ë˜ì–´ ìˆë‹¤.

ë¶„ëª…íˆ, <span style="color:red">**KGì˜ ì ì¬ì  ë…¸ë“œ ê´€ë ¨ì„±ì€ ì˜í™” ì¶”ì²œ ì‘ì—…ì— ë„ì›€ì´ ëœë‹¤**</span>. ì´ëŸ¬í•œ ìƒ˜í”Œì˜ ê²½ìš°, RelphormerëŠ” ê¹Šì€ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³  ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì‚°ì¶œí•  ê²ƒì´ë‹¤. ì „ë°˜ì ìœ¼ë¡œ, Relphromerë¥¼ ì‚¬ìš©í•œ KG í‘œí˜„ì´ Link predictionì„ í†µí•´ ë” ë‚˜ì€ ë³¸ì§ˆì ì¸ í‰ê°€ ì„±ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ í•™ìŠµëœ Knowledge Representationì„ í†µí•´ QA ë° Recommandationì˜ KG-based downstream taskë¥¼ ì´‰ì§„í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

## 3. Ablation Study 1

<span style="font-size:110%"><b>How do different key modules in the Relphormer framework contribute to the overall performance?</b></span>

### 1) Optimization Object

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/4f38bf80-0c3e-467c-8aac-5a1d33ec6bc0">
</p>

Knowledge Graphì—ëŠ” Relational Patternì´ ìˆìœ¼ë©°, 1-N, N-1 ë° N-N ê´€ê³„ì™€ ê°™ì€ íŒ¨í„´ì„ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ì ‘ê·¼ ë°©ì‹ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ íŠ¹ì • ì—”í‹°í‹°-ë¦´ë ˆì´ì…˜ ìŒ(â„, ğ‘Ÿ)ì´ ì£¼ì–´ì§€ë©´ ì¼ë°˜ì ìœ¼ë¡œ tailì˜ ìˆ˜ëŠ” ë‘˜ ì´ìƒì´ë‹¤. Masked Knowledge Modeling(MKM) ì—†ì´, ëŒ€ì‹  negative log-likelihoodì„ ì‚¬ìš©í•˜ì—¬ ablation studyë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤. Table 5ì—ì„œ MKMì´ ìˆëŠ” ëª¨ë¸ì€ ë‘ ë°ì´í„° ì„¸íŠ¸ ëª¨ë‘ì—ì„œ Hit@1ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì§€ë§Œ WN18Rì—ì„œ MRì˜ í–¥ìƒì„ ë‹¬ì„±í•˜ì§€ ëª»í•œë‹¤. ì´ëŠ” <span style = "color:red">**WN18RRì— ì¶©ë¶„í•œ êµ¬ì¡°ì  íŠ¹ì§•ì´ ì—†ê¸° ë•Œë¬¸**</span>ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ rankì— ëŒ€í•œ **NLL ê¸°ë°˜ì˜ ìµœì í™”ê°€ ë” ìœ ë¦¬**í•  ìˆ˜ ìˆë‹¤.

<br/>

### 2) Global Node
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/faab45a3-838e-4616-a8c4-eaa02a93a13b">
</p>
ì´ ì‹¤í—˜ì—ì„œëŠ” global nodeì— ì˜í–¥ë ¥ì— ëŒ€í•´ì„œ ì‹¤í—˜í•œë‹¤. *w/o global node*ëŠ” global nodeê°€ ì—†ëŠ” ëª¨ë¸ì„ ë‚˜íƒ€ë‚¸ë‹¤. Figure 5.ì—ì„œ baseline ëª¨ë¸ë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ, global nodeê°€ ì—†ìœ¼ë©´ ì•ˆì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ì´ë¥¼ í†µí•´ <span style="color:red">**global nodeëŠ” global informationì„ ë³´ì¡´í•˜ëŠ”ë° ì¢‹ì€ ì†”ë£¨ì…˜ì„ì„ ì…ì¦**</span>í•œë‹¤.

<br/>

### 3) Case Analysis

Table 5.ì—ì„œ ë³´ì—¬ì§€ë“¯, FB15k-237ì˜ ëª‡ëª‡ Tripleì„ í†µí•´ ablation studyë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. Relphormerì˜ ê²½ìš° Structural informationê³¼ Textual featureë¥¼ ë™ì‹œì— ë‹¤ë¤„ ì—”í‹°í‹°ì™€ ë¦´ë ˆì´ì…˜ ë ˆë²¨ì—ì„œì˜ ë‹¤ì–‘ì„± ë¬¸ì œ(Heterogeneity Probelm)ë¥¼ í’€ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Graph Embedding ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ RotatEì™€ ë¹„êµí•˜ë©´ RotatEëŠ” ê·¸ë˜í”„ì˜ êµ¬ì¡° ì •ë³´ë§Œì„ ê°€ì§€ê³  í•™ìŠµí•œë‹¤. ë”°ë¼ì„œ Graphì˜ Textual informationì— ëŒ€í•´ í•™ìŠµí•˜ì§€ ëª»í•´ ì„±ëŠ¥ì´ Relphormerì— ë¹„í•´ ë’¤ì³ì§„ë‹¤. ëŒ€ì¡°ì ìœ¼ë¡œ ì˜¤ì§ Textual Encodingë§Œì„ ê°€ì§€ê³  í•™ìŠµí•œ [StAR](https://meaningful96.github.io/paperreview/5StaR/)ëŠ” ê·¸ë˜í”„ì˜ Context ì •ë³´ë§Œì„ ì´ìš©í•œë‹¤. ë”°ë¼ì„œ, ê·¸ë˜í”„ì˜ êµ¬ì¡° ì •ë³´ê°€ ë°˜ì˜ë˜ì§€ ëª»í•´ Relphormerì— ë¹„êµí•˜ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.

 âœ StARëŠ” Texture Encoding ê¸°ë°˜ì˜ ëª¨ë¸ì´ì§€ë§Œ, StAR(Self-Adp): ì•™ìƒë¸”ì„ í†µí•´ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ê³¼ì—° StAR(Texture Encoding)ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµë¥¼ í•˜ëŠ”ê²Œ í•©ë¦¬ì ì¸ê°€?

<br/>

## 4. Ablation Study 2

<span style="font-size:110%"><b>How effective is the proposed Relphormer model in addressing heterogeneity KG Structure and semantic textual description?</b></span>

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/f7b1f539-86a4-4d98-9612-ce15079f2aaa">
</p>

### 1) The number of sampled contextualized sub-graph triples

Figure 6. Tripleì˜ ìˆ˜ë¥¼ 4ë¶€í„° 64ê¹Œì§€ ë°”ê¿”ê°€ë©° ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. ê·¸ ìˆ˜ê°€ ì‘ì„ ë•ŒëŠ” Center tripleê³¼ ì—°ê²°ëœ ì ì€ ìˆ˜ì˜ contextual nodeë“¤ì´ ìƒ˜í”Œë§ëœë‹¤. ì´ ì‹¤í—˜ì„ í†µí•´ì„œ ëª…í™•í•œ ê²ƒì€, <span style="color:red">**contextualized subgraphì˜ í¬ê¸°ë¥¼ í‚¤ìš°ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤**</span>ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ Tripleì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì§€ë©´ ì„±ëŠ¥ì€ ì¼ì • ìˆ˜ì¤€ ì˜¬ë¼ê°€ë‹¤ê°€ **Saturation**ëœë‹¤. ë…¼ë¬¸ì—ì„œ ì €ìëŠ” ê·¸ ì´ìœ ê°€ neighborhoodì˜ ì •ë³´ê°€ ìœ ìš©í•˜ë”ë¼ë„, <u>ë„ˆë¬´ ë§ì€ unrelatedëœ ì •ë³´ë“¤ì´ ì¼ì¢…ì˜ noiseë¡œ ì‘ìš©</u>í•´ ì„±ëŠ¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¼ì¹œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° noiseì—­í• ì„ í•˜ëŠ” low-quality nodeë“¤ì€ negativ contextual informationìœ¼ë¡œ ì‘ìš©í•œë‹¤.

<br/>

### 2) Structure-Enhanced self-attention

Structure-Enhanced attentionì˜ ì˜í–¥ë ¥ì„ ì¦ëª…í•˜ê¸°ìœ„í•´ Figure 5.ì˜ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. ëª¨ë“  ëª¨ë¸ì´ Structure-Enhanced attentionì„ í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì €ìëŠ” ë¬´ì‘ìœ„ë¡œ ëœë¤í•˜ê²Œ ì˜ˆì‹œë¥¼ ê°€ì ¸ì™€ attention matrixë¥¼ ì‹œê°í™”í•´ structure-enhanced self-attentionì˜ ì˜í–¥ë ¥ì„ ì‹¤í—˜í•˜ì˜€ë‹¤. Figure 7.ì—ì„œì™€ ê°™ì´ Structure-Enhanced Self-Attentionì€ attention weight ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, <span style="color:red">**Structure-Enhanced Self-Attentionê³¼ í•¨ê»˜ êµ¬ì¡° ì •ë³´ë¥¼ ì£¼ì…í•˜ëŠ” ê²ƒì€ ì—”í‹°í‹°ë“¤ì˜ ê±°ë¦¬ì— ì˜ë¯¸ì ì¸ ìƒê´€ê´€ê³„ë¥¼ í¬ì°©**</span>í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ ì—”í‹°í‹°ëŠ” sub-graphë‚´ ë©€ë¦¬ ë–¨ì–´ì§„ ì—”í‹°í‹°ë¥¼ í†µí•´ Structure correlationì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

<br/>
<br/>

# Contribution

1. Transformer ê¸°ë°˜ì˜ ìƒˆë¡œìš´ ëª¨ë¸ì¸ Relphormerë¥¼ ì œì•ˆ
2. 6ê°œì˜ Benchmark Datasetì— ëŒ€í•˜ì—¬ ê¸°ì¡´ì˜ Graph Embedding ëª¨ë¸ë“¤ê³¼ Transformer ê¸°ë°˜ ëª¨ë¸ë“¤ì— ë¹„í•´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
3. <span style ="color:red">Attention biasë¥¼ ì´ìš©í•´ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³  Knowledge Graphì— ì í•©í•œ Self-attention mechanismì„ ì œì‹œ(**Structure enhanced self-attention**)</span>
    - íŠ¹íˆ<span style = "color:green"> $$ \phi(i, j)$$</span>ë¥¼ ì œì‹œí•œ Structure-enhanced Self-attentionì´ ê°€ì¥ í° Contribution

# Reference
[Inductive Biasë€ ë¬´ì—‡ì¼ê¹Œ?](https://re-code-cord.tistory.com/entry/Inductive-Bias%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C)  
["Relational Graph Transformer for Knowledge Graph Completion-Relphormer"](https://meaningful96.github.io/paperreview/Relphormer/)  

