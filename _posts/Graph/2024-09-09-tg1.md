---
title: "[그래프 AI]Training GNNs - Over Smoothing, Over Squashing"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-09-09
last_modified_at: 2024-09-09
---

# GNN 레이어 Stacking

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/50a6f80a-cf13-453d-a2f9-f5583588f0ec">
</p>

**Graph Neural Networks(GNN)**은 노드, 엣지, 그래프와 같은 구조적 데이터를 처리하기 위해 고안된 신경망이다. GNN의 기본 구성 요소는 노드의 특성(feature)과 그래프 구조 정보(엣지)로 이루어져 있으며, 각 노드는 이웃 노드로부터 정보를 받아 자신의 **표현(representation)**을 업데이트한다.

GNN 레이어를 쌓는다는 것은, 이러한 정보 교환 과정을 여러 번 반복한다는 것을 의미한다. 즉, 첫 번째 GNN 레이어에서는 1-hop 거리에 있는 이웃 노드로부터 정보를 받아들여 업데이트하며, 두 번째 레이어에서는 2-hop 거리에 있는 노드까지 정보를 수집하고, 세 번째 레이어에서는 3-hop 거리까지 정보를 모은다.

이 과정에서, 첫 번째 레이어의 입력은 $$h_v(0)$$으로 나타내며, 이는 타겟 노드의 초기 특성 값이다. 각 레이어를 거칠 때마다 $$h_v(1)$$, $$h_v(2)$$ 등의 새로운 노드 표현이 계산되고, 최종적으로 $$h_v(3)$$까지 업데이트된다. 각 레이어에서의 출력은 다음 레이어의 입력으로 사용되며, 이를 통해 노드의 정보를 점점 더 넓은 이웃으로부터 받아들이게 된다. 따라서 <span style="color:red">**GNN 레이어를 많이 쌓는 것은 노드가 멀리 떨어진 이웃 노드의 정보까지도 반영**</span>할 수 있다는 장점이 있다.

많은 층을 쌓으면, 층 수만큼의 hop수만큼 neighbor 정보를 모아올 수 있지만, 반대로 단점도 존재하는데, 바로 **Over-Smoothing**과 **Over Squashing**문제가 발생할 수 있다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/1b7e4191-11fc-4e61-872f-ae6d7d4163f4">
</p>

# Reference
\[1\] CS224W 강의--
title: "[그래프 AI]Training GNNs - GNN 학습 시 고려할 점"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-09-09
last_modified_at: 2024-09-09
---

# GNN Layer Stacking

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/50a6f80a-cf13-453d-a2f9-f5583588f0ec">
</p>

**Graph Neural Networks(GNN)**은 노드, 엣지, 그래프와 같은 구조적 데이터를 처리하기 위해 고안된 신경망이다. GNN의 기본 구성 요소는 노드의 특성(feature)과 그래프 구조 정보(엣지)로 이루어져 있으며, 각 노드는 이웃 노드로부터 정보를 받아 자신의 **표현(representation)**을 업데이트한다.

GNN 레이어를 쌓는다는 것은, 이러한 정보 교환 과정을 여러 번 반복한다는 것을 의미한다. 즉, 첫 번째 GNN 레이어에서는 1-hop 거리에 있는 이웃 노드로부터 정보를 받아들여 업데이트하며, 두 번째 레이어에서는 2-hop 거리에 있는 노드까지 정보를 수집하고, 세 번째 레이어에서는 3-hop 거리까지 정보를 모은다.

이 과정에서, 첫 번째 레이어의 입력은 $$h_v(0)$$으로 나타내며, 이는 타겟 노드의 초기 특성 값이다. 각 레이어를 거칠 때마다 $$h_v(1)$$, $$h_v(2)$$ 등의 새로운 노드 표현이 계산되고, 최종적으로 $$h_v(3)$$까지 업데이트된다. 각 레이어에서의 출력은 다음 레이어의 입력으로 사용되며, 이를 통해 노드의 정보를 점점 더 넓은 이웃으로부터 받아들이게 된다. 따라서 <span style="color:red">**GNN 레이어를 많이 쌓는 것은 노드가 멀리 떨어진 이웃 노드의 정보까지도 반영**</span>할 수 있다는 장점이 있다.

많은 층을 쌓으면, 층 수만큼의 hop수만큼 neighbor 정보를 모아올 수 있지만, 반대로 단점도 존재하는데, 바로 **Over-Smoothing**과 **Over Squashing**문제가 발생할 수 있다. 

## Problem 1. Over Smoothing
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/1b7e4191-11fc-4e61-872f-ae6d7d4163f4">
</p>

Over-Smoothing은 GNN이 깊어질수록 **모든 노드의 임베딩이 동일하게 수렴하는 현상**이다. 이는 GNN 레이어가 쌓일수록 각 노드가 더 멀리 떨어진 이웃으로부터 정보를 받아들이며 발생한다. GNN에서 각 노드는 K-레이어 GNN일 때 **K-hop 거리**까지 이웃 노드로부터 정보를 모을 수 있는데, 이를 **Receptive Field**라고 부른다.**Receptive Field**가 증가할수록, 많은 노드들이 겹치는 정보를 받아들이게 되고, 이는 **각 노드의 임베딩이 구별되지 않고 비슷해지는 문제**를 야기한다. 

예를 들어, 1-hop 이웃의 경우 겹치는 노드가 적지만, 3-hop으로 확장되면 거의 **모든 노드가 공유되는 정보를 받아들이게 된다**. 
결과적으로 GNN의 레이어가 깊어질수록 **모든 노드가 유사한 정보를 취합하게 되고**, 그로 인해 **노드 간 차별성이 사라지면서 모든 노드의 임베딩이 동일하게 수렴하게 된다**. 이러한 현상은 **노드 간의 구별 가능한 표현을 유지하는 데 방해가 되며**, GNN 모델의 성능 저하로 이어질 수 있다.

## Problem 2. Over Squashing
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/6bd3351d-94fc-41da-8ad8-5dc2418a084c">
</p>

**Over-Squashing**은 **특정 노드로부터 이웃 노드들의 정보가 압축되어 전달되는 현상**이다. 이는 GNN이 레이어가 깊어질수록 타겟 노드가 **지수적으로 늘어나는 이웃 노드들의 정보를 고정된 크기의 벡터로 압축**하여 받아들이기 때문이다. Over-Squashing은 **타겟 노드 관점에서 발생하는 문제**로, 멀리 떨어진 이웃에서 중요한 정보를 전달받기 어렵게 만든다. 

예를 들어, GNN 레이어를 깊게 쌓으면 Depth가 깊어질수록 **타겟 노드로부터 멀리 떨어진 이웃 노드들의 정보를 수용하기 어려워진다**. 
결과적으로 hint가 충분히 전달되지 않고, 이는 모델의 성능 저하로 이어진다.

레이어를 무작정 늘린다고 성능이 좋아지지 않으며, **레이어 수가 특정 임계값을 넘어가면 Accuracy가 급격히 떨어지는 현상**이 발생한다. 
이는 Over-Squashing으로 인해 중요한 정보가 고정된 크기의 벡터에 **압축되는 과정에서 손실**되기 때문이다. 따라서, Over-Squashing은 **타겟 노드가 멀리 있는 이웃 노드들의 중요한 정보를 효과적으로 수용하지 못하게 만드는 문제**로 GNN의 성능에 부정적인 영향을 미친다.

## Over-Smoothing 해결책 1

<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/1ede8203-d284-437a-9a2a-9d574b85d71d">
</p>

<span style="font-size:110%">**Solution1: Increase the expressive power within each GNN 레이어**</span>  

Over-Smoothing 문제를 해결하는 첫 번째 방법은 **각각의 얕은 GNN 레이어의 표현력을 강화하는 것**이다. GNN 레이어는 **Message Passing**, **Aggregation**, 필요에 따라 **Combine**을 수행하는 파트로 구성된다. 이러한 과정에서 **표현력을 강화하기 위해** 각각의 함수들을 **Multi-Layer Perceptron(MLP)** 레이어로 구성할 수 있다. 즉, **GNN 레이어 하나당 총 3개의 MLP 레이어로 구성**하여 GNN 레이어의 표현력을 높이는 것이다. 이를 통해, **얕은 GNN 레이어도 충분히 많은 정보를 처리할 수 있는 능력을 갖추게 되고**, Over-Smoothing 문제를 완화할 수 있다.

## Over-Smoothing 해결책 2
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/e9605a29-8d09-4828-a3fd-908d9e4f232c">
</p>

Over-Smoothing 문제를 해결하는 두 번째 방법은 **GNN 레이어에 앞뒤로 MLP 레이어를 추가하여 Deep Neural Net (DNN)을 구성하는 것**이다. 

이는 **다른 도메인에서도 널리 사용되는 방법**으로, 이를 통해 **모델의 표현력을 증가**시킬 수 있다. 구체적으로는, **Pre-Processing 레이어**와 **Post-Processing 레이어**를 추가하여 노드의 특성과 임베딩을 더욱 효과적으로 다룰 수 있다.

- **Pre-Processing 레이어**는 **노드의 feature를 인코딩**하는 역할을 하며, 이미지나 텍스트 같은 데이터를 처리할 때 유용하다.
- **Post-Processing 레이어**는 **추론이나 노드 임베딩에 대한 변환**을 처리하는 역할을 하며, 그래프 분류나 지식 그래프와 같은 작업에 적합하다.

이와 같은 레이어 추가는 **실제 적용 시 모델의 성능을 크게 향상**시키는 것으로 알려져 있다.

## Over-Smoothing 해결책 3
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/44763a6b-3c21-4b48-ac06-a739de6d7c1d">
</p>

Skip-Connection은 **임베딩 결과와 원래 정보가 완전히 상이할 경우 잔차를 학습할 수 있도록 두 정보를 더해주는 방법**이다. 이를 통해 모델이 **정규화(Normalization) 효과**를 얻으며, **일반화(Generalization) 성능을 향상**시킬 수 있다.

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/b78e8612-1cf4-40a5-8fa1-5337bc9802b5">
</p>

Skip-Connection은 **Shortcut**이라고도 하며, 이는 **하나의 Mixture Model**을 구성하는 방식이다. 예를 들어, N개의 Skip-Connection이 존재할 경우, **2^N개의 경로(Path)를 가지게 되며**, 이는 자연스럽게 얕은 GNN과 깊은 GNN을 결합한 **Mixture Model**을 형성한다.


<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/ed68e3e2-80af-43e4-8fff-c4f3af00d993">
</p>

결과적으로, **Skip-Connection을 사용하면 다양한 경로를 통해 정보가 전달**되며, 이를 통해 모델의 성능을 개선할 수 있다. 이는 **잔차 연결(Residual Connection)**의 원리와 유사하며, 모델이 **다양한 깊이에서 정보를 취합**할 수 있도록 돕는다.

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/967267bf-bb3b-4de0-b1be-17ea79f3fe62">
</p>

이 방식은 **GNN 모델에서 종종 사용**된다. 타겟 노드가 $$v$$일 때, $$h_v^{(1)}$$은 1-hop 거리에 있는 이웃으로부터 메시지를 받은 것이고, $$h_v^{(2)}$$와 $$h_v^{(3)}$$은 각각 2-hop과 3-hop까지의 정보를 수집한 값이다. 이 방식에서 **최종 레이어에 모든 입력을 한 번씩 잔차 학습으로 보내는 이유**는, 실제로 **몇 hop까지의 정보가 Receptive Field에 걸치는지 명확하지 않기 때문**이다. 따라서 **모든 hop의 정보를 한 번씩 더 제공하여, 마지막 Aggregation에서 학습을 통해 이를 판단**하도록 만든다. 이 방식은 **마지막 레이어에서의 Aggregation을 통해 모든 hop의 정보를 한꺼번에 모으는 역할**을 하며, 이를 통해 더 깊은 정보와 얕은 정보가 모두 반영된 노드 임베딩을 얻을 수 있다.



# Reference
\[1\] [CS224W 강의](https://web.stanford.edu/class/cs224w/)
