---
title: "[NLP]언어 모델을 위한 평가지표 1. GLUE"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-11
last_modified_at: 2024-08-11
---

# GLUE(General Language Understanding Evaluation)  
## GLUE의 정의
**GLUE (General Language Understanding Evaluation)**는 자연어 이해 능력을 평가하기 위한 벤치마크이다. 즉, 자연어 이해(Natural Language Understanding, <span style="color:red">**NLU**</span>) 태스크을 평가하는 데 사용된다. 이는 텍스트 분류, 문장 관계 추론, 텍스트 유사도 평가 등 다양한 NLP 태스크을 포함한다.

NLU의 핵심 과제는 일반화(`generality`), 유연성(`flexibility`), 강건성(`robust`) 등이 인간이 언어를 이해하는 능력을 모델이 학습하는 것이다. 따라서, GLUE는 NLU기반의 모델을 평가하기 위한 지표인 것이다. GLUE를 제안한 저자들은 인간의 언어 이해 방식과 유사한 여러 태스크를 만들어 NLU 모델의 성능을 측정하고자 하였다. 

- Question Answering
- Sentiment Analysis
- Textual Entailment

## GLUE Task
GLUE는 (논문에서) 9개의 영어 문장 이해 태스크를 제공한다. GLUE 벤치마크로 NLU Task에서 저자들이 달성하고자 하는 목표는 일반화 가능한 자연어 이해 시스템을 발전시키는 것이라고 한다. 이 아홉 개의 태스크는 크게 1.**Sentence Pair Tasks**와 2.**Single Sentence Classification** 두 가지 카테고리로 분류된다. 

1. **Sentence pair tasks**
  - **MNLI**, Multi-Genre Natural Language Interface
  - **QQP**, Quora Question Pairs: detect paraphrase questions
  - **QNLI**, Question Natural Language Interface
  - **STS-B**, The Semantic Textual Similarity Benchmark
  - **MRPC**, Microsoft Research Paraphrase Corpus
  - **RTE**, Recognizing Textual Entailment
  - **WNLI**, Winograd NLI (small natural language inference dataset)

2. **Single sentence classification**
  - SST-2, The Standard Sentiment Treebank
  - CoLA, The Corpus of Linguistic Acceptability: detect  whether sentences are grammatical.

### 1) MNLI
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/122af87c-698d-416e-891b-e37b3f564ab3" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) [1] A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference (Williams et al., 2018)</em>
  </figcaption>
</figure>

**Multi-Genre Natural Language Interface(MNLI)**는 자연어 이해(Natural Language Understanding, NLU) 평가를 위한 벤치마크 데이터셋으로, 특히 <span style="color:red">**문장 간의 언어적 함의(textual entailment)를 평가**</span>하는 데 중점을 둔다. 이 데이터셋은 Stanford에서 제안한 "Recognizing Textual Entailment (RTE)" 태스크의 확장판으로, 다양한 장르의 텍스트를 포함하고 있어, 모델이 다양한 맥락에서 언어적 함의를 이해할 수 있는지 평가하는 데 유용하다.

MNLI 태스크에서는 각 샘플이 **전제(premise)**와 **가설(hypothesis)**로 이루어진 문장 쌍으로 구성된다. 모델은 주어진 전제와 가설 사이의 관계를 세 가지 범주 중 하나로 분류해야 한다.

- **Entailment**(포함):  전제 문장이 참이라면, 가설 문장도 참이 되어야 하는 관계이다. 예를 들어, 전제가 "모든 새는 날 수 있다"이고, 가설이 "참새는 날 수 있다"라면, 이 두 문장 사이에는 포함 관계가 성립된다.
- **Contradiction**(모순): 전제 문장이 참이라면, 가설 문장은 거짓이어야 하는 관계이다. 예를 들어, 전제가 "모든 새는 날 수 있다"이고, 가설이 "펭귄은 날 수 없다"라면, 이 두 문장은 모순 관계에 있다.
- **Neutral**(중립): 전제 문장이 참이더라도, 가설 문장의 참과 거짓 여부를 결정할 수 없는 관계이다. 예를 들어, 전제가 "사람들이 공원에서 산책을 하고 있다"이고, 가설이 "사람들이 피크닉을 즐기고 있다"라면, 두 문장 사이의 관계는 중립이다. 전제가 참이더라도 가설이 참인지 거짓인지를 알 수 없다.

MNLI의 특징 중 하나는 다양한 장르의 텍스트를 포함하고 있다는 점이다. 뉴스, 픽션, 법적 문서, 포럼 게시물 등 여러 장르에서 발췌된 문장 쌍을 포함함으로써, **모델이 장르에 따른 언어적 특성을 고려할 수 있는지를 평가**한다. 이러한 특성은 모델의 일반화 능력을 테스트하는 데 중요한 역할을 하며, 다양한 맥락에서 자연어 함의 추론 태스크의 성능을 측정하는 데 기여한다.

MNLI는 또한 "Matched"와 "Mismatched"라는 두 가지 평가 세트를 제공한다. Matched 세트는 모델이 훈련된 것과 같은 장르의 텍스트로 구성된 반면, Mismatched 세트는 다른 장르의 텍스트로 구성되어 있어, 모델의 도메인 전이 능력을 평가할 수 있다. 이 데이터셋은 자연어 이해, 특히 텍스트 간의 함의 관계를 평가하는 데 중요한 역할을 하며, 다양한 NLU 모델의 성능을 비교하는 데 널리 사용된다. 

<br/>

### 2) QQP

**The Quora Question Pairs (QQP)** 데에터셋은 <span style="color:red">**질문의 의미론적 중복 여부를 판별**</span>하기 위해 구성된 데이터셋으로, 자연어 처리(NLP) 분야에서 특히 중복 질문 검출 태스크에 널리 사용된다. 이 데이터셋은 Quora 플랫폼에서 수집된 질문 쌍들로 구성되어 있으며, 각 질문 쌍이 의미론적으로 동일한지 여부를 판단하는 과제를 포함한다. 이는 사용자가 같은 질문을 반복하지 않도록 하거나, 중복 질문을 효과적으로 관리하기 위해 중요한 역할을 한다.

QQP 데이터셋에서는 각 샘플이 두 개의 질문으로 이루어져 있으며, 이 두 질문이 같은 의미를 가지는지 여부에 따라 이진 분류 라벨이 부여된다. 라벨은 두 가지 범주로 나뉜다.

- **중복(Duplicate)**: 두 질문이 의미적으로 동일하거나 거의 동일한 정보를 묻고 있을 때 부여된다. 예를 들어, "What is the capital of France?"와 "Which city is the capital of France?"와 같은 질문 쌍은 중복으로 간주된다.
- **비중복(Non-Duplicate)**: 두 질문이 다른 정보를 묻거나, 의미적으로 다를 때 부여된다. 예를 들어, "How to learn programming?"과 "What are the best programming languages?"와 같은 질문 쌍은 비중복으로 간주된다.

QQP 데이터셋에서 중요한 문제 중 하나는 **라벨 불균형 문제**이다. **대부분의 질문 쌍이 비중복으로 라벨링**되어 있으며, 이는 모델이 학습할 때 중복 질문을 제대로 인식하지 못할 가능성을 높인다. 이 때문에 단순히 정확도(`Accuracy`)를 사용하는 것만으로는 모델의 성능을 적절하게 평가하기 어려울 수 있다.

따라서, QQP 태스크에서는 `F1-Score`와 `Accuracy` 두 가지 평가지표를 함께 사용하여 모델 성능을 평가한다. `F1-Score`는 정밀도(`Precision`)와 재현율(`Recall`)을 조화 평균한 값으로, 특히 라벨 불균형 상황에서 모델의 성능을 더 균형 있게 평가할 수 있는 지표이다. 정밀도는 모델이 중복 질문으로 예측한 사례 중 실제로 중복인 비율을 나타내며, 재현율은 실제 중복 질문 중 모델이 올바르게 예측한 비율을 나타낸다.` F1-Score`는 이 두 측면을 균형 있게 평가함으로써, 모델이 중복 질문을 정확히 인식하는 능력을 측정하는 데 유용하다. ([F1-Score](https://meaningful96.github.io/deeplearning/evaluation_metric/#3-f1-score))

반면, `Accuracy`는 전체 예측 중에서 올바르게 분류된 사례의 비율을 나타내며, 모델의 전반적인 성능을 평가하는 데 사용된다. 그러나 **라벨 불균형이 심한 경우,** `Accuracy`**가 높더라도 실제로 모델이 중요한 범주(즉, 중복 질문)를 잘 인식하지 못할 수 있기 때문에,** `F1-Score`**와 함께 사용하는 것이 일반적**이다.

QQP 데이터셋은 자연어 처리에서 중복성 검출과 같은 실용적인 문제를 해결하기 위한 중요한 자원으로 활용되며, 모델이 의미적으로 동일한 문장을 얼마나 잘 인식할 수 있는지를 평가하는 데 널리 사용된다. 이 데이터셋은 특히 Quora와 같은 Q&A 플랫폼에서 유사 질문을 효율적으로 처리하고 관리하는 데 중요한 역할을 한다.

<br/>

### 3) QNLI
**QNLI(Question Natural Language Inference)** 원래 **The Stanford Question Answering Dataset (SQuAD)**에서 파생된 데이터셋이다. 이 데이터셋은 질문-응답 태스크에서 발전된 형태로, 문단-질문 쌍으로 구성되어 있다. 각 문단은 여러 문장으로 이루어져 있으며, 그중 한 문장이 사람이 만든 질문에 대한 답을 포함하고 있다.

이 태스크에서 문제를 약간 변형하여, <span style="color:red">**단순한 질문-응답 태스크가 아닌 문장 쌍 분류 태스크로 확장**</span>했다. 구체적으로, 문단 내에서 질문의 내용을 담고 있는 문장과 질문 문장을 쌍으로 만들어, 이들 문장 쌍이 질문에 대한 답을 제공하는지 여부를 분류하는 태스크을 수행한다. 또한, 중복되는 어휘가 적은 문장 쌍을 필터링하는 과정을 통해, 단순히 어휘가 겹치는 문장 쌍을 모델이 정답으로 추론하지 못하도록 했다.

결과적으로, QNLI 태스크에서 모델이 수행해야 할 태스크은 context sentence(문단 내의 원래 문장)가 질문에 대한 답을 담고 있는지를 분류하는 것이다. 이 태스크는 단순한 어휘 매칭이 아닌, 문장의 의미를 이해하고 추론하는 능력을 요구한다.

저자는 이러한 태스크 변형을 통해, 모델이 "단순히 겹치는 어휘쌍이 많은 문장 쌍"을 정답으로 추론하는 것을 방지하고, 보다 일반화된 성능을 평가할 수 있다고 주장한다. 즉, QNLI는 모델이 의미론적 추론 능력을 통해 문장 간의 관계를 이해하는지를 측정하는 중요한 도구로 활용된다. 이로 인해, QNLI는 자연어 추론(NLI) 및 의미 이해 관련 연구에서 중요한 역할을 한다.


<br/>

### 4) STS-B

**STS-B(The Semantic Textual Similarity Benchmark)**는 <span style="color:red">**문장 간의 의미적 유사성을 평가**</span>하기 위해 구성된 데이터셋이다. 이 데이터셋은 다양한 출처에서 가져온 문장 쌍들로 이루어져 있으며, 여기에는 뉴스 헤드라인, 이미지 및 비디오 캡션, 그리고 자연어 추론 데이터 등이 포함된다. 각 문장 쌍은 인간 평가자들이 1 ~ 5까지의 척도로 유사도를 라벨링한 데이터를 포함하고 있으며, 1은 거의 유사하지 않음을, 5는 매우 유사함을 의미한다.

STS-B 태스크는 주어진 문장 쌍에 대해 모델이 이 유사도를 예측하는 태스크을 수행하도록 한다. 즉, 모델이 두 문장 간의 의미적 유사성을 정량적으로 평가하는 능력을 측정하는 태스크이다. 이 데이터셋에서 모델의 성능을 평가하기 위해 **피어슨(Pearson)** 상관 계수와 **스피어먼(Spearman)** 상관 계수라는 두 가지 주요 지표가 사용된다.

**Pearson** 상관 계수는 **예측된 유사도와 실제 라벨 간의 선형 상관 관계를 측정**하며, 값이 1에 가까울수록 예측과 실제 라벨이 강하게 일치함을 의미한다. $$r$$은 피어슨 상관계수이고, $$x_i, y_i$$는 관측값, $$\overline{x}, \overline{y}$$는 두 변수의 평균값이다. $$n$$은 관측값의 개수를 의미한다.

<center>$$r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}$$</center>


**Spearman** 상관 계수는 **순위 기반 상관 관계를 측정**하는데, 이는 **예측된 유사도와 실제 라벨 간의 비선형 관계**도 잘 평가할 수 있도록 한다. Spearman 상관 계수는 두 변수 간의 일관된 순위를 유지하는 정도를 측정하며, 모델이 순위를 얼마나 잘 예측하는지를 평가한다. $$\rho$$는 스피어먼 상고간계수, $$d_i$$는 각 관측값 쌍의 순위 차이(= $$x_i$$와 $$y_i$$ 간의 순위 차이), $$n$$은 관측값의 개수를 의미한다.

<center>$$\rho = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}$$</center>

STS-B는 자연어 처리 분야에서 **문장 간의 의미적 유사성**을 평가하는 모델의 성능을 비교하는 데 중요한 벤치마크로 사용되며, 다양한 문맥과 출처에서 온 문장을 다루기 때문에 모델의 전반적인 유사성 인식 능력을 평가하는 데 유용하다.

<br/>

### 5) MPRC
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/95a94c08-696c-4518-85b3-d09958593a37" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) [3] Automatically Constructing a Corpus of Sentential Paraphrases (Dolan & Brockett, 2005)</em>
  </figcaption>
</figure>


**MRPC(The Microsoft Research Paraphrase Corpus)** 문장 쌍과 그에 대한 라벨로 구성된 데이터셋이다. 이 데이터셋은 주로 <span style="color:red">**두 문장이 의미론적으로 동일한지를 판별하는 이진 분류 태스크**</span>에 사용된다. MRPC 태스크에서 각 문장 쌍은 두 가지 범주로 라벨링된다.

- **Paraphrase(동의어)**: 두 문장이 의미론적으로 동일하거나 매우 유사할 때 부여된다.
- **Non-Paraphrase(비동의어)**: 두 문장이 의미론적으로 다를 때 부여된다.

이 데이터셋에서 중요한 점은 QQP와 마찬가지로 **라벨 불균형 문제**이다. 즉, 대부분의 문장 쌍이 특정 라벨(예: 비동의어)로 분류되는 경향이 있어, 모델이 불균형한 데이터를 학습할 때 성능 평가에 어려움이 생길 수 있다. 이 때문에, MRPC 태스크에서는 모델 성능을 평가하기 위해 `F1-Score`와 `Accuracy` 두 가지 평가지표를 함께 사용한다.

- **Accuracy**는 전체 예측 중에서 올바르게 분류된 사례의 비율을 나타내며, 모델의 전반적인 성능을 평가하는 데 사용된다.
- **F1-Score**는 정밀도(Precision)와 재현율(Recall)의 조화 평균으로, 특히 라벨 불균형 상황에서 모델의 성능을 더 균형 있게 평가할 수 있는 지표이다. 정밀도는 모델이 Paraphrase로 예측한 문장 쌍 중 실제로 Paraphrase인 비율을 나타내고, 재현율은 실제 Paraphrase 문장 쌍 중에서 모델이 올바르게 예측한 비율을 의미한다.

MRPC는 자연어 처리 분야에서 의미론적 유사성을 측정하고, 문장 간의 동의어 관계를 평가하는 모델의 성능을 비교하는 데 중요한 벤치마크로 널리 사용된다. 이 데이터셋은 모델이 의미론적으로 유사한 문장을 얼마나 잘 인식할 수 있는지를 평가하는 데 중점을 둔다.

<br/>

### 6) RTE

**RTE**는 **Recognizing Textual Entailment**의 줄임말로, 연례 텍스트 함의 챌린지(annual textual entailment challenge)에서 가져온 데이터를 사용한다. 이 태스크는 MNLI와 유사하지만, **Neutral**과 **Contradiction** 라벨을 **Not-Entailment**로 합쳐 <span style="color:red">**이진 분류 태스크**</span>로 구성된 것이 특징이다.

- Entailment
- Non-Entailment = Neutral + Contrardiction

<br/>

### 7) WNLI
**WNLI**는 **Winograd NLI**의 줄임말로, The Winograd Schema Challenge에서 데이터를 가져왔다. 이 챌린지는 시스템이 대명사가 포함된 문장을 읽고, 그 대명사가 가리키는 대상을 리스트에서 선택하는 과제이다.

이 과제를 태스크로 만들기 위해, 저자는 가능한 모든 참조 대상에 대해 모호한 대명사를 조합하여 문장 쌍을 구성했다. WNLI 태스크의 목표는 <span style="color:red">**대명사가 포함된 문장이 원래 문장의 함의를 담고 있는지를 예측**</span>하는 것이다.

```bash
Substituted Sentence (대명사 포함 문장)
"I put the cake away in the refrigerator. It has a lot of leftovers in it."

Original Sentence
"The refrigerator has a lot of leftovers in it."
```
이 예시에서 대명사 "It"이 원래 문장의 "the refrigerator"를 정확히 가리키고 있으므로, 모델은 이를 1로 예측해야 맞다. 

<br/>

### 7) CoLA
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/73edeae0-3341-4413-a5e7-daab6b76633e" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) [3] Neural Network Acceptability Judgements (Warstadt et al., 2018)</em>
  </figcaption>
</figure>

CoLA는 2. Single sentence classification에 속하는 태스크로, The Corpus of Linguistic Acceptability의 줄임말이다. 이는 "언어학적으로 수용 가능한 문장인지" 판별하는 태스크이다. 이 태스크는 수용 가능한지 아닌지를 판별하기 때문에 **이진 분류(Binary Classification)**에 해당한다. 평가 지표는 Matthews Correlation Coefficient(MCC)를 사용한다.

<center>$$\text{MCC} = \frac{\text{TF} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FP})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}$$</center>

MCC는 상관계수라서 값은 \[$$-1, 1$$\] 사이로 나오고, 값이 $$0$$이면 Uinformed Guessing이라고 한다.

<br/>

### 8) SST-2
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/d50d534b-3906-42a8-abbc-331802830f6f" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) [4] github.com/pilsung-kang</em>
  </figcaption>
</figure>

**SST**는 **The Stanford Sentiment Treebank**의 줄임말로, 영화 리뷰와 그에 따른 긍정/부정 라벨링이 포함된 데이터셋이다. 이 데이터셋의 태스크는 <span style="color:red">**문장이 주어졌을 때 감성 분석을 통해 해당 문장의 감성이 긍정적인지 부정적인지 구분하는 이진 분류**</span>를 수행하는 것이다.

SST는 영화 리뷰 텍스트를 기반으로 하며, 문장 단위로 긍정 또는 부정 라벨이 부여되어 있어, 모델이 텍스트의 감성을 이해하고 분류하는 능력을 평가하는 데 사용된다.

<br/>

# Reference  
\[1\] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics.    
\[2\] William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005)  
\[3\] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments.    
\[4\] Github: [github.com/pilsung-kang](https://github.com/pilsung-kang/Text-Analytics)  
\[5\] 블로그: [GLUE - 이것은 "풀"이 아니다 (자연어 이해 벤치마크를 파헤쳐 보자!)](https://velog.io/@raqoon886/GLUE)
