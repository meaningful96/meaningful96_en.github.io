---
title: "[NLP]언어 모델을 위한 평가지표 2. BLEU와 ROUGE"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-11
last_modified_at: 2024-08-11
---

BLEU와 ROUGE는 최근 활발히 연구가 진행되는 **Natural Language Generation(NLG) 모델들을 위한 평가 방법**이다. 즉, Generated Sentence를 평가하는 방식이다. Generative Model은 주로 Supervised Learning 방식으로 정답이 되는 Reference Sentence가 있다. 모델로부터 생성되는 문장을 Generated Sentence 그리고 비교하는 정답을 Reference Sentence라고 한다. 이렇게 생성된 문장을 평가하는 두 방식이 바로 BLEU와 ROUGE인 것이다.

- **BLEU**: Reference Setence의 단어가 Generated Sentence에 포함되는 정도.
- **ROUGE**: Generated Sentence의 단어가 Reference Sentence에 포함되는 정도.


# BLEU 란?

BLEU는 기계 번역의 품질을 평가하는 지표로, 주어진 <span style="color:red">**기계 번역 결과(Generated Sentence)와 하나 이상의 레퍼런스(Reference Sentence) 번역 간의 유사성을 측정**</span>한다. BLEU 점수는 **n-gram 매칭**을 기반으로 하며, **정답과의 일치 비율을 계산**한다. BLEU는 정답과 비교할 때 번역된 문장 내에서 n-gram이 얼마나 잘 매칭되는지를 파악하고, 일반적으로 `Precision`을 측정하여 점수를 계산한다.

BLEU의 핵심 개념 중 하나는 '**Brevity Penalty(길이 패널티)**'이다. 기계 번역이 너무 짧을 경우, 일치하는 n-gram의 비율이 높아져 높은 `Precision`을 얻을 수 있지만, 번역 결과가 의미적으로 부정확해질 수 있다. 이를 방지하기 위해 BLEU는 Brevity Penalty를 도입하여 너무 짧은 번역에 페널티를 부여한다. BLEU 점수는 다음 수식에 의해 계산된다.

<center>$$\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)$$</center>

여기서 $$BP$$가 Brevity Penalty로 정의되며, 이는 다음과 같이 계산된다.

<center>$$BP = \begin{cases} 1 & \text{if } c > r \\
\exp(1 - \frac{r}{c}) & \text{if } c \leq r 
\end{cases}$$</center>

여기서 $$c$$는 번역된 문장의 길이이고, $$r$$는 참조 번역의 길이이다. $$p_n$$은 n-gram precision을 나타내며, $$w_n$$은 각 n-gram에 대한 가중치를 의미한다. 일반적으로 모든 n-gram에 대해 동일한 가중치를 부여한다. BLEU 스코어의 결과는 \[0, 1\] 사이의 값을 가진다. BLEU 점수가 0이면 번역 결과가 참조 번역과 전혀 겹치지 않음을 의미하며, 이는 번역 품질이 낮음을 나타낸다. 반면, BLEU 점수가 1이면 번역 결과가 참조 번역과 완벽하게 일치함을 의미하며, 이는 번역 품질이 높음을 나타낸다.

- BLEU Score $$\sim$$ 1: High Quality
- BLEU Score $$\sim$$ 0: Low Quality

BLEU는 유용하지만 완벽하지 않은 지표이다. 문장을 번역하는 데에는 여러 가지 유효한 방법이 존재하며, 좋은 번역일지라도 인간 번역과의 n-gram 겹침 비율이 낮으면 BLEU 점수가 낮게 나올 수 있다. 이처럼 BLEU 점수는 번역 품질을 평가하는 유용한 지표이지만, 다양한 번역 방식과 의미적 유효성을 모두 반영하기에는 한계가 있다. 

## BLEU 스코어 계산 보충

<center>$$\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)$$</center>

위에서 사용한 수식은 가장 일반화된 형태로, n-gram precision $$p_n$$의 로그 평균을 지수함수를 통해 계산한다. 그 값에 지수함수를 적용한다. 또한 Brevity Penalty (BP): 여기서는 번역된 문장의 길이 $$c$$와 참조 문장의 길이 $$r$$에 따라 BP가 계산된다. 만약 로그 평균에서 모든 n-gram에 동일한 가중치를 부여하면 $$w_n = \frac{1}{N}$$이 된다.

여기서 n-gram의 로그 평균이 아닌, n-gram의 곱셈 형태로 나타낼 수 있다. 이해를 돕기 위해 4-gram으로 가정하고 수식을 전개하면 다음과 같다. 먼저 n-gram percision과 가중치이다.

<center>$$precision_i = \frac{\sum_{s \in \text{cand-corpus}} \sum_{i \in S} \min\left(m^i_{\text{cand}}, m^i_{\text{ref}}\right)}{w^i_t}$$</center>

<center>$$w^i_t = \sum_{s' \in \text{cand-corpus}} \sum_{i' \in S'} m^{i'}_{\text{cand}}$$</center>

이 때, Notation은 다음과 같다.
- $$m^i_{\text{cand}}$$는 후보 번역에서 참조 번역과 일치하는 i-gram의 개수
- $$m^i_{\text{ref}}$$는 참조 번역에서 i-gram의 개수
- $$w^i_t$$는 후보 번역에서 i-gram의 총 개수

최종적으로 BLEU Score에서 n-gram pecision의 곱 형태는 다음과 같다.

<center>$$BLEU = \min\left(1, \exp\left(1 - \frac{|reference|}{|output|}\right)\right)\left(\prod_{i=1}^{4} precision_i\right)^{\frac{1}{4}}$$</center>

**Brevity penalty**는 다시 한 번 설명하면, **생성된 번역이 참조 번역의 가장 가까운 길이보다 너무 짧을 경우 지수 감소를 통해 페널티를 부여**하는 것이다. 이 방법은 BLEU 점수에 recall 용어가 없다는 점을 보완한다.

N-gram overlap은 몇 개의 unigram, bigram, trigram, 그리고 4-gram(i=1,...,4)이 참조 번역에서 대응되는 n-gram과 일치하는지를 세는 방법이다. 이 용어는 precision 지표로 작용한다. Unigram은 적합성을 나타내고, 더 긴 n-gram은 번역의 유창성을 나타낸다. 과다 계산을 피하기 위해 n-gram의 개수는 참조 번역에서 발생하는 최대 n-gram 개수로 클리핑된다.

처음 일반화된 로그 평균 기반의 BLEU 스코어와의 차이는 크게 두 가지이다.

- **가중치**
  - 로그 평균 기반의 식에서는 가중치를 $$w_n$$으로 명시적으로 나타내며, 모든 n-gram에 동일한 가중치를 부여할 수 있다.
  - 반면, 평균 식에서는 n-gram의 precision을 개별적으로 계산하여 곱한다.

- **Brevity Penalty 적용 방식**
  - 로그 평균 수식에서는 Brevity Penalty를 점수에 직접 곱하는 방식이다.
  - 곱셈 수식에서는 $$\text{min}(1,BP)$$를 취하여 Brevity Penalty가 적용된 형태로 계산한다. 

## BLEU Score 예시

- **Reference**: "The NASA Opportunity rover is battling a massive dust storm on Mars."
- **Candidate 1**: "The Opportunity rover is combating a big sandstorm on Mars."
- **Candidate 2**: "A NASA rover is fighting a massive storm on Mars."

<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/b1561fe9-66cf-4a6f-b113-bcd9231ca997">
</p>

<span style="font-size:105%">**Bigram Pecision (2-gram)**</span>  
**Candidate 1**:
- 일치하는 bigram: "Opportunity rover", "rover is", "on Mars", "Mras ."
- **Precision** = $$\frac{4}{10} = 0.400$$

**Candidate 2**:
- 일치하는 bigram: "Opportunity rover", "rover is", "a massive", "on Mars", "Mars ."
- **Precision** = $$\frac{5}{10} = 0.500$$

<span style="font-size:105%">**4-gram Pecision**</span>    
**Candidate 1**:
- 일치하는 4-gram: 없음
- **Precision** = $$\frac{0}{8} = 0.000$$

**Candidate 2**:
- 일치하는 4-gram: "storm on Mars ."
- **Precision** = $$\frac{1}{8} = 0.125$$

<br/>
<br/>


# ROUGE 란?

**ROUGE(Recall-Oriented Understudy for Gisting Evaluation)**는 주로 <span style="color:red">**텍스트 요약 시스템의 성능을 평가**</span>하기 위해 사용되는 지표로, 기계 번역이나 텍스트 생성 시스템의 평가에도 널리 활용된다. ROUGE는 참조 텍스트와 생성된 텍스트 사이의 유사성을 측정하며, 다양한 변형 지표를 통해 평가를 진행한다. 특히 ROUGE는 Precision을 중심으로 평가하는 BLEU와는 달리, **`Recall`을 중심으로 평가**를 하지만, `Precision`과 `F1-Score`도 함께 고려하는 다양한 지표를 제공한다.

ROUGE에는 여러 가지 변형이 있지만, 가장 널리 사용되는 것은 ROUGE-N과 ROUGE-L이다. ROUGE-N은 n-gram recall을 측정하며, ROUGE-L은 Longest Common Subsequence(LCS)를 기반으로 평가한다. ROUGE 점수는 0에서 1 사이의 값으로, 1에 가까울수록 요약의 성능이 높다는 것을 의미한다.

## ROUGE-N
**ROUGE-N**은 n-gram recall을 측정하는 지표이다. 이는 **생성된(candidate) 문장과 참조(reference) 문장 사이에서 n-gram이 얼마나 잘 매칭되는지를 평가**한다. ROUGE-N은 요약된 텍스트에서 참조 텍스트와 일치하는 n-gram의 비율을 계산한다. 일반적으로 n의 값은 1, 2 또는 3으로 설정되며, 각각 ROUGE-1, ROUGE-2, ROUGE-3로 불린다. ROUGE-N 점수는 다음과 같은 수식으로 계산된다.

<center>$$\text{ROUGE-N} = \frac{\sum_{S \in \text{References}} \sum_{\text{gram}_n \in S} \text{Count}_{\text{match}}(\text{gram}_n)}{\sum_{S \in \text{References}} \sum_{\text{gram}_n \in S} \text{Count}(\text{gram}_n)}$$</center>

여기서 $$\text{Count}_{\text{match}}(\text{gram}_n)$$은 참조 문장과 생성된 문장 사이의 n-gram 일치 수를, $$\text{Count}(\text{gram}_n)$$은 참조 문장에서의 n-gram의 총수를 의미한다.

## ROUGE-L
**ROUGE-L**은 Longest Common Subsequence(LCS)를 기반으로 한 지표이다. LCS는 두 시퀀스 사이에서 가장 긴 공통 부분 시퀀스를 찾아내는 방법으로, ROUGE-L은 이 **LCS를 기반으로 요약된 텍스트가 참조 텍스트와 얼마나 유사한지를 평가**한다. LCS를 사용하면 단순한 n-gram 기반 평가보다 더 유연하게 일치하는 부분을 찾을 수 있어, 문장의 순서가 다소 달라지더라도 중요한 정보를 포함하고 있는지를 평가할 수 있다.

<center>$$\text{ROUGE-L} = \frac{LCS(\text{reference}, \text{candidate})}{\text{length of reference}}$$</center>

여기서 $$LCS(\text{reference}, \text{candidate})$$는 참조 문장과 생성된 문장 간의 최장 공통 부분 시퀀스의 길이를 의미하고, $$\text{length of reference}$$는 참조 문장의 길이를 의미한다.

<br/>
<br/>
