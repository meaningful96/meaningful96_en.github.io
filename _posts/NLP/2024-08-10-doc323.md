---
title: "[NLP]N-gram 언어 모델: 전통적인 통계적 자연어 처리 방식"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-10
last_modified_at: 2024-08-10
---

# N-gram의 정의
언어 모델(Language Model)은 문장 내 앞서 등장한 단어를 기반으로 이어서 등장할 적절한 단어를 예측하는 모델이다. 언어 모델의 발전 방향은 크게 두 가지로 나눠진다.

- 통계학 기반의 언어 모델
- 인공신경망 기반의 언어 모델

N-gram은 이 중에서도 통계학 기반의 언어 모델이다. <span style="color:red">**특정 단어가 주어진 문맥에서 나타날 확률을 계산**하는 모델이다. 여기서 "$$N$$"은 문맥에서 고려하는 단어의 개수를 의미한다. 예를 들어, 2-gram(또는 bigram) 모델은 두 개의 연속된 단어 쌍을, 3-gram(또는 trigram) 모델은 세 개의 연속된 단어를 기반으로 확률을 계산한다.

다시 말해, N-gram 언어 모델은 **$$N$$개의 연속적인 토큰(단어 또는 문자)을 하나의 그룹으로 간주**하여 그 단위들의 출현 빈도와 확률을 계산하는 방식이다. 이를 통해 모델은 문장에서 특정 단어가 주어진 문맥에서 나올 확률을 예측한다. 예를 들어, "The cat is on the mat"이라는 문장이 있을 때, 2-gram(또는 bigram) 모델은 "The cat", "cat is", "is on", "on the", "the mat"와 같은 두 단어씩 짝지은 단위들을 고려하여 각 단위의 출현 빈도와 확률을 계산한다. 이 경우, 모델은 각각의 두 단어 쌍을 독립적인 단위로 보고, 그 단위들이 나타날 확률을 계산하는 것이다. 예를 들어, "The cat"이라는 bigram의 확률은 "The"가 먼저 주어졌을 때 "cat"이 뒤따를 확률로 계산된다.

<br/>
<br/>

# N-gram의 수학적 정의
N-gram 언어 모델은 통계적 언어 모델에서 발생하는 희소성(Spasity) 문제를 극복하기 위해 고안된 방법이다. 예를 들어, $$N$$개의 단어로 이루어진 문장에서 다음 단어를 예측할 때, 전통적인 통계적 언어 모델(SLM)은 이전의 $$N$$개의 단어 전체를 고려하여 다음 단어의 확률을 계산한다. 이 과정에서 모델은 학습에 사용된 코퍼스에서 $$N$$개의 단어와 그 순서가 정확히 일치하는 문장이 몇 번 나타나는지, 그리고 그 뒤에 이어지는 단어가 포함된 문장이 얼마나 자주 등장하는지를 세어 확률을 산출한다.

하지만, 아무리 많은 문장을 학습에 사용하더라도 동일한 단어와 순서로 구성된 문장이 자주 등장하기는 어렵다. 또한, 그 뒤에 이어지는 단어까지 포함된 문장이 코퍼스에서 반복적으로 나타나는 경우도 드물다. N-gram 모델은 문장에서 일부 단어만을 고려함으로써 이러한 한계를 줄이고, 문장이 코퍼스에서 나타날 확률을 보다 효율적으로 계산할 수 있도록 설계되었다.

N-gram 언어 모델의 주요 아이디어는 각 단어의 발생 확률이 해당 단어 이전의 $$N-1$$개 단어에 의존한다는 가정에 기반한다. 이 모델은 다음과 같은 수식으로 표현할 수 있다. 이 수식은 N-gram 언어 모델이 $$N-1$$개의 이전 단어만을 고려하여 현재 단어의 확률을 계산함을 보여준다.

<center>$$P(w_1, w_2, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i \mid w_{i-($$N-1$$)}, \dots, w_{i-1})$$</center>

<br/>
<br/>

# N-gram의 종류

# 사진 첨부(중앙 정렬)
<p align="center">
<img width="500" alt="1" src="https://github.com/user-attachments/assets/e1c7e1cd-9a0e-43f7-9dbf-6975b9d31417">
</p>

## 1. Unigram 모델
Unigram 모델은 $$N=1$$인 경우로, 각 단어의 발생 확률이 독립적으로 계산된다. 이 모델은 이전 단어에 관계없이 단어의 출현 빈도를 기반으로 확률을 부여한다.

<center>$$P(w_1, w_2, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i)$$</center>

## 2. Bigram 모델
Bigram 모델은 N=2인 경우로, 각 단어의 발생 확률이 그 이전 단어에만 의존한다. 예를 들어, "the cat"이라는 단어 쌍에서 "cat"의 확률은 "the"라는 단어에 의해 결정된다.

<center>$$P(w_1, w_2, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i \mid w_{i-1})$$</center>

## 3. Trigram 모델
Trigram 모델은 N=3인 경우로, 현재 단어의 확률이 이전 두 개의 단어에 의해 결정된다. 예를 들어, "the cat is"라는 단어 시퀀스에서 "is"의 확률은 "the cat"이라는 문맥에 따라 달라진다.

<center>$$P(w_1, w_2, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i \mid w_{i-2}, w_{i-1})$$</center>

## 4. Higher-order N-gram 모델
Trigram 모델을 넘어서는 N-gram 모델은 N이 더 큰 경우를 의미하며, 더 많은 이전 단어를 고려하여 현재 단어의 확률을 계산한다. N이 커질수록 모델은 문맥을 더 깊이 이해할 수 있지만, 데이터 희소성 문제와 계산 복잡성도 함께 증가한다.

<br/>
<br/>

# N-gram 언어 모델의 한계점

# 1. 데이터 희소성 문제
N-gram 언어 모델의 가장 큰 한계 중 하나는 **데이터 희소성(Data Sparsity) 문제**이다. **$$N$$이 커질수록 특정 N-gram이 텍스트에서 나타날 확률이 낮아진다**. 예를 들어, 4-gram 또는 5-gram과 같은 고차 N-gram을 사용할 때, "The quick brown fox jumps"라는 4-gram이 코퍼스에 얼마나 자주 등장할 수 있을까? 실제로 이러한 특정한 조합은 매우 드물며, 이로 인해 모델은 이러한 드문 조합에 대한 신뢰할 수 있는 확률 추정을 제공하기 어렵게 된다. 데이터가 충분하지 않으면 모델은 새로운 또는 드문 N-gram에 대해 부정확한 확률을 예측하게 되며, 이는 전체 모델의 성능 저하로 이어질 수 있다.

# 2. 문맥의 제한된 이해
N-gram 모델은 **고정된 길이의 문맥(Window Size)만을 고려**하기 때문에, 문장의 의미를 깊이 이해하는 데 한계가 있다. 예를 들어, Trigram 모델은 단지 이전 두 단어만을 고려하므로, 문장 전체의 의미를 반영하기 어렵다. "I will go to the store because I need to buy some"이라는 문장에서, Trigram 모델은 "I need to buy some"이라는 문맥을 제대로 이해하지 못하고, 단순히 "some" 뒤에 올 단어를 예측하게 된다. 이로 인해 모델이 예측한 단어가 문맥에 적합하지 않게 되거나, 문장의 의미를 왜곡할 가능성이 높아진다.

# 3. 계산 복잡성
$$N$$의 값이 커질수록 **N-gram의 수가 기하급수적으로 증가**한다. 예를 들어, 1만 개의 단어로 이루어진 코퍼스에서 2-gram(Bigram)을 사용할 경우 가능한 조합의 수는 약 1억 개에 달하지만, 4-gram을 사용할 경우 그 조합의 수는 10조 개에 달할 수 있다. 이렇게 많은 N-gram 조합을 저장하고 계산하려면 **막대한 컴퓨팅 자원과 메모리가 필요**하다. 이로 인해 실용적인 모델을 구축하고 운영하는 데 있어 비용과 시간이 크게 증가할 수 있다.

# 4. 문법과 의미의 제한적 반영
N-gram 언어 모델은 **단어의 빈도와 연속성에 기반해 확률을 계산**하기 때문에, **복잡한 문법적 구조나 문장의 의미를 충분히 반영하지 못할** 수 있다. 예를 들어, "The cat the dog chased was black"이라는 문장에서 N-gram 모델은 "cat the dog"이나 "dog chased was"와 같은 비문법적 조합을 단순히 빈도에 따라 처리할 수 있다. 이 경우 모델은 문법적으로 부적절하거나 의미적으로 불완전한 문장을 생성할 수 있다. 이러한 제한은 모델이 생성한 텍스트의 품질을 저하시킬 수 있다.

# 5. 상충(Trade-off) 문제
N-gram 언어 모델에서는 N값을 설정할 때 상충(Trade-off) 문제가 발생한다. **$$N$$의 값이 너무 크면 문맥을 깊이 반영할 수 있지만, 데이터 희소성 문제와 모델의 크기 증가로 인한 계산 복잡성 문제가 심화**된다. 예를 들어, $$N=5$$인 5-gram 모델을 사용할 경우, "I am going to the market to buy fresh"라는 문맥에서 "fresh" 뒤에 오는 단어를 예측할 때 문장의 전체 문맥을 고려할 수 있다. 그러나 이러한 높은 N값은 해당 문장이 코퍼스에서 자주 등장하지 않기 때문에 희소성 문제가 발생하고, 모델의 예측 신뢰도가 떨어질 수 있다.

반대로, N의 값이 너무 작으면(예: Unigram 또는 Bigram) 전체 데이터에서 문장 카운팅은 더 잘 이루어질 수 있지만, 문맥을 충분히 반영하지 못해 문장의 일관성이 떨어지거나 자연스러운 텍스트 생성이 어려워질 수 있다. 예를 들어, Bigram 모델은 "I am" 뒤에 올 단어를 "going", "happy", "tired" 등으로 예측할 수 있지만, 이러한 예측은 이전 문맥을 충분히 고려하지 못하기 때문에 문장의 흐름이 부자연스러울 수 있다. 이러한 이유로 N의 값은 트레이드오프를 고려하여 적절히 설정해야 하며, 일반적으로 Trigram 모델 정도의 복잡도가 실용적이고 효과적인 선택으로 권장된다.

<br/>
<br/>

# Reference
[블로그: \[NLP\] N-gram 언어 모델의 개념, 종류, 한계점](https://heytech.tistory.com/343?category=453616)    
[N-Gram](https://botpenguin.com/glossary/n-gram)  







