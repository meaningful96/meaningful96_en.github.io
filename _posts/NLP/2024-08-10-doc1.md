---
title: "[NLP]TF-IDF 란?"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-10
last_modified_at: 2024-08-10
---

# TFD-ID의 정의

**TF-IDF(Term Frequency-Inverse Document Frequency)**는 텍스트 마이닝과 자연어 처리에서 중요한 기법으로, <span style="color:red">**특정 문서 내에서 자주 등장하지만 전체 문서에서는 드물게 등장하는 단어**</span>에 높은 가중치를 부여하는 방법이다. TF-IDF는 문서 내에서 자주 등장하는 단어가 해당 문서의 주제를 잘 반영한다고 가정하는데, 단순히 자주 등장한다고 해서 중요한 단어로 간주될 수는 없기에, 이를 보정하기 위한 방법으로 만들어졌다.

TF-IDF는 일반적으로 문서 내에서 단어의 출현 빈도만을 고려하는 DTM과는 달리, **단어의 중요도**를 고려하기 때문에 유의미한 정보를 추출하는데 효과적이다. 

# TFD-ID의 계산법
## TF (Term Frequency)
TF는 <span style="color:red">**특정 단어 $$t$$가 특정 문서 $$d$$**</span>에 얼마나 자주 등장하는지를 나타낸다. TF 계산방법은 카운트 기반의 단어표현 방법인 DTM과 Bag of Words(BoW)에서 문서마다 단어별 출현 횟수를 계산하는 방법과 동일하다. 즉, 단어의 **등장 빈도**를 사용하여 계산하며, 보통 다음과 같이 계산된다. 

<center>$$\text{TF}(t) = \frac{\text{해당 단어 t의 빈도}}{문서 내 모든 단어의 빈도 합}$$</center>

중요한 점은 TF는 모든 문서가 아닌 **특정 문서에서 단어 $$t$$의 등장 빈도수**를 나타낸다. TF를 구하는 방법을 살펴보기 위해 세 개의 문서가 있다고 가정하자. 그리고 문서 1에서 특정 단어 "cat"에 대한 TF를 구한다고 가정한다.

- 문서 1: “The cat chased the mouse under the table.”
- 문서 2: “The mouse found a piece of cheese.”
- 문서 3: “The dog barked at the cat loudly.”

<p align="center">
<img width="1000" alt="1" src="https://github.com/user-attachments/assets/2b41bdcf-4cd1-4d7b-bb2b-57699f10fd0b">
</p>

이를 표로 나타내면 위의 그림과 같이 나온다. 문서 1에서 "cat"라는 단어는 한 번 등장한 것을 알 수 있다. 따라서 TF는 $$1$$이다. 다시 말해, TF는 특정 문서에서 단어의 빈도를 나타내며, 단어가 그 문서에서 얼마나 자주 등장하는지를 측정한다. 높은 TF 값을 가진 단어는 해당 문서에서 많이 등장하는 단어이다. 그러나 위의 경우에서처럼 단순히 TF만 사용하면, 'the'와 'a' 같은 **불용어(stop words)도 높은 빈도를 가지게 되어 중요하게 간주**될 수 있다.

## IDF (Inverse Document Frequency)
DF(Document Frequency) <span style="color:red">**특정 단어 $$t$$가 전체 문서 집합 $$\mathcal{D}$$**</span>에서 등장하는 빈도수를 나타낸다. 위의 예시를 다시 사용하면 "cat"라는 단어는 문서 1,2,3에서 각각 1,0,1번씩 등장하였다. 따라서 해당 예시에서 "cat"에대한 DF값은 $$2$$이다. 이처럼 어떤 단어가 많은 문서에서 등장한다면, DF 값이 높아진다. 하지만, DF 값이 높다는 것은 그 단어가 많은 문서에서 흔하게 등장한다는 것을 의미할 뿐, 그 단어가 특정 문서에서 중요한 의미를 가진다는 것을 보장하지 않는다. TF와 마찬가지로 불용어가 압도적으로 모든 문서에서 많이 등장하기 때문이다.

이러한 단점을 방지하고자 나온 것이 **IDF(Inverse Document Frequency)**이다. DF와는 달리, IDF는 <span style="color:red">**특정 단어가 전체 문서 집합에서 얼마나 드물게 등장하는지**</span>를 나타낸다. 어떤 단어가 거의 모든 문서에서 등장한다면, 그 단어는 특정 문서에서 중요한 단어로 간주되지 않도록 IDF 값이 낮아진다. IDF 값은 불용어와 같이 대부분의 문서에 공통으로 등장하는 단어들의 가중치를 낮추는 역할을 한다. 이를 수식으로 나타내면 다음과 같다.

<center>$$\text{IDF}(t) = \log \frac{\text{전체 문서의 수}}{\text{해당 단어 t를 포함하는 문서의 수}}$$</center>

## TF-IDF
TF와 IDF를 곱하여 구한 값이 TF-IDF다. TF-IDF 값이 높을수록 해당 단어가 특정 문서에서 중요하다는 것을 의미한다.

<center> $$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)$$ </center>

<br/>
<br/>

# Python 코드 예시
예를 들어, 뉴스 기사 모음에서 특정 주제에 관한 단어들이 등장하는 빈도를 분석하고자 할 때, TF-IDF를 사용하면 각 기사 내에서 중요한 단어들을 파악할 수 있다. 이로 인해 검색 엔진에서 문서의 관련성을 평가하거나, 텍스트 분류, 클러스터링 등에 활용될 수 있다.

TF-IDF는 단순한 빈도 분석보다 훨씬 정교하게 문서의 주요 키워드를 추출할 수 있는 방법이기 때문에, 다양한 텍스트 분석에서 널리 사용되고 있다.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 샘플 문서 리스트
documents = [
  “The cat chased the mouse under the table.”,
  “The mouse found a piece of cheese.”,
  “The dog barked at the cat loudly.”
]

# TF-IDF 벡터라이저 초기화
vectorizer = TfidfVectorizer()

# 문서에 TF-IDF 적용
tfidf_matrix = vectorizer.fit_transform(documents)

# 단어 리스트 출력
print("단어 리스트:", vectorizer.get_feature_names_out())

# TF-IDF 행렬 출력
print("TF-IDF 행렬:\n", tfidf_matrix.toarray())
```

\[**출력**\]
```bash
단어 리스트: ['at' 'barked' 'cat' 'chased' 'cheese' 'dog' 'found' 'loudly' 'mouse' 'of'
 'piece' 'table' 'the' 'under']
TF-IDF 행렬:
 [[0.    0.    0.28155563 0.37021182 0.    0.
 0.    0.    0.28155563 0.    0.    0.37021182
  0.6559592  0.37021182]
 [0.    0.    0.    0.    0.45050407 0.
  0.45050407 0.    0.34261996 0.45050407 0.45050407 0.
  0.26607496 0.        ]
 [0.40914568 0.40914568 0.31116583 0.    0.    0.40914568
  0.    0.40914568 0.    0.    0.    0.
  0.48329606 0.        ]]
```
- 문장 1. “The cat chased the mouse under the table.”
  - \[0. 0. 0.28155563 0.37021182 0. 0. 0. 0. 0.28155563 0. 0. 0.37021182 0.6559592  0.37021182\]
  - chased 가중치: 0.28155563


