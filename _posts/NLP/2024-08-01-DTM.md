---
title: "[NLP]문서 단어 행렬(Document-Term Matrix, DTM)"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-01
last_modified_at: 2024-08-01
---

# Document-Term Matrix (DTM)

문서 단어 행렬(Documnet-Term Matrix, DTM)는 [BoW](https://meaningful96.github.io/nlp/BoW/)에서 확장된 개념이다. DTM은 **<span style="color:red">여러 문서 데이터(=Corpus)</span>에서 등장한 모든 단어를 출현 빈도에 대한 행렬로 표현**하는 것이다. DTM은 BoW의 일종이므로 이 역시 국소 표현(Local Representation)에 속하며, 카운트 기반의 단어 표현 방법이다.

예를 들어 다음과 같이 문서가 있다고 가정하자.
- 문서 1: "The cat chased the mouse under the table."
- 문서 2: "The mouse found a piece of cheese."
- 문서 3: "The dog barked at the cat loudly."

<p align="center">
<img width="1000" alt="1" src="https://github.com/user-attachments/assets/2b41bdcf-4cd1-4d7b-bb2b-57699f10fd0b">
</p>

```bash
단어 집합: {'The': 0, 'cat': 1, 'chased': 2, 'the': 3, 'mouse': 4, 'under': 5, 'table': 6, 'found': 7, 'a': 8, 'piece': 9, 'of': 10, 'cheese': 11, 'dog': 12, 'barked': 13, 'at': 14, 'loudly': 15}
DTM:
문서 1: [1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
문서 2: [1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]
문서 3: [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```
단어들을 위와 같이 인덱싱 할 수 있고, 이 인덱싱을 토대로 각 문서별로 크기가 동일한 벡터를 만들어 위와 같이 나타낼 수 있다. 

# DTM의 한계점
<span style="font-size:110%">**희소 행렬 표현(Sparse Matrix Representation)**</span>  
희소 행렬(Sparse Matrix)란, 대부분의 요소가 0인 행렬을 말한다. DTM은 희소 행렬 표현 문제로 인해 높은 계산 복잡도와 저장공간 낭비 문제가 발생한다.

DTM은 문서 개수와 문서 내 모든 단어 집합의 크기만큼의 차원을 가진다. 예를 들어, 문서 개수가 5개이고 문서 내에 포함된 단어가 100개이면 $$5 \times 100$$ 크기의 행렬이 만들어진다. 이 때, 첫번째 문서에 포함된 단어가 3개라고 가정하면, 첫 번째 문서의 표현식에서 3개를 제외한 97개가 0이 된다. 이처럼 DTM은 희소 행렬을 생성하게되며, 문서의 개수와 단어 수에 따라 기하급수적으로 행렬의 크기가 증가해 메모리 비효율적일 수 있다.

또한 DTM을 처리하는 과정에서 모든 단어를 인덱싱하고 카운팅해야하는데, 이는 매우 큰 계산 복잡도를 요구한다. 

<span style="font-size:110%">**단순 빈도수 기반 단어 표현**</span>  
이는 BoW에서와 유사한 한계점이다. 여러 문서에서 등장하는 단어의 출현 빈도만을 고려하기 때문에, 문서 내의 문장의 문맥적 의미, 단어 간의 배열, 문장의 순서 등을 전혀 고려하지 못한다. 또한, a나 the와 같이 관사 등의 불요어의 출현 빈도가 압도적으로 많기 때문에 실제로 중요한 의미를 가져 많이 등장했음에도 이 불용어들과 구분히 안되는 경우가 발생할 수 있다.


# Reference
[\[NLP\] 문서 단어 행렬(DTM) 개념 이해](https://heytech.tistory.com/335)
